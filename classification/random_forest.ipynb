{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Встановлення залежностей"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable",
      "\nRequirement already satisfied: wfdb in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (3.4.1)\nRequirement already satisfied: requests>=2.8.1 in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (from wfdb) (2.21.0)\nRequirement already satisfied: matplotlib>=3.3.4 in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (from wfdb) (3.3.4)\nRequirement already satisfied: numpy>=1.10.1 in c:\\program files\\python\\lib\\site-packages (from wfdb) (1.19.5)\nRequirement already satisfied: pandas>=0.17.0 in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (from wfdb) (0.24.2)\nRequirement already satisfied: scipy>=0.17.0 in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (from wfdb) (1.5.3)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\program files\\python\\lib\\site-packages (from matplotlib>=3.3.4->wfdb) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in c:\\program files\\python\\lib\\site-packages (from matplotlib>=3.3.4->wfdb) (0.10.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in c:\\program files\\python\\lib\\site-packages (from matplotlib>=3.3.4->wfdb) (1.2.0)\nRequirement already satisfied: python-dateutil>=2.1 in c:\\program files\\python\\lib\\site-packages (from matplotlib>=3.3.4->wfdb) (2.8.1)\nRequirement already satisfied: pillow>=6.2.0 in c:\\program files\\python\\lib\\site-packages (from matplotlib>=3.3.4->wfdb) (7.2.0)\nRequirement already satisfied: six in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (from cycler>=0.10->matplotlib>=3.3.4->wfdb) (1.12.0)\nRequirement already satisfied: pytz>=2011k in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (from pandas>=0.17.0->wfdb) (2020.1)\nRequirement already satisfied: idna<2.9,>=2.5 in c:\\program files\\python\\lib\\site-packages (from requests>=2.8.1->wfdb) (2.8)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\program files\\python\\lib\\site-packages (from requests>=2.8.1->wfdb) (3.0.4)\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (from requests>=2.8.1->wfdb) (1.24.3)\nRequirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python\\lib\\site-packages (from requests>=2.8.1->wfdb) (2020.6.20)\n",
      "Defaulting to user installation because normal site-packages is not writeable",
      "\nRequirement already satisfied: py-ecg-detectors in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (1.0.2)\nRequirement already satisfied: pywavelets in c:\\program files\\python\\lib\\site-packages (from py-ecg-detectors) (1.1.1)\nRequirement already satisfied: numpy in c:\\program files\\python\\lib\\site-packages (from py-ecg-detectors) (1.19.5)\nRequirement already satisfied: scipy in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (from py-ecg-detectors) (1.5.3)\nRequirement already satisfied: gatspy in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (from py-ecg-detectors) (0.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable",
      "\nRequirement already satisfied: sklearn in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (0.0)\nRequirement already satisfied: scikit-learn in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (from sklearn) (0.24.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (from scikit-learn->sklearn) (2.1.0)\nRequirement already satisfied: scipy>=0.19.1 in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (from scikit-learn->sklearn) (1.5.3)\nRequirement already satisfied: numpy>=1.13.3 in c:\\program files\\python\\lib\\site-packages (from scikit-learn->sklearn) (1.19.5)\nRequirement already satisfied: joblib>=0.11 in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: gensim in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (4.1.2)\nRequirement already satisfied: smart-open>=1.8.1 in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (from gensim) (5.2.1)\nRequirement already satisfied: dataclasses in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (from gensim) (0.8)\nRequirement already satisfied: scipy>=0.18.1 in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (from gensim) (1.5.3)\nRequirement already satisfied: Cython==0.29.23 in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (from gensim) (0.29.23)\nRequirement already satisfied: numpy>=1.17.0 in c:\\program files\\python\\lib\\site-packages (from gensim) (1.19.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: scikit-learn in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (0.24.2)\nRequirement already satisfied: scipy>=0.19.1 in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (from scikit-learn) (1.5.3)\nRequirement already satisfied: numpy>=1.13.3 in c:\\program files\\python\\lib\\site-packages (from scikit-learn) (1.19.5)\nRequirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (from scikit-learn) (2.1.0)\nRequirement already satisfied: joblib>=0.11 in c:\\users\\mikle\\appdata\\roaming\\python\\python36\\site-packages (from scikit-learn) (0.17.0)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 21.2.4 is available.\nYou should consider upgrading via the 'c:\\program files\\python\\python.exe -m pip install --upgrade pip' command.\n",
      "WARNING: You are using pip version 21.1.1; however, version 21.2.4 is available.\nYou should consider upgrading via the 'c:\\program files\\python\\python.exe -m pip install --upgrade pip' command.\n",
      "WARNING: You are using pip version 21.1.1; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the 'c:\\program files\\python\\python.exe -m pip install --upgrade pip' command.\n",
      "WARNING: You are using pip version 21.1.1; however, version 21.2.4 is available.\nYou should consider upgrading via the 'c:\\program files\\python\\python.exe -m pip install --upgrade pip' command.\n",
      "WARNING: You are using pip version 21.1.1; however, version 21.2.4 is available.\nYou should consider upgrading via the 'c:\\program files\\python\\python.exe -m pip install --upgrade pip' command.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "!pip install wfdb\n",
    "!pip install py-ecg-detectors\n",
    "!pip install sklearn\n",
    "!pip install gensim\n",
    "!pip install -U scikit-learn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import os\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Завантаження даних ЕКГ"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "You already have the data.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "if os.path.isdir(\"../mitdb\"):\n",
    "    print('You already have the data.')\n",
    "else:\n",
    "    wfdb.dl_database('../mitdb', 'mitdb')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1cdbae3cf905>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrecord\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecord\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwfdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdsamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mitdb/100'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampto\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# record[0][:,0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#annotation = wfdb.rdann('mitdb/100','atr', sampfrom=50)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# record[1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\wfdb\\io\\record.py\u001b[0m in \u001b[0;36mrdsamp\u001b[1;34m(record_name, sampfrom, sampto, channels, pn_dir, channel_names, warn_empty, return_res)\u001b[0m\n\u001b[0;32m   3730\u001b[0m                       \u001b[0msampto\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msampto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchannels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphysical\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3731\u001b[0m                       \u001b[0mpn_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpn_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm2s\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_res\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_res\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3732\u001b[1;33m                       channel_names=channel_names, warn_empty=warn_empty)\n\u001b[0m\u001b[0;32m   3733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3734\u001b[0m     \u001b[0msignals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_signal\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\wfdb\\io\\record.py\u001b[0m in \u001b[0;36mrdrecord\u001b[1;34m(record_name, sampfrom, sampto, channels, physical, pn_dir, m2s, smooth_frames, ignore_skew, return_res, force_channels, channel_names, warn_empty)\u001b[0m\n\u001b[0;32m   3431\u001b[0m         \u001b[0mrecord\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwav2mit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpn_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpn_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecord_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3432\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3433\u001b[1;33m         \u001b[0mrecord\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdheader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpn_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpn_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrd_segments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3435\u001b[0m     \u001b[1;31m# Set defaults for sampto and channels input variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\wfdb\\io\\record.py\u001b[0m in \u001b[0;36mrdheader\u001b[1;34m(record_name, pn_dir, rd_segments)\u001b[0m\n\u001b[0;32m   3258\u001b[0m     \u001b[1;31m# Read the header file. Separate comment and non-comment lines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3259\u001b[0m     header_lines, comment_lines = _header._read_header_lines(base_record_name,\n\u001b[1;32m-> 3260\u001b[1;33m                                                              dir_name, pn_dir)\n\u001b[0m\u001b[0;32m   3261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3262\u001b[0m     \u001b[1;31m# Get fields from record line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\wfdb\\io\\_header.py\u001b[0m in \u001b[0;36m_read_header_lines\u001b[1;34m(base_record_name, dir_name, pn_dir)\u001b[0m\n\u001b[0;32m    850\u001b[0m     \u001b[1;31m# Read local file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    851\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpn_dir\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 852\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    853\u001b[0m             \u001b[1;31m# Record line followed by signal/segment lines if any\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m             \u001b[0mheader_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\mikle\\\\Documents\\\\0 KPI\\\\diploma\\\\word2vec-analysis\\\\classification\\\\mitdb\\\\100.hea'"
     ],
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\mikle\\\\Documents\\\\0 KPI\\\\diploma\\\\word2vec-analysis\\\\classification\\\\mitdb\\\\100.hea'",
     "output_type": "error"
    }
   ],
   "source": [
    "record = record = wfdb.rdsamp('../mitdb/100', sampto=500000)\n",
    "record[1]\n",
    "# record[0][:,0]\n",
    "#annotation = wfdb.rdann('../mitdb/100','atr', sampfrom=50)\n",
    "# record[1]\n",
    "# record[0][:,0]\n",
    "#sample = annotation.__dict__\n",
    "#sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Виділення R піків"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ecgdetectors import Detectors\n",
    "fs = 250\n",
    "detectors = Detectors(fs)\n",
    "unfiltered_ecg = record[0][:, 1]\n",
    "r_peaks = detectors.engzee_detector(unfiltered_ecg)\n",
    "# Amendment for first peak\n",
    "r_peaks[0] += 20"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "r_peaks_split = r_peaks[:11]\n",
    "plt.figure(figsize=(24,6))\n",
    "plt.plot(unfiltered_ecg[:3100])\n",
    "plt.plot(r_peaks_split, unfiltered_ecg[r_peaks_split], 'ro')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Розподіл ЕКГ сигналу на серцебиття"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find heartbeat lenght\n",
    "distances = []\n",
    "for i in range(len(r_peaks)):\n",
    "  if i + 1 < len(r_peaks):\n",
    "    distances.append(r_peaks[i + 1] - r_peaks[i])\n",
    "distances = np.array(distances)\n",
    "heart_beat_len = int(distances.mean())\n",
    "heart_beat_len"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Beat EGC signal\n",
    "beats = []\n",
    "for i in range(len(r_peaks)):\n",
    "  if i != 0 :\n",
    "    r_index = r_peaks[i]\n",
    "    retreat = heart_beat_len // 2\n",
    "    beats.append(unfiltered_ecg[r_index - retreat:r_index + retreat])\n",
    "len(beats)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Візуалізація серцебиттів"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ig, ax = plt.subplots(nrows=10, figsize=(3,36))\n",
    "plots = []\n",
    "for beat in beats:\n",
    "  plots.append(beat)\n",
    "\n",
    "\n",
    "\n",
    "i = 0 \n",
    "for plot in plots:\n",
    "  if i < 10:\n",
    "    ax[i].plot(plot)\n",
    "    ax[i].set_ylabel('Heartbeat {}'.format(i + 1))\n",
    "    ax[i].set_xlabel('Datapoints')\n",
    "  i += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Виділення хвиль в кожному серцебитті"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p_waves = []\n",
    "qrs_waves = []\n",
    "t_waves = []\n",
    "for j in range(len(beats)):\n",
    "  if j + 1 != len(beats):\n",
    "    p_waves.append(beats[j][:retreat - 15])\n",
    "    qrs_waves.append(beats[j][retreat - 15:retreat + 15])\n",
    "    t_waves.append(beats[j][retreat + 15:])\n",
    "\n",
    "print(len(t_waves[0]))\n",
    "print(len(p_waves))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Кластеризація виділених хвиль\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# P and T waves clustering (20 clusters)\n",
    "from sklearn.cluster import KMeans\n",
    "pt_waves = np.array(p_waves + t_waves)\n",
    "print(pt_waves.shape)\n",
    "kmeans = KMeans(init='k-means++', n_clusters=300, n_init=10)\n",
    "kmeans.fit(pt_waves)\n",
    "predict_pt = kmeans.predict(pt_waves)\n",
    "predict_pt.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# QRS waves clustering (6 clusters)\n",
    "qrs_waves = np.array(qrs_waves)\n",
    "kmeans_1 = KMeans(init='k-means++', n_clusters=40, n_init=10)\n",
    "kmeans_1.fit(qrs_waves)\n",
    "predict_qrs = kmeans_1.predict(qrs_waves)\n",
    "predict_qrs.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Присвоювання символів хвилі кожного кластеру"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "alphabet_for_pt = {0:'a', 1:'b', 2:'c', 3:'d', 4:'e', 5:'f', 6:'g', 7:'h', 8:'i',\n",
    "                   9:'j', 10:'k', 11:'l', 12:'m', 13:'n', 14:'o', 15:'p', 16:'q', 17:'r',\n",
    "                   18:'s', 19:'t'}\n",
    "alphabet_for_qrs = {0:'u', 1:'v', 2:'w', 3:'x', 4:'y', 5:'z'}\n",
    "\n",
    "def get_symbol_pt(x):\n",
    "  return alphabet_for_pt[x]\n",
    "\n",
    "def get_symbol_qrs(x):\n",
    "  return alphabet_for_qrs[x]\n",
    "\n",
    "vfunc = np.vectorize(get_symbol_pt)\n",
    "vfunc_2 = np.vectorize(get_symbol_qrs)\n",
    "\n",
    "# predict_pt_letters = vfunc(predict_pt)\n",
    "# predict_qrs_letters = vfunc_2(predict_qrs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#len(predict_qrs_letters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# words = []\n",
    "# for i in range(len(predict_qrs_letters)):\n",
    "#   word = ''\n",
    "#   word += predict_pt_letters[i]\n",
    "#   word += predict_qrs_letters[i]\n",
    "#   word += predict_pt_letters[i + len(predict_qrs_letters)//2]\n",
    "#   words.append(word)\n",
    "#\n",
    "# words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# word2vec = Word2Vec([words,], min_count=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# vocabulary = word2vec.wv.key_to_index\n",
    "# vocabulary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#sim = word2vec.wv.most_similar('jwl')\n",
    "#sim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#word2vec.wv['jwl']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_blobs\n",
    "# X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "#                        cluster_std=0.60, random_state=0)\n",
    "# from sklearn.cluster import KMeans\n",
    "# kmeansS = KMeans(n_clusters=4)\n",
    "# kmeansS.fit(X)\n",
    "# kmeansS.cluster_centers_.shape\n",
    "#\n",
    "# np.matrix([1, 2, 3])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reversed_alphabet_for_pt = {}\n",
    "for k,v in alphabet_for_pt.items():\n",
    "  reversed_alphabet_for_pt[v] = k\n",
    "\n",
    "reversed_alphabet_for_qrs = {}\n",
    "for k,v in alphabet_for_qrs.items():\n",
    "  reversed_alphabet_for_qrs[v] = k\n",
    "\n",
    "pr_centers = kmeans.cluster_centers_\n",
    "qrs_centers = kmeans_1.cluster_centers_\n",
    "# KMeans Original data: 3484x128\n",
    "print(\"pr_centers.shape:\")\n",
    "print(pr_centers.shape)\n",
    "print(\"qrs_centers.shape:\")\n",
    "print(qrs_centers.shape)\n",
    "\n",
    "def get_pr_from_symbol(x):\n",
    "  return  pr_centers[x]\n",
    "\n",
    "def get_qrs_from_symbol(x):\n",
    "  return qrs_centers[x]\n",
    "\n",
    "reverse_vfunc = np.vectorize(get_pr_from_symbol, signature='()->(n)')\n",
    "reverse_vfunc_2 = np.vectorize(get_qrs_from_symbol, signature='()->(n)')\n",
    "\n",
    "predict_pt_original = reverse_vfunc(predict_pt)\n",
    "predict_qrs_original = reverse_vfunc_2(predict_qrs)\n",
    "print(\"predict_pt_original.shape:\")\n",
    "print(predict_pt_original.shape)\n",
    "print(\"predict_qrs_original.shape:\")\n",
    "print(predict_qrs_original.shape)\n",
    "\n",
    "splited_pt = np.split(predict_pt_original, 2)\n",
    "predict_t_original = splited_pt[0]\n",
    "predict_p_original = splited_pt[1]\n",
    "print(str(predict_pt_original.shape) + \" = \" + str(predict_t_original.shape) + \" + \" + str(predict_p_original.shape))\n",
    "\n",
    "restored_beats = np.concatenate((predict_t_original, predict_qrs_original, predict_p_original), axis=1)\n",
    "print(\"restored_beats.shape = \" + str(restored_beats.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Original:\")\n",
    "print(beats[0].shape)\n",
    "print(\"Restored:\")\n",
    "print(restored_beats[0].shape)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=10, ncols=2, figsize=(6,36))\n",
    "restored_plots = []\n",
    "for beat in restored_beats:\n",
    "  restored_plots.append(beat)\n",
    "\n",
    "original_plots = []\n",
    "for beat in beats:\n",
    "  original_plots.append(beat)\n",
    "\n",
    "for i in range(10):\n",
    "  ax1 = ax[i][0]\n",
    "  ax2 = ax[i][1]\n",
    "  ax1.plot(restored_plots[i])\n",
    "  ax1.set_ylabel('Heartbeat {}'.format(i + 1))\n",
    "  ax1.set_xlabel('Datapoints')\n",
    "\n",
    "  ax2.plot(original_plots[i])\n",
    "  ax2.set_ylabel('Heartbeat {}'.format(i + 1))\n",
    "  ax2.set_xlabel('Datapoints')\n",
    "\n",
    "plt.savefig(\"comparing.png\", transparent=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "for i in range(10):\n",
    "  rmspe = np.sqrt(np.mean(np.square(((beats[i] - restored_beats[i]) / beats[i])), axis=0))\n",
    "  print(rmspe)\n",
    "\n",
    "\n",
    "# len(words)\n",
    "# len(vocabulary)\n",
    "# kmeans.cluster_centers_.shape\n",
    "# kmeans_1.cluster_centers_.shape\n",
    "# #kmeans.score(predict_pt)\n",
    "# predict_pt\n",
    "# #kmeans.cluster_centers_[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "r_peaks_split = r_peaks[:11]\n",
    "plt.figure(figsize=(24,6))\n",
    "original_full = np.concatenate(beats)[:3100]\n",
    "plt.plot(original_full)\n",
    "restored_full = np.concatenate(restored_beats)[:3100]\n",
    "plt.plot(restored_full)\n",
    "plt.savefig(\"comparing_original.png\", transparent=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-455014b68c17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_full\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrestored_full\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0moriginal_full\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#%history -g\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "np.sqrt(np.mean(np.square(((original_full - restored_full) / original_full)), axis=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      " 1/1:\nimport numpy as np\nimport matplotlib.pyplot as plt\n 1/2:\nimport numpy as np\nimport matplotlib.pyplot as plt\n 1/3:\nimport numpy as np\nimport matplotlib.pyplot as plt\n 1/4:\nimport numpy as np\nimport matplotlib.pyplot as plt\n 1/5:\nimport numpy as np\nimport matplotlib.pyplot as plt\n 1/6:\ndef sigmoid(x):\n    return 1/(1+e(-x))\n\nx = np.linspace(-5, 5, 100)\ny = sigmoid(x)\n\nplt.plot(x, y)\nplt.ylim(0, 1)\nplt.show()\n 1/7:\nimport numpy as np\nimport matplotlib.pyplot as plt\n 1/8:\ndef sigmoid(x):\n    return 1/(1+math.e(-x))\n\nx = np.linspace(-5, 5, 100)\ny = sigmoid(x)\n\nplt.plot(x, y)\nplt.ylim(0, 1)\nplt.show()\n 1/9:\ndef sigmoid(x):\n    return 1 / (1 + math.e(-x))\n\nx = np.linspace(-5, 5, 100)\ny = sigmoid(x)\n\nplt.plot(x, y)\nplt.ylim(0, 1)\nplt.show()\n1/10:\ndef sigmoid(x):\n    return 1 / (1 + math.e(-x))\n\nx = np.linspace(-5, 5, 100)\ny = sigmoid(x)\n\nplt.plot(x, y)\nplt.ylim(0, 1)\nplt.show()\n1/11:\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\n\nx = np.linspace(-5, 5, 100)\ny = sigmoid(x)\n\nplt.plot(x, y)\nplt.ylim(0, 1)\nplt.show()\n1/12:\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\n\nx = np.linspace(-5, 5, 100)\ny = sigmoid(x)\n\nplt.plot(x, y)\nplt.ylim(0, 1)\nplt.show()\n1/13:\nimport math\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\n\nx = np.linspace(-5, 5, 100)\ny = sigmoid(x)\n\nplt.plot(x, y)\nplt.ylim(0, 1)\nplt.show()\n1/14:\nimport numpy as np\nimport matplotlib.pyplot as plt\n1/15:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n1/16:\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\n\nx = np.linspace(-5, 5, 100)\ny = sigmoid(x)\n\nplt.plot(x, y)\nplt.ylim(0, 1)\nplt.show()\n1/17:\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\n\nx = np.linspace(-5, 5, 100)\ny = sigmoid(x)\n\nplt.plot(x, y)\nplt.ylim(0, 1)\nplt.show()\n1/18:\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\n\nx = np.linspace(-5, 5, 100)\nx\ny = sigmoid(x)\n\nplt.plot(x, y)\nplt.ylim(0, 1)\nplt.show()\n1/19:\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\n\nx = np.linspace(-5, 5, 100)\nprint(x)\ny = sigmoid(x)\n\nplt.plot(x, y)\nplt.ylim(0, 1)\nplt.show()\n1/20:\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\n\ndef sigmoidEach(m):\n    return map(m, sigmoid)\n\nx = sigmoidEach(np.linspace(-5, 5, 100))\nprint(x)\ny = sigmoid(x)\n\nplt.plot(x, y)\nplt.ylim(0, 1)\nplt.show()\n1/21:\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\n\ndef sigmoidEach(m):\n    return map(sigmoid, m)\n\nx = sigmoidEach(np.linspace(-5, 5, 100))\nprint(x)\ny = sigmoid(x)\n\nplt.plot(x, y)\nplt.ylim(0, 1)\nplt.show()\n1/22:\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\n\ndef sigmoidEach(m):\n    return map(sigmoid, m)\n\nx = np.linspace(-5, 5, 100)\nprint(x)\ny = sigmoidEach(x)\n\nplt.plot(x, y)\nplt.ylim(0, 1)\nplt.show()\n1/23:\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\n\ndef sigmoidEach(m):\n    return list(map(sigmoid, m))\n\nx = np.linspace(-5, 5, 100)\ny = sigmoidEach(x)\n\nplt.plot(x, y)\nplt.ylim(0, 1)\nplt.show()\n1/24:\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\n\ndef sigmoidEach(m):\n    return list(map(sigmoid, m))\n\nx = np.linspace(-5, 5, 100)\ny = sigmoidEach(x)\n\nplt.plot(x, y)\nplt.ylim(0, 1)\nplt.show()\n1/25:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M * 0.99)\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/26:\nalpha = 0.1\nv = np.array(0, 1)\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M * 0.99)\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/27:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M * 0.99)\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/28:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M * 0.99)\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/29:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]])\n\nvs = [v]\nfor i in range(100):\n    print(v)\n    print(M)\n    v = v.dot(M * 0.99)\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/30:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M * 0.99)\n    vs.append(v)\n    \nvs = np.array(vs)\nprint(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/31:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M * 0.99)\n    vs.append(v)\n    \nvs = np.array(vs)\nprint(vs.T)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/32:\nalpha = 0.1\nv = np.matrix([[0, 1]])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M * 0.99)\n    vs.append(v)\n    \nvs = np.array(vs)\nprint(vs.T)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/33:\nalpha = 0.1\nv = np.matrix([[0, 1]])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    vs.append(v)\n    \nvs = np.array(vs)\nprint(vs.T)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/34:\nalpha = 0.1\nv = np.matrix([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/35:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/36:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    vs.append(*v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/37:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    vs.append(v[0])\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/38:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    vs.append(v[0])\n    \nvs = np.array(vs)\nprint(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/39:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    vs.append(v.flatten())\n    \nvs = np.array(vs)\nprint(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/40:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    print(v)\n    v = v.dot(M)\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/41:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    print(*v)\n    v = v.dot(M)\n    vs.append(*v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/42:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    print(*v)\n    v = v.dot(M)\n    vs.append(v.flatten())\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/43:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    print(*v)\n    v = v.dot(M)\n    vs.append(v.flatten()[0])\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/44:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    print(v.flatten()[0])\n    v = v.dot(M)\n    vs.append(v.flatten()[0])\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/45:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    print(v.flatten())\n    v = v.dot(M)\n    vs.append(v.flatten()[0])\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/46:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    print(v[0])\n    v = v.dot(M)\n    vs.append(v.flatten()[0])\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/47:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    print(v[0][0])\n    v = v.dot(M)\n    vs.append(v.flatten()[0])\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/48:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    print(v[0][0])\n    v = v.dot(M)\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/49:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    print(v[0][0])\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/50:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    print(np.asarray(v))\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/51:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    print(np.squeez(v))\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/52:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.matrix([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    print(np.squeeze(v))\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/53:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.array([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    print(np.squeeze(v))\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/54:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.array([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    vs.append(np.squeeze(v))\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/55:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.array([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    vs.append(np.squeeze(v))\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/56:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.array([math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/57:\nalpha = 0.1\nv = np.array([0, 1])\nM = np.array([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    vs.append(np.squeeze(v))\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n1/58:\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n\nX_p = # TODO\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/59:\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n\n#X_p = # TODO\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/60:\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n\nX_p = \nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/61:\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n\nX_p = \nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/62:\nfrom np.linalg import matrix_power\nmatrix_power([[1, 2], [3, 4]], 2)\n1/63:\nfrom numpy.linalg import matrix_power\nmatrix_power([[1, 2], [3, 4]], 2)\n1/64:\nfrom numpy.linalg import matrix_power\nnp.power([[1, 2], [3, 4]], 2)\n1/65: np.power([[1, 2], [3, 4]], [[2, 3], [4, 5]])\n1/66: #np.power([[1, 2], [3, 4]], [[2, 3], [4, 5]])\n1/67:\n#np.power([[1, 2], [3, 4]], [[2, 3], [4, 5]])\nnp.linspace(0, 10)\n1/68:\n#np.power([[1, 2], [3, 4]], [[2, 3], [4, 5]])\nnp.linspace(0, 10, 10)\n1/69:\n#np.power([[1, 2], [3, 4]], [[2, 3], [4, 5]])\nnp.arrange(10)\n1/70:\n#np.power([[1, 2], [3, 4]], [[2, 3], [4, 5]])\nnp.arange(10)\n1/71:\nnp.power([[1, 2], [3, 4]], [[2, 3], [4, 5]])\n#np.arange(10)\n1/72:\nnp.power([[1, 2], [3, 4]], [2, 3])\n#np.arange(10)\n1/73:\nnp.power([[1, 2], [3, 4]], [2, 3])\n#np.arange(10)\n1/74:\n#np.power([[1, 2], [3, 4]], [2, 3])\n#np.arange(10)\nnp.arange(10) @ np.ones(10)\n1/75:\n#np.power([[1, 2], [3, 4]], [2, 3])\n#np.arange(10)\nnp.arange(10) @ np.ones((10, 10))\n1/76:\n#np.power([[1, 2], [3, 4]], [2, 3])\n#np.arange(10)\nnp.array(np.arange(10) * 3)\n1/77:\n#np.power([[1, 2], [3, 4]], [2, 3])\n#np.arange(10)\nnp.array([np.arange(10), ] * 3)\n1/78:\n#np.power([[1, 2], [3, 4]], [2, 3])\n#np.arange(10)\nnp.power(np.array([np.arange(10),] * 3), np.arrange(10))\n1/79:\n#np.power([[1, 2], [3, 4]], [2, 3])\n#np.arange(10)\nnp.power(np.array([np.arange(10),] * 3), np.arange(10))\n1/80:\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\nprint(np.array([X,] * X.size))\nX_p = np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/81:\ndef toPowMatrix(array):\n    res = np.concatenate(np.ones(array.size), array)\n    for i in range(2, array.size):\n        print(i)\n    \n\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\ntoPowMatrix(X)\n\nX_p = np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/82:\ndef toPowMatrix(array):\n    res = np.concatenate(np.ones(array.size), array)\n    for i in range(2, array.size):\n        res = np.concatenate(res, res[-1] * array)\n    return res\n    \n\nprint(toPowMatrix(np.array([1, 2, 3])))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n\nX_p = np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/83:\ndef toPowMatrix(array):\n    res = np.concatenate(np.ones(array.size), array)\n    for i in range(2, array.size):\n        res = np.concatenate(res, res[-1] * array)\n    return res\n    \n\nprint(toPowMatrix(np.array([1, 2, 3])))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n\nX_p = np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/84: np.concatenate([[1, 2, 3]], [1, 2])\n1/85: np.concatenate(([[1, 2, 3]], [1, 2])\n1/86: np.concatenate(([[1, 2, 3]], [1, 2]))\n1/87: np.concatenate(([1, 2, 3], [1, 2]))\n1/88: np.concatenate(([[1, 2, 3]], [[1, 2]]))\n1/89: np.concatenate(([[1, 2, 3]], [[1, 2, 4]]))\n1/90:\ndef toPowMatrix(array):\n    res = np.concatenate([np.ones(array.size)], [array])\n    for i in range(2, array.size):\n        res = np.concatenate(res, res[-1] * array)\n    return res\n    \n\nprint(toPowMatrix(np.array([1, 2, 3])))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n\nX_p = np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/91:\ndef toPowMatrix(array):\n    res = np.concatenate(([np.ones(array.size)], [array]))\n    for i in range(2, array.size):\n        res = np.concatenate((res, res[-1] * array))\n    return res\n    \n\nprint(toPowMatrix(np.array([1, 2, 3])))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n\nX_p = np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/92:\ndef toPowMatrix(array):\n    res = np.concatenate(([np.ones(array.size)], [array]))\n    for i in range(2, array.size):\n        res = np.concatenate((res, np.multiply(res[-1], array)))\n    return res\n    \n\nprint(toPowMatrix(np.array([1, 2, 3])))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n\nX_p = np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/93:\ndef toPowMatrix(array):\n    res = np.concatenate(([np.ones(array.size)], [array]))\n    for i in range(2, array.size):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res\n    \n\nprint(toPowMatrix(np.array([1, 2, 3])))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n\nX_p = np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/94:\ndef toPowMatrix(array):\n    res = np.concatenate(([np.ones(array.size)], [array]))\n    for i in range(2, array.size):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res\n    \n\nprint(toPowMatrix(np.array([1, 2, 3])))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n\nX_p = toPowMatrix(X)#np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/95:\ndef toPowMatrix(array):\n    res = np.concatenate(([np.ones(array.size)], [array]))\n    for i in range(2, array.size):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res\n    \n\nprint(toPowMatrix(np.array([1, 2, 3])))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\nprint(X)\nX_p = toPowMatrix(X)#np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/96:\ndef toPowMatrix(array):\n    res = np.concatenate(([np.ones(array.size)], [array]))\n    for i in range(2, array.size):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res\n    \n\nprint(toPowMatrix(np.array([1, 2, 3])))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\nprint(X[0])\nX_p = toPowMatrix(X)#np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/97:\ndef toPowMatrix(array):\n    res = np.concatenate(([np.ones(array.size)], [array]))\n    for i in range(2, array.size):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res\n    \n\nprint(toPowMatrix(np.array([1, 2, 3])))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\nprint(*X)\nX_p = toPowMatrix(X)#np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/98:\ndef toPowMatrix(array):\n    res = np.concatenate(([np.ones(array.size)], [array]))\n    for i in range(2, array.size):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res\n    \n\nprint(toPowMatrix(np.array([1, 2, 3])))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\nprint(X)\nX_p = toPowMatrix(X)#np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/99:\ndef toPowMatrix(array):\n    res = np.concatenate(([np.ones(array.size)], [array]))\n    for i in range(2, array.size):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res\n    \n\nprint(toPowMatrix(np.array([[1], [2], [3]])))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\nprint(X)\nX_p = toPowMatrix(X)#np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/100:\ndef toPowMatrix(array):\n    array = array.transpose()\n    res = np.concatenate(([np.ones(array.size)], [array]))\n    for i in range(2, array.size):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n    \n\nprint(toPowMatrix(np.array([[1], [2], [3]])))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\nprint(X)\nX_p = toPowMatrix(X)#np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/101:\ndef toPowMatrix(array):\n    array = array.transpose()\n    print(array)\n    res = np.concatenate(([np.ones(array.size)], [array]))\n    for i in range(2, array.size):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n    \n\nprint(toPowMatrix(np.array([[1], [2], [3]])))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\nprint(X)\nX_p = toPowMatrix(X)#np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/102:\ndef toPowMatrix(array):\n    array = *array.transpose()\n    res = np.concatenate(([np.ones(array.size)], [array]))\n    for i in range(2, array.size):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n    \n\nprint(toPowMatrix(np.array([[1], [2], [3]])))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\nprint(X)\nX_p = toPowMatrix(X)#np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/103:\ndef toPowMatrix(array):\n    array = * array.transpose()\n    res = np.concatenate(([np.ones(array.size)], [array]))\n    for i in range(2, array.size):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n    \n\nprint(toPowMatrix(np.array([[1], [2], [3]])))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\nprint(X)\nX_p = toPowMatrix(X)#np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/104:\ndef toPowMatrix(array):\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))\n    for i in range(2, array.size):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n    \n\nprint(toPowMatrix(np.array([[1], [2], [3]])))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\nprint(X)\nX_p = toPowMatrix(X)#np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/105:\ndef toPowMatrix(array):\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))\n    for i in range(2, array.size):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n    \n\nprint(toPowMatrix(np.array([[1], [2], [3]])))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\nX_p = toPowMatrix(X)#np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/106:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n    \n\nprint(toPowMatrix(np.array([[1], [2], [3]]), 0))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\nX_p = toPowMatrix(X)#np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/107:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n    \n\nprint(toPowMatrix(np.array([[1], [2], [3]]), 1))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\nX_p = toPowMatrix(X)#np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/108:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n    \n\nprint(toPowMatrix(np.array([[1], [2], [3]]), 1))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\nX_p = toPowMatrix(X, 5)#np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/109:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n    \n\nprint(toPowMatrix(np.array([[1], [2], [3]]), 1))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\nX_p = toPowMatrix(X, 5)#np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/110:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n    \n\nprint(toPowMatrix(np.array([[1], [2], [3]]), 0))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\nX_p = toPowMatrix(X, 5)#np.power(np.array([X,] * X.size), np.arange(X.size))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/111:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/112:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    return np.power(np.array([X,] * maxPow), np.arange(maxPow))\n\nprint(toPowMatrixAlt([[1], [2], [3]]))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/113:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    return np.power(np.array([X,] * maxPow), np.arange(maxPow))\n\nprint(toPowMatrixAlt([[1], [2], [3]], 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/114:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    array = array.transpose()[0]\n    return np.power(np.array([X,] * maxPow), np.arange(maxPow))\n\nprint(toPowMatrixAlt([[1], [2], [3]], 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/115:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    array = array.transpose()[0]\n    return np.power(np.array([X,] * maxPow), np.arange(maxPow))\n\nprint(toPowMatrixAlt(np.array([1], [2], [3]), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/116:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    array = array.transpose()[0]\n    return np.power(np.array([X,] * maxPow), np.arange(maxPow))\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/117:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    array = array.transpose()[0]\n    return np.power(np.array([array,] * maxPow), np.arange(maxPow))\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/118:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    array = array.transpose()[0]\n    return np.power(np.array([array,] * maxPow), np.arange(maxPow)).transpose()\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/119:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    array = array.transpose()[0]\n    print(np.array([array,] * maxPow))\n    return np.power(np.array([array,] * maxPow), np.arange(maxPow)).transpose()\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/120:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    array = array.transpose()[0]\n    print(np.arange(maxPow))\n    return np.power(np.array([array,] * maxPow), np.arange(maxPow)).transpose()\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/121:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    array = array.transpose()[0]\n    print([array]*maxPow)\n    return np.power(np.array([array] * maxPow), np.arange(maxPow)).transpose()\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/122:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    #array = array.transpose()[0]\n    print([array]*maxPow)\n    return np.power(np.array([array] * maxPow), np.arange(maxPow)).transpose()\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/123:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    #array = array.transpose()[0]\n    print(np.array(array)*maxPow)\n    return np.power(np.array([array] * maxPow), np.arange(maxPow)).transpose()\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/124:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    #array = array.transpose()[0]\n    print(np.array([array]*maxPow))\n    return np.power(np.array([array] * maxPow), np.arange(maxPow)).transpose()\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/125:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    array = array.transpose()[0]\n    print(np.array([array]*maxPow))\n    return np.power(np.array([array] * maxPow), np.arange(maxPow)).transpose()\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/126:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    #array = array.transpose()[0]\n    print(np.array([array]*maxPow))\n    return np.power(np.array([array] * maxPow), np.arange(maxPow)).transpose()\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/127:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    #array = array.transpose()[0]\n    print(np.array(array * maxPow))\n    return np.power(np.array([array] * maxPow), np.arange(maxPow)).transpose()\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/128:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    #array = array.transpose()[0]\n    print(np.array([array]*maxPow))\n    return np.power(np.array([array] * maxPow), np.arange(maxPow)).transpose()\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/129:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    #array = array.transpose()[0]\n    print(np.array([array]*maxPow).flatten())\n    return np.power(np.array([array] * maxPow), np.arange(maxPow)).transpose()\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/130:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    #array = array.transpose()[0]\n    print(*np.array([array]*maxPow)\n    return np.power(np.array([array] * maxPow), np.arange(maxPow)).transpose()\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/131:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    #array = array.transpose()[0]\n    print(np.array(*([array]*maxPow)))\n    return np.power(np.array([array] * maxPow), np.arange(maxPow)).transpose()\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/132:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    #array = array.transpose()[0]\n    print(np.array( (*([array] * maxPow)) ) )\n    return np.power(np.array([array] * maxPow), np.arange(maxPow)).transpose()\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/133:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    #array = array.transpose()[0]\n    print(np.array( [*([array] * maxPow)] ) )\n    return np.power(np.array([array] * maxPow), np.arange(maxPow)).transpose()\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/134:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    array = array.transpose()[0]\n    print(np.array([array] * maxPow) )\n    return np.power(np.array([array] * maxPow), np.arange(maxPow)).transpose()\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/135:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    array = array.transpose()[0]\n    print(np.array([array] * maxPow) )\n    return np.power(np.array([array] * maxPow), np.arange(maxPow)).transpose()\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\n#X_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/136:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    array = array.transpose()[0]\n    print(np.array([array] * maxPow) )\n    return np.power(np.array([array] * maxPow).transpose(), np.arange(maxPow)).transpose()\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\n#X_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/137:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    array = array.transpose()[0]\n    print(np.array([array] * maxPow) )\n    return np.power(np.array([array] * maxPow).transpose(), np.arange(maxPow))\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\n#X_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/138:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    array = array.transpose()[0]\n    print(np.array([array] * maxPow) )\n    return np.power(np.array([array] * maxPow).transpose(), np.arange(maxPow)).transpose()\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\n#X_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/139:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    array = array.transpose()[0]\n    print(np.array([array] * maxPow) )\n    return np.power(np.array([array] * maxPow).transpose(), np.arange(maxPow))\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\n#X_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/140:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    array = array.transpose()[0]\n    print(np.array([array] * maxPow) )\n    return np.power(np.array([array] * maxPow).transpose(), np.arange(maxPow))\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\nprint(toPowMatrix(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\n#X_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/141:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    maxPow += 1\n    array = array.transpose()[0]\n    print(np.array([array] * maxPow) )\n    return np.power(np.array([array] * maxPow).transpose(), np.arange(maxPow))\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\nprint(toPowMatrix(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\n#X_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/142:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    maxPow += 1\n    #array = array.transpose()[0]\n    return np.power(np.array([array] * maxPow), np.arange(maxPow))\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\nprint(toPowMatrix(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\n#X_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/143:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    maxPow += 1\n    #array = array.transpose()[0]\n    print(np.array([array] * maxPow))\n    return np.power(np.array([array] * maxPow).transpose(), np.arange(maxPow))\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\nprint(toPowMatrix(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\n#X_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/144:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    maxPow += 1\n    array = array.transpose()[0]\n    return np.power(np.array([array] * maxPow).transpose(), np.arange(maxPow))\n\nprint(toPowMatrixAlt(np.array(([1], [2], [3])), 3))\nprint(toPowMatrix(np.array(([1], [2], [3])), 3))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\n#X_p = np.power(np.array([X,] * X.size), np.arange(5))\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/145:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    maxPow += 1\n    array = array.transpose()[0]\n    return np.power(np.array([array] * maxPow).transpose(), np.arange(maxPow))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = toPowMatrixAlt(X, 5)\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/146:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    maxPow += 1\n    array = array.transpose()[0]\n    return np.power(np.array([array] * maxPow).transpose(), np.arange(maxPow))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\nX_p = toPowMatrix(X, 5)\n#X_p = toPowMatrixAlt(X, 5)\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/147:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    maxPow += 1\n    array = array.transpose()[0]\n    return np.power(np.array([array] * maxPow).transpose(), np.arange(maxPow))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = toPowMatrixAlt(X, 5)\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/148:\ndef toPowMatrix(array, maxPow):\n    if(maxPow == 0):\n        return [np.ones((array.size, 1))]\n    array = array.transpose()[0]\n    res = np.concatenate(([np.ones(array.size)], [array]))    \n    for i in range(2, maxPow + 1):\n        res = np.concatenate((res, [np.multiply(res[-1], array)]))\n    return res.transpose()\n\ndef toPowMatrixAlt(array, maxPow):\n    maxPow += 1\n    array = array.transpose()[0]\n    return np.power(np.array([array] * maxPow).transpose(), np.arange(maxPow))\n\nX = np.linspace(-2.2, 2.2, 1000)[:, np.newaxis]\nprint(f'Shape of X: {X.shape}')\n#X_p = toPowMatrix(X, 5)\nX_p = toPowMatrixAlt(X, 5)\nprint(f'Shape of X_p {X_p.shape}')\nassert X_p.shape == (1000, 6)\n\nplt.plot(\n    X[..., 0],\n    X_p @ np.array([[0, 4, 0, -5, 0, 1]]).T\n)\nplt.show()\n1/149:\ndef rastrigin(x, y):\n    return 20 + (x*2 − 10*math.cos(2*math.pi*x)) + (y*2 − 10*math.cos(2*math.pi*y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\n\nplt.figure(figsize=(10, 10))\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/150:\ndef rastrigin(x, y):\n    return 20 + (x*2 - 10*math.cos(2*math.pi*x)) + (y*2 - 10*math.cos(2*math.pi*y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\n\nplt.figure(figsize=(10, 10))\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/151:\ndef rastrigin(x, y):\n    return 20 + (x*2\n                 - 10*math.cos(2*math.pi*x))\n            + (y*2\n                - 10*math.cos(2*math.pi*y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\n\nplt.figure(figsize=(10, 10))\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/152:\ndef rastrigin(x, y):\n    return 20 + (x*2\n                 - 10*math.cos(2*math.pi*x))\n+ (y*2 - 10*math.cos(2*math.pi*y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\n\nplt.figure(figsize=(10, 10))\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/153:\ndef rastrigin(x, y):\n    return 20 + (x*2\n                 - 10*math.cos(2*math.pi*x))\n+ (y*2 \n   - 10*math.cos(2*math.pi*y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\n\nplt.figure(figsize=(10, 10))\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/154:\ndef rastrigin(x, y):\n    return [[1, 1], [2, 2]]\n    return 20 + (x*2 - 10*math.cos(2*math.pi*x))+ (y*2 - 10*math.cos(2 * math.pi * y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\n\nplt.figure(figsize=(10, 10))\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/155:\ndef rastrigin(x, y):\n    return [[2, 2], [2, 2]]\n    return 20 + (x*2 - 10*math.cos(2*math.pi*x))+ (y*2 - 10*math.cos(2 * math.pi * y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\n\nplt.figure(figsize=(10, 10))\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/156:\ndef rastrigin(x, y):\n    return [np.sin(x), y]\n    return 20 + (x*2 - 10*math.cos(2*math.pi*x))+ (y*2 - 10*math.cos(2 * math.pi * y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\n\nplt.figure(figsize=(10, 10))\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/157:\ndef rastrigin(x, y):\n    return 20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\n\nplt.figure(figsize=(10, 10))\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/158:\ndef rastrigin(x, y):\n    return [x, y]\n    return 20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\n\nplt.figure(figsize=(10, 10))\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/159:\ndef rastrigin(x, y):\n    return [x, x]\n    return 20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\n\nplt.figure(figsize=(10, 10))\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/160:\ndef rastrigin(x, y):\n    return [x, y]\n    return 20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\n\nplt.figure(figsize=(10, 10))\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/161:\ndef rastrigin(x, y):\n    return [x, y]\n    return 20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\n\nplt.figure(figsize=(10, 10))\nplt.plot(rastrigin(x, y))\n#plt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/162:\ndef rastrigin(x, y):\n    return [x, y]\n    return 20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\n\nplt.figure(figsize=(10, 10))\nplt.plot([[0, 0], [1, 1]])\n#plt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/163:\ndef rastrigin(x, y):\n    return [x, y]\n    return 20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\nprint(np.meshgrid(x, y))\n\nplt.figure(figsize=(10, 10))\nplt.plot([[0, 0], [1, 1]])\n#plt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/164:\ndef rastrigin(x, y):\n    return [x, y]\n    return 20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\nprint(np.meshgrid([1, 2], [3, 4]))\n\nplt.figure(figsize=(10, 10))\nplt.plot([[0, 0], [1, 1]])\n#plt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/165:\ndef rastrigin(x, y):\n    return [x, y]\n    return 20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\nprint(np.meshgrid([1, 2], [3, 4]))\n\nplt.figure(figsize=(10, 10))\nplt.plot([[0, 0], [1, 1]], 'o')\n#plt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/166:\ndef rastrigin(x, y):\n    return [x, y]\n    return 20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\nprint(np.meshgrid([1, 2], [3, 4]))\n\nplt.figure(figsize=(10, 10))\nplt.plot(np.meshgrid([1, 2], [3, 4]), 'o')\n#plt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/167:\ndef rastrigin(x, y):\n    return [x, y]\n    return 20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\nprint(np.meshgrid([1, 2], [3, 4]))\n\nplt.figure(figsize=(10, 10))\n    plt.plot([[1, 2], [3, 4], 'o')\n#plt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/168:\ndef rastrigin(x, y):\n    return [x, y]\n    return 20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\nprint(np.meshgrid([1, 2], [3, 4]))\n\nplt.figure(figsize=(10, 10))\nplt.plot([[1, 2], [3, 4], 'o')\n#plt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/169:\ndef rastrigin(x, y):\n    return [x, y]\n    return 20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\nprint(np.meshgrid([1, 2], [3, 4]))\n\nplt.figure(figsize=(10, 10))\nplt.plot([[1, 2], [3, 4]], 'o')\n#plt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/170:\ndef rastrigin(x, y):\n    return [x, y]\n    return 20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\nprint(np.meshgrid([1, 2], [3, 4]))\n\nplt.figure(figsize=(10, 10))\nplt.contourf([[1, 2], [3, 4]])\n#plt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/171:\ndef rastrigin(x, y):\n    return [x, y]\n    return 20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\nprint(np.meshgrid([1, 2], [3, 4]))\n\nplt.figure(figsize=(10, 10))\nplt.contourf([[0, 0], [1, 1]])\n#plt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/172:\ndef rastrigin(x, y):\n    return [x, y]\n    return 20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\nprint(np.meshgrid([1, 2], [3, 4]))\n\nplt.figure(figsize=(10, 10))\nplt.contourf([[0, 1], [1, 1]])\n#plt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/173:\ndef rastrigin(x, y):\n    return [x, y]\n    return 20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\nprint(np.meshgrid([1, 2], [3, 4]))\n\nplt.figure(figsize=(10, 10))\nplt.contourf([[0, 1], [1, 1], [5, 4]])\n#plt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/174:\ndef rastrigin(x, y):\n    return [x, y]\n    return 20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\nprint(np.meshgrid([1, 2], [3, 4]))\n\nplt.figure(figsize=(10, 10))\nplt.contourf([[0, 1, 5], [1, 1, 4], [5, 4, 3]])\n#plt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/175:\ndef rastrigin(x, y):\n    return np.meshgrid(x, y, 20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y)))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\nprint(np.meshgrid([1, 2], [3, 4]))\n\nplt.figure(figsize=(10, 10))\nplt.contourf([[0, 1, 5], [1, 1, 4], [5, 4, 3]])\n#plt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/176:\ndef rastrigin(x, y):\n    return np.meshgrid(x, y, 20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y)))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\nprint(np.meshgrid([1, 2], [3, 4]))\n\nplt.figure(figsize=(10, 10))\n#plt.contourf([[0, 1, 5], [1, 1, 4], [5, 4, 3]])\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/177:\ndef rastrigin(x, y):\n    print(20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y)))\n    return np.meshgrid(x, y,\n                       20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y)))\n\nx = np.linspace(-5, 5, 10)\ny = np.linspace(-5, 5, 10)\nprint(np.meshgrid([1, 2], [3, 4]))\n\nplt.figure(figsize=(10, 10))\n#plt.contourf([[0, 1, 5], [1, 1, 4], [5, 4, 3]])\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/178:\ndef rastrigin(x, y):\n    print(20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y)))\n    return np.meshgrid(x, y,\n                       20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y)))\n\nx = np.linspace(-5, 5, 2)\ny = np.linspace(-5, 5, 2)\nprint(np.meshgrid([1, 2], [3, 4]))\n\nplt.figure(figsize=(10, 10))\n#plt.contourf([[0, 1, 5], [1, 1, 4], [5, 4, 3]])\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/179:\ndef rastrigin(x, y):\n    return np.meshgrid(x, y,\n                       20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y)))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\nprint(np.meshgrid([1, 2], [3, 4]))\n\nplt.figure(figsize=(10, 10))\n#plt.contourf([[0, 1, 5], [1, 1, 4], [5, 4, 3]])\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/180:\ndef rastrigin(x, y):\n    return np.meshgrid(x, y)\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\nprint(np.meshgrid([1, 2], [3, 4]))\n\nplt.figure(figsize=(10, 10))\n#plt.contourf([[0, 1, 5], [1, 1, 4], [5, 4, 3]])\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/181:\ndef rastrigin(x, y):\n    return np.meshgrid(x, y)\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\n\nplt.figure(figsize=(10, 10))\n#plt.contourf([[0, 1, 5], [1, 1, 4], [5, 4, 3]])\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/182:\ndef rastrigin(x, y):\n    return np.meshgrid(x, y,\n                       20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y)))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\nprint(x)\n\nplt.figure(figsize=(10, 10))\n#plt.contourf([[0, 1, 5], [1, 1, 4], [5, 4, 3]])\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n1/183:\ndef rastrigin(x, y):\n    return np.meshgrid(x, y,\n                       20 + (x*2 - 10*np.cos(2*math.pi*x))+ (y*2 - 10*np.cos(2 * math.pi * y)))\n\nx = np.linspace(-5, 5, 10)\ny = np.linspace(-5, 5, 10)\nprint(x)\n\nplt.figure(figsize=(10, 10))\n#plt.contourf([[0, 1, 5], [1, 1, 4], [5, 4, 3]])\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n 3/1:\nimport numpy as np\nimport random\nimport math\nimport matplotlib.pyplot as plt\n\nv = np.array([0, 1])\na = random.random() * 2 * math.pi\nM = np.array([[math.cos(a), -math.sin(a)],[math.sin(a), math.cos(a)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M) * 0.99\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n 3/2:\nimport numpy as np\nimport random\nimport math\nimport matplotlib.pyplot as plt\n\nv = np.array([0, 1])\na = random.random() * 2 * math.pi\nM = np.array([[math.cos(a), -math.sin(a)],[math.sin(a), math.cos(a)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M) * 0.99\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n 3/3:\nimport numpy as np\nimport random\nimport math\nimport matplotlib.pyplot as plt\n\nv = np.array([0, 1])\na = random.random() * 2 * math.pi\nM = np.array([[math.cos(a), -math.sin(a)],[math.sin(a), math.cos(a)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M) * 0.99\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n 3/4:\nimport numpy as np\nimport random\nimport math\nimport matplotlib.pyplot as plt\n\nv = np.array([0, 1])\na = random.random() * 2 * math.pi\nM = np.array([[math.cos(a), -math.sin(a)],[math.sin(a), math.cos(a)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M) * 0.99\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n 3/5:\nimport numpy as np\nimport random\nimport math\nimport matplotlib.pyplot as plt\n\nv = np.array([0, 1])\na = random.random() * 2 * math.pi\nM = np.array([[math.cos(a), -math.sin(a)],[math.sin(a), math.cos(a)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M) * 0.99\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n 3/6:\nimport numpy as np\nimport random\nimport math\nimport matplotlib.pyplot as plt\n\nv = np.array([0, 1])\na = random.random() * 2 * math.pi\nM = np.array([[math.cos(a), -math.sin(a)],[math.sin(a), math.cos(a)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M) * 0.99\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n 3/7:\nimport numpy as np\nimport random\nimport math\nimport matplotlib.pyplot as plt\n\nv = np.array([0, 1])\na = random.random() * 2 * math.pi\nM = np.array([[math.cos(a), -math.sin(a)],[math.sin(a), math.cos(a)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M) * 0.99\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n 3/8:\nimport numpy as np\nimport random\nimport math\nimport matplotlib.pyplot as plt\n\nv = np.array([0, 1])\na = random.random() * 2 * math.pi\nM = np.array([[math.cos(a), -math.sin(a)],[math.sin(a), math.cos(a)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M) * 0.99\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n 3/9:\nimport numpy as np\nimport random\nimport math\nimport matplotlib.pyplot as plt\n\nv = np.array([0, 1])\na = random.random() * 2 * math.pi\nM = np.array([[math.cos(a), -math.sin(a)],[math.sin(a), math.cos(a)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M) * 0.99\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n3/10:\nimport numpy as np\nimport random\nimport math\nimport matplotlib.pyplot as plt\n\nv = np.array([0, 1])\na = random.random() * 2 * math.pi\nM = np.array([[math.cos(a), -math.sin(a)],[math.sin(a), math.cos(a)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M) * 0.99\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n3/11:\nimport numpy as np\nimport random\nimport math\nimport matplotlib.pyplot as plt\n\nv = np.array([0, 1])\na = random.random() * 2 * math.pi\nM = np.array([[math.cos(a), -math.sin(a)],[math.sin(a), math.cos(a)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M) * 0.99\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n3/12:\nimport numpy as np\nimport random\nimport math\nimport matplotlib.pyplot as plt\n\nv = np.array([0, 1])\na = random.random() * 2 * math.pi\nM = np.array([[math.cos(a), -math.sin(a)],[math.sin(a), math.cos(a)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M) * 0.99\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n3/13:\nimport numpy as np\nimport random\nimport math\nimport matplotlib.pyplot as plt\n\nv = np.array([0, 1])\na = random.random() * 2 * math.pi\nM = np.array([[math.cos(a), -math.sin(a)],[math.sin(a), math.cos(a)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M) * 0.99\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n 4/1:\nimport random\n\nalpha = random.random()\nv = np.array([0, 1])\nM = np.array([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    vs.append(np.squeeze(v))\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n 4/2:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n 4/3:\nimport random\n\nalpha = random.random()\nv = np.array([0, 1])\nM = np.array([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    vs.append(np.squeeze(v))\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n 4/4:\nimport random\n\nalpha = random.random()\nv = np.array([0, 1])\nM = np.array([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    vs.append(np.squeeze(v))\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n 4/5:\nimport random\n\nalpha = random.random()\nv = np.array([0, 1])\nM = np.array([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    vs.append(np.squeeze(v))\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n 4/6:\nimport random\n\nalpha = random.random()\nv = np.array([0, 1])\nM = np.array([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    vs.append(np.squeeze(v))\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n 4/7:\nimport random\n\nalpha = random.random()\nv = np.array([0, 1])\nM = np.array([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    vs.append(np.squeeze(v))\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n 4/8:\nimport random\n\nalpha = random.random()\nv = np.array([0, 1])\nM = np.array([[math.cos(alpha), -math.sin(alpha)],\n              [math.sin(alpha), math.cos(alpha)]]) * 0.99\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M)\n    vs.append(np.squeeze(v))\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n3/14:\nimport numpy as np\nimport random\nimport math\nimport matplotlib.pyplot as plt\n\nv = np.array([0, 1])\na = random.random() * 2 * math.pi\nM = np.array([[math.cos(a), -math.sin(a)],[math.sin(a), math.cos(a)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M) * 0.99\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n3/15:\nimport numpy as np\nimport random\nimport math\nimport matplotlib.pyplot as plt\n\nv = np.array([0, 1])\na = random.random() * 2 * math.pi\nM = np.array([[math.cos(a), -math.sin(a)],[math.sin(a), math.cos(a)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M) * 0.99\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n3/16:\nimport numpy as np\nimport random\nimport math\nimport matplotlib.pyplot as plt\n\nv = np.array([0, 1])\na = random.random() * 2 * math.pi\nM = np.array([[math.cos(a), -math.sin(a)],[math.sin(a), math.cos(a)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M) * 0.99\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n3/17:\nimport numpy as np\nimport random\nimport math\nimport matplotlib.pyplot as plt\n\nv = np.array([0, 1])\na = random.random() * 2 * math.pi\nM = np.array([[math.cos(a), -math.sin(a)],[math.sin(a), math.cos(a)]])\n\nvs = [v]\nfor i in range(100):\n    v = v.dot(M) * 0.99\n    vs.append(v)\n    \nvs = np.array(vs)\n\nplt.figure(figsize=(6, 6))\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.plot(*vs.T)\nplt.show()\n 4/9:\ndef rastrigin(x, y):\n    X, Y = np.meshgrid(x, y)\n    return 20 + (X**2 - 10*np.cos(2*math.pi*X))+ (Y**2 - 10*np.cos(2 * math.pi * Y)))\n\nx = np.linspace(-5, 5, 10)\ny = np.linspace(-5, 5, 10)\nprint(x)\n\nplt.figure(figsize=(10, 10))\n#plt.contourf([[0, 1, 5], [1, 1, 4], [5, 4, 3]])\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n4/10:\ndef rastrigin(x, y):\n    X, Y = np.meshgrid(x, y)\n    return 20 + (X**2 - 10*np.cos(2*math.pi*X))+ (Y**2 - 10*np.cos(2 * math.pi * Y))\n\nx = np.linspace(-5, 5, 10)\ny = np.linspace(-5, 5, 10)\nprint(x)\n\nplt.figure(figsize=(10, 10))\n#plt.contourf([[0, 1, 5], [1, 1, 4], [5, 4, 3]])\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n4/11:\ndef rastrigin(x, y):\n    X, Y = np.meshgrid(x, y)\n    return 20 + (X**2 - 10*np.cos(2*math.pi*X))+ (Y**2 - 10*np.cos(2 * math.pi * Y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\nprint(x)\n\nplt.figure(figsize=(10, 10))\n#plt.contourf([[0, 1, 5], [1, 1, 4], [5, 4, 3]])\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n4/12:\ndef rastrigin(x, y):\n    X, Y = np.meshgrid(x, y)\n    return 20 + (X**2 - 10*np.cos(2*math.pi*X))+ (Y**2 - 10*np.cos(2 * math.pi * Y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\n\nplt.figure(figsize=(10, 10))\n#plt.contourf([[0, 1, 5], [1, 1, 4], [5, 4, 3]])\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n4/13:\ndef rastrigin(x, y):\n    X, Y = np.meshgrid(x, y)\n    return 20 + (X**2 - 10*np.cos(2*math.pi*X))+ (Y**2 - 10*np.cos(2 * math.pi * Y))\n\nx = np.linspace(-5, 5, 500)\ny = np.linspace(-5, 5, 500)\n\nplt.figure(figsize=(10, 10))\n#plt.contourf([[0, 1, 5], [1, 1, 4], [5, 4, 3]])\nplt.contourf(rastrigin(x, y), levels=10)\nplt.show()\n4/14:\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoidEach(m):\n    return list(map(sigmoid, m))\n\nx = np.linspace(-5, 5, 100)\ny = sigmoid(x)\n\nplt.plot(x, y)\nplt.ylim(0, 1)\nplt.show()\n 5/1:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n 5/2:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n 5/3:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n 7/1:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n 7/2:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n 7/3: data = pd.read_csv('data.csv')\n 7/4: data = pd.read_csv('data.csv')\n 7/5:\ndata = pd.read_csv('data.csv')\nx, y = data.values[:, :-2].astype(np.float32), data.values[:, -2:-1].astype(np.float32)\n\nnp.random.seed(1337)\nis_train = np.random.uniform(size=(x.shape[0],)) < 0.95\n\nx_train, y_train = x[is_train], y[is_train]\nx_test, y_test = x[~is_train], y[~is_train]\n\nprint(f'Train samples: {len(x_train)}')\nprint(f'Test samples: {len(x_test)}')\n 6/1:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n 9/1:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n 9/2:\ndata = pd.read_csv('data.csv')\ndata\n 9/3:\nx, y = data.values[:, :-2].astype(np.float32), data.values[:, -2:-1].astype(np.float32)\n\nnp.random.seed(1337)\nis_train = np.random.uniform(size=(x.shape[0],)) < 0.95\n\nx_train, y_train = x[is_train], y[is_train]\nx_test, y_test = x[~is_train], y[~is_train]\n\nprint(f'Train samples: {len(x_train)}')\nprint(f'Test samples: {len(x_test)}')\n 9/4:\nx, y = data.values[:, :-2].astype(np.float32), data.values[:, -2:-1].astype(np.float32)\n\nnp.random.seed(1337)\nis_train = np.random.uniform(size=(x.shape[0],)) < 0.95\n\nx_train, y_train = x[is_train], y[is_train]\nx_test, y_test = x[~is_train], y[~is_train]\n\nprint(f'Train samples: {len(x_train)}')\nprint(f'Test samples: {len(x_test)}')\n 7/6:\ndef toPowMatrix(matrix, p):\n    res = list()\n    res.append(np.ones((array.size, 1)).transpose())\n    return res\n\ntoPowMatrix(null, 2)\n 7/7:\ndef toPowMatrix(matrix, p):\n    res = list()\n    res.append(np.ones((array.size, 1)).transpose())\n    return res\n\ntoPowMatrix([], 2)\n 7/8:\ndef toPowMatrix(matrix, p):\n    res = list()\n    res.append(np.ones((matrix.size, 1)).transpose())\n    return res\n\ntoPowMatrix([1, 2, 3], 2)\n 7/9:\ndef toPowMatrix(matrix, p):\n    res = list()\n    res.append(np.ones((matrix.size, 1)).transpose())\n    return res\n\ntoPowMatrix(np.matrix([1, 2, 3]), 2)\n7/10:\ndef toPowMatrix(matrix, p):\n    res = list()\n    res.append(np.ones((matrix.size, 1)).transpose())\n    res.append(matrix)   \n    for i in range(2, maxPow + 1):\n        res.append(np.multiply(res[-1], matrix)))\n    return np.concatenate(res)\n\ntoPowMatrix(np.matrix([1, 2, 3]), 2)\n7/11:\ndef toPowMatrix(matrix, p):\n    res = list()\n    res.append(np.ones((matrix.size, 1)).transpose())\n    res.append(matrix)   \n    for i in range(2, maxPow + 1):\n        res.append(np.multiply(res[-1], matrix))\n    return np.concatenate(res)\n\ntoPowMatrix(np.matrix([1, 2, 3]), 2)\n7/12:\ndef toPowMatrix(matrix, p):\n    res = list()\n    res.append(np.ones((matrix.size, 1)).transpose())\n    res.append(matrix)   \n    for i in range(2, p + 1):\n        res.append(np.multiply(res[-1], matrix))\n    return np.concatenate(res)\n\ntoPowMatrix(np.matrix([1, 2, 3]), 2)\n7/13:\ndef toPowMatrix(matrix, p):\n    res = list()\n    res.append(np.ones((matrix.size, 1)).transpose())\n    res.append(matrix)   \n    for i in range(2, p + 1):\n        res.append(np.multiply(res[-1], matrix))\n    return np.concatenate(res)\n\ntoPowMatrix(np.matrix([[1, 2, 3], [4, 5, 6]]), 2)\n7/14:\ndef toPowMatrix(matrix, p):\n    res = list()\n    res.append(np.ones((matrix.size, 1)).transpose())\n    res.append(matrix)   \n    for i in range(2, p + 1):\n        res.append(np.multiply(res[-1], matrix))\n    print(res)\n    return np.concatenate(res)\n\ntoPowMatrix(np.matrix([[1, 2, 3], [4, 5, 6]]), 2)\n7/15:\ndef toPowMatrix(matrix, p):\n    res = list()\n    res.append(np.ones(matrix.shape).transpose())\n    res.append(matrix)   \n    for i in range(2, p + 1):\n        res.append(np.multiply(res[-1], matrix))\n    print(res)\n    return np.concatenate(res)\n\ntoPowMatrix(np.matrix([[1, 2, 3], [4, 5, 6]]), 2)\n7/16:\ndef toPowMatrix(matrix, p):\n    res = list()\n    res.append(np.ones(matrix.shape))\n    res.append(matrix)   \n    for i in range(2, p + 1):\n        res.append(np.multiply(res[-1], matrix))\n    print(res)\n    return np.concatenate(res)\n\ntoPowMatrix(np.matrix([[1, 2, 3], [4, 5, 6]]), 2)\n7/17:\ndef toPowMatrix(matrix, p):\n    res = list()\n    res.append(np.ones(matrix.shape))\n    res.append(matrix)   \n    for i in range(2, p + 1):\n        res.append(np.multiply(res[-1], matrix))\n    print(res)\n    return np.concatenate(res).transpose()\n\ntoPowMatrix(np.matrix([[1, 2, 3], [4, 5, 6]]), 2)\n7/18:\ndef toPowMatrix(matrix, p):\n    res = list()\n    res.append(np.ones(matrix.shape))\n    res.append(matrix)   \n    for i in range(2, p + 1):\n        res.append(np.multiply(res[-1], matrix))\n    print(res)\n    return np.concatenate(res)\n\ntoPowMatrix(np.matrix([[1, 2, 3], [4, 5, 6]]), 2)\n7/19:\ndef toPowMatrix(matrix, p):\n    res = list()\n    res.append(np.ones(matrix.shape))\n    res.append(matrix)   \n    for i in range(2, p + 1):\n        res.append(np.multiply(res[-1], matrix))\n    print(res)\n    return np.concatenate(res, axis = 1)\n\ntoPowMatrix(np.matrix([[1, 2, 3],\n                       [4, 5, 6]]), 2)\n7/20:\ndef toPowMatrix(matrix, p):\n    res = list()\n    res.append(np.ones(matrix.shape))\n    res.append(matrix)   \n    for i in range(2, p + 1):\n        res.append(np.multiply(res[-1], matrix))\n    return np.concatenate(res, axis = 1)\n\ntoPowMatrix(np.matrix([[1, 2, 3],\n                       [4, 5, 6]]), 2)\n7/21:\ndef toPowMatrix(matrix, p):\n    res = list()\n    res.append(np.ones(matrix.shape))\n    res.append(matrix)   \n    for i in range(2, p + 1):\n        res.append(np.multiply(res[-1], matrix))\n    return np.concatenate(res, axis = 1)\n\ntoPowMatrix(np.matrix([[1, 2, 3],\n                       [4, 5, 6]]), 2)\n7/22: np.arrange(16).reshape(2, 8)\n7/23: np.arange(16).reshape(2, 8)\n7/24:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\n7/25:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\nnp.zeroes(np.shape.y)\n7/26:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\nnp.zeroes(x.shape.y)\n7/27:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\nnp.zero(x.shape.y)\n7/28:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\nnp.zeros(x.shape.y)\n7/29:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\nnp.zeros(x.shape[0])\n7/30:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\nnp.zeros((1, x.shape[0]))\n7/31:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\nnp.zeros((0, x.shape[0]))\n7/32:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\nnp.zeros((x.shape[0], 1))\n7/33:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\nnp.concatenate(np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1))\n7/34:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\nnp.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)))\n7/35:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\nnp.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\n7/36:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\nnp.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\nx\n7/37:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\nnp.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\nx.mean(axis = 1)\n7/38:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\nnp.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\nx\n7/39:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\nnp.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\nx + [0, 1]\n7/40:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\nnp.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\nx + [[0], [1]]\n7/41:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\nnp.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\nx.mean(axis = 0)\n7/42:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\nnp.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\nx.mean(axis = 0)\nnp.zeroes((x.shape[0], 1))\n7/43:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\nnp.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\nx.mean(axis = 0)\nnp.zeros((x.shape[0], 1))\n7/44:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\nnp.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\nx.mean(axis = 0)\nnp.zeros((x.shape[0], 2))\n7/45:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\nnp.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\n#x.mean(axis = 0)\n#np.zeros((x.shape[0], 2))\n7/46:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\n#np.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\n#x.mean(axis = 0)\n#np.zeros((x.shape[0], 2))\nnp.random.rand(3)\n7/47:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\n#np.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\n#x.mean(axis = 0)\n#np.zeros((x.shape[0], 2))\nnp.random.randint(3, 10)\n7/48:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\n#np.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\n#x.mean(axis = 0)\n#np.zeros((x.shape[0], 2))\nnp.random.randint(3, size=10)\n7/49:\nx = np.arange(16).reshape(2, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\n#np.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\n#x.mean(axis = 0)\n#np.zeros((x.shape[0], 2))\nnp.random.choice(20, size=10, replace=False)\n7/50:\nx = np.arange(16).reshape(3, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\n#np.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\n#x.mean(axis = 0)\n#np.zeros((x.shape[0], 2))\n#np.random.choice(20, size=10, replace=False)\nx[0]\n7/51:\nx = np.arange(25).reshape(3, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\n#np.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\n#x.mean(axis = 0)\n#np.zeros((x.shape[0], 2))\n#np.random.choice(20, size=10, replace=False)\nx[0]\n7/52:\nx = np.arange(24).reshape(3, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\n#np.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\n#x.mean(axis = 0)\n#np.zeros((x.shape[0], 2))\n#np.random.choice(20, size=10, replace=False)\nx[0]\n7/53:\nx = np.arange(24).reshape(3, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\n#np.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\n#x.mean(axis = 0)\n#np.zeros((x.shape[0], 2))\n#np.random.choice(20, size=10, replace=False)\nnp.concatenate((x[0], x[1]))\n7/54:\nx = np.arange(24).reshape(3, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\n#np.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\n#x.mean(axis = 0)\n#np.zeros((x.shape[0], 2))\n#np.random.choice(20, size=10, replace=False)\nnp.concatenate((x[0], x[1]), axis= 1)\n7/55:\nx = np.arange(24).reshape(3, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\n#np.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\n#x.mean(axis = 0)\n#np.zeros((x.shape[0], 2))\n#np.random.choice(20, size=10, replace=False)\nnp.concatenate((x[0], x[1]), axis=0)\n7/56:\nx = np.arange(24).reshape(3, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\n#np.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\n#x.mean(axis = 0)\n#np.zeros((x.shape[0], 2))\n#np.random.choice(20, size=10, replace=False)\nnp.concatenate(([x[0]], [x[1]]), axis=1)\n7/57:\nx = np.arange(24).reshape(3, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\n#np.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\n#x.mean(axis = 0)\n#np.zeros((x.shape[0], 2))\n#np.random.choice(20, size=10, replace=False)\nnp.concatenate(([x[0]], [x[1]]), axis=0)\n7/58:\nx = np.arange(24).reshape(3, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\n#np.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\n#x.mean(axis = 0)\n#np.zeros((x.shape[0], 2))\n#np.random.choice(20, size=10, replace=False)\nnp.concatenate(([x[0]], [x[1]]))\n 9/5:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n        self.k = 0\n        self.b = 0\n    \n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    \n    def h(x):\n        return sigmoid(x*self.k + self.b)\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu / self.sigma)\n        return np.concatenate(np.ones(((X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 1))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() * (y - x * theta) + self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = #TODO\n        v_t = v_1\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = #TODO\n            theta = #TODO\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n 9/6:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n 9/7:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n 9/8:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n        self.k = 0\n        self.b = 0\n    \n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    \n    def h(x):\n        return sigmoid(x*self.k + self.b)\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu / self.sigma)\n        return np.concatenate(np.ones(((X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 1))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() * (y - x * theta) + self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = #TODO\n        v_t = v_1\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = #TODO\n            theta = #TODO\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/1:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu / self.sigma)\n        return np.concatenate(np.ones(((X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 1))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/2:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/3:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport math\n12/4:\ndata = pd.read_csv('data.csv')\ndata\n12/5:\nx, y = data.values[:, :-2].astype(np.float32), data.values[:, -2:-1].astype(np.float32)\n\nnp.random.seed(1337)\nis_train = np.random.uniform(size=(x.shape[0],)) < 0.95\n\nx_train, y_train = x[is_train], y[is_train]\nx_test, y_test = x[~is_train], y[~is_train]\n\nprint(f'Train samples: {len(x_train)}')\nprint(f'Test samples: {len(x_test)}')\n12/6:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu / self.sigma)\n        return np.concatenate(np.ones(((X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 1))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/7:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/8:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu / self.sigma)\n        return np.concatenate(np.ones((X.shape[0],1)), X)), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 1))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/9:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu / self.sigma)\n        return np.concatenate(np.ones(shape=(X.shape[0],1)), X)), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 1))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/10:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu / self.sigma)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 1))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/11:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/12:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        print(x)\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu / self.sigma)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 1))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/13:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/14:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        print(x.shape)\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu / self.sigma)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 1))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/15:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/16:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        print(x.shape)\n        print(self.mu.shape)\n        print(self.sigma.shape)\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu / self.sigma)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 1))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/17:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/18:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        print(x.shape)\n        print(self.mu.shape)\n        print(self.sigma.shape)\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu / self.sigma)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/19:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/20:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        print(x.shape)\n        print(self.mu.shape)\n        print(self.sigma == 0)\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu / self.sigma)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/21:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/22:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu / sigma)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/23:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/24:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu / sigma)\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/25:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/26:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu / sigma)\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        print(x.shape)\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/27:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/28:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu / sigma)\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 1), np.std(X, axis = 1))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        print(x.shape)\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/29:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n13/1:\nx = np.arange(24).reshape(3, 8)\nprint(x)\nprint(np.delete(x, 0, axis = 1))\n#np.concatenate((np.zeros((x.shape[0], 1)), np.delete(x, 0, axis = 1)), axis=1)\n#x.mean(axis = 0)\n#np.zeros((x.shape[0], 2))\n#np.random.choice(20, size=10, replace=False)\nnp.concatenate(([x[0]], [x[1]]))\n13/2:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n13/3: y = np.arrange(24).reshape(3, 6)\n13/4: y = np.arange(24).reshape(3, 6)\n13/5: y = np.arange(18).reshape(3, 6)\n13/6:\ny = np.arange(18).reshape(3, 6)\ny\n13/7:\ny = np.arange(18).reshape(3, 6)\ny - np.arange(6)\n13/8:\ny = np.arange(18).reshape(3, 6)\ny\n13/9:\ny = np.arange(18).reshape(3, 6)\ny / np.arange(6)\n13/10:\ny = np.arange(18).reshape(3, 6)\ny / np.arange(6) + 1\n13/11:\ny = np.arange(18).reshape(3, 6)\ny / (np.arange(6) + 1)\n12/30:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        print(X.shape)\n        print(sigma.shape)\n        print(self.mu.shape)\n        X = (X - self.mu / sigma)\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 1), np.std(X, axis = 1))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        print(x.shape)\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/31:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/32:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        print(X.shape)\n        print(sigma.shape)\n        print(self.mu.shape)\n        X = (X - self.mu / sigma)\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        print(x.shape)\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/33:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/34:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        print(X.shape)\n        print(sigma.shape)\n        print(self.mu.shape)\n        X = (X - self.mu / sigma)\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        print(x.shape)\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        print(x[:, 1:].mean(axis=0))\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/35:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/36:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        print(X.shape)\n        print(sigma.shape)\n        print(self.mu.shape)\n        X = (X - self.mu / sigma)\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        print(\"before: \" + x.shape)\n        x = self.preprocess(x)\n        print(\"before: \" + x.shape)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        print(x[:, 1:].mean(axis=0))\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/37:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/38:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        print(X.shape)\n        print(sigma.shape)\n        print(self.mu.shape)\n        X = (X - self.mu / sigma)\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        print(\"before: \" + str(x.shape))\n        x = self.preprocess(x)\n        print(\"before: \" + str(x.shape))\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        print(x[:, 1:].mean(axis=0))\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/39:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/40:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        print(len(res))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        print(X.shape)\n        print(sigma.shape)\n        print(self.mu.shape)\n        X = (X - self.mu / sigma)\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        print(\"before: \" + str(x.shape))\n        x = self.preprocess(x)\n        print(\"before: \" + str(x.shape))\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        print(x[:, 1:].mean(axis=0))\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/41:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/42:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        print(len(res))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        print(X.shape)\n        print(sigma.shape)\n        print(self.mu.shape)\n        X = (X - self.mu / sigma)\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        print(\"before: \" + str(x.shape))\n        x = self.preprocess(x)\n        print(\"before: \" + str(x.shape))\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        print(x[:, 1:].mean(axis=0))\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/43:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/44:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape[1]))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        print(X.shape)\n        print(sigma.shape)\n        print(self.mu.shape)\n        X = (X - self.mu / sigma)\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        print(\"before: \" + str(x.shape))\n        x = self.preprocess(x)\n        print(\"preprocess: \" + str(x.shape))\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/45:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/46:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones(x.shape[1], axis=1))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        print(X.shape)\n        print(sigma.shape)\n        print(self.mu.shape)\n        X = (X - self.mu / sigma)\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        print(\"before: \" + str(x.shape))\n        x = self.preprocess(x)\n        print(\"preprocess: \" + str(x.shape))\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/47:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/48:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((1, x.shape[1])))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        print(X.shape)\n        print(sigma.shape)\n        print(self.mu.shape)\n        X = (X - self.mu / sigma)\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        print(\"before: \" + str(x.shape))\n        x = self.preprocess(x)\n        print(\"preprocess: \" + str(x.shape))\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/49:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n13/12:\ny = np.arange(18).reshape(3, 6)\ny / (np.arange(6) + 1)\n13/13: np.ones((1, 4))\n13/14: np.ones((1, 4), axis=1)\n13/15: np.ones((4, 1), axis=1)\n13/16: np.ones((4, 1))\n12/50:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[1], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        print(X.shape)\n        print(sigma.shape)\n        print(self.mu.shape)\n        X = (X - self.mu / sigma)\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        print(\"before: \" + str(x.shape))\n        x = self.preprocess(x)\n        print(\"preprocess: \" + str(x.shape))\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/51:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/52:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        print(X.shape)\n        print(sigma.shape)\n        print(self.mu.shape)\n        X = (X - self.mu / sigma)\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        print(\"before: \" + str(x.shape))\n        x = self.preprocess(x)\n        print(\"preprocess: \" + str(x.shape))\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/53:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/54:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        print(X.shape)\n        print(sigma.shape)\n        print(self.mu.shape)\n        X = (X - self.mu / sigma)\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        print(\"before: \" + str(x.shape))\n        x = self.preprocess(x)\n        print(\"preprocess: \" + str(x.shape))\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        print(x[:, 1:].mean(axis=0))\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/55:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/56:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        print(X.shape)\n        print(sigma.shape)\n        print(self.mu.shape)\n        X = (X - self.mu / sigma)\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        print(\"before: \" + str(x.shape))\n        x = self.preprocess(x)\n        print(\"preprocess: \" + str(x.shape))\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        print(x[:, 1:])\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/57:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        print(X.shape)\n        print(sigma.shape)\n        print(self.mu.shape)\n        X = (X - self.mu / sigma)\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        print(\"before: \" + str(x.shape))\n        x = self.preprocess(x)\n        print(\"preprocess: \" + str(x.shape))\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        print(x[:, 1:].shape)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/58:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/59:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        print(X.shape)\n        print(sigma.shape)\n        print(self.mu.shape)\n        X = (X - self.mu / sigma)\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        print(\"before: \" + str(x.shape))\n        x = self.preprocess(x)\n        print(\"preprocess: \" + str(x.shape))\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        print(x[:, 1:])\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/60:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu / sigma)\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        print(\"before: \" + str(x.shape))\n        x = self.preprocess(x)\n        print(\"preprocess: \" + str(x.shape))\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        print(x[:, 1:])\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/61:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/62:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + np.eye(x.shape) @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        print(\"before: \" + str(x.shape))\n        x = self.preprocess(x)\n        print(\"preprocess: \" + str(x.shape))\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        print(x[:, 1:])\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/63:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/64:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(x.shape[0])\n        eye[0,0] = 0\n        return -2 / x.size * x.transpose() @ (y - x @ theta) + eye @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/65:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/66:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(x.shape[0])\n        eye[0,0] = 0\n        return -2 / x.size\n    * x.transpose() @ (y - x @ theta)\n    + eye\n    @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/67:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(x.shape[0])\n        eye[0,0] = 0\n        return -2 / x.size\n               * x.transpose() @ (y - x @ theta)\n               + eye\n               @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/68:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(x.shape[0])\n        eye[0,0] = 0\n        return -2 / x.size * x.transpose() @ (y - x @ theta)\n               + eye @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/69:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(x.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose() @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta) + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/70:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(x.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye\n                @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/71:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/72:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(x.shape[0])\n        eye[0,0] = 0\n        2 * self.alpha2 * theta\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye\n                @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/73:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/74:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(x.shape[0])\n        eye[0,0] = 0\n        print((2 * self.alpha2 * theta).shape)\n        print((self.alpha1 * np.sign(theta)).shape)\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye\n                @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/75:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/76:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(x.shape[0])\n        eye[0,0] = 0\n        print(eye.shape)\n        print((2 * self.alpha2 * theta).shape)\n        print((self.alpha1 * np.sign(theta)).shape)\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye\n                @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/77:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/78:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        print(eye.shape)\n        print((2 * self.alpha2 * theta).shape)\n        print((self.alpha1 * np.sign(theta)).shape)\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye\n                @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/79:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/80:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye\n                @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/81:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/82:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta) @ eye)\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/83:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/84:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/85:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/86:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport math\n12/87:\ndata = pd.read_csv('data.csv')\ndata\n12/88:\nx, y = data.values[:, :-2].astype(np.float32), data.values[:, -2:-1].astype(np.float32)\n\nnp.random.seed(1337)\nis_train = np.random.uniform(size=(x.shape[0],)) < 0.95\n\nx_train, y_train = x[is_train], y[is_train]\nx_test, y_test = x[~is_train], y[~is_train]\n\nprint(f'Train samples: {len(x_train)}')\nprint(f'Test samples: {len(x_test)}')\n12/89:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/90:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/91:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            print(\"yes\")\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.abs(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/92:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/93:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        print(np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1).shape)\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.abs(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/94:\nreg = PolynomialRegression(0, 0, 2, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/95:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/96:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = h(x, theta)\n        return np.abs(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/97:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/98:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = self.h(x, theta)\n        return np.abs(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/99:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/100:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(self, x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = self.h(x, theta)\n        return np.abs(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/101:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/102:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/103:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/104:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/105:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/106:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/107:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/108:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/109:\nreg = PolynomialRegression(0, 0, 2, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n12/110:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(self, x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = self.h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n12/111:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n14/1:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport math\n14/2:\ndata = pd.read_csv('data.csv')\ndata\n14/3:\nx, y = data.values[:, :-2].astype(np.float32), data.values[:, -2:-1].astype(np.float32)\n\nnp.random.seed(1337)\nis_train = np.random.uniform(size=(x.shape[0],)) < 0.95\n\nx_train, y_train = x[is_train], y[is_train]\nx_test, y_test = x[~is_train], y[~is_train]\n\nprint(f'Train samples: {len(x_train)}')\nprint(f'Test samples: {len(x_test)}')\n14/4:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(self, x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = self.h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n14/5:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n14/6:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n14/7:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport math\nimport tqdm\n14/8:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nscoreFunc = lambda alpha1, alpha2:\n    #reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    #return reg.score(x_test, y_test)\n    return 3 * alpha1 * alpha2\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = someFunc(tqdm(alpha1Array))\nprint(alpha1Score)\n\nfig, ax = plt.subplots(ncols=2, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot([1e-3, 1e-2, 1e-1, 1], [1, 0.5, 0.3, 2])\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot([1e-3, 1e-2, 1e-1, 1], [1, 0.5, 0.3, 2])\nplt.show()\n14/9:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    #reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    #return reg.score(x_test, y_test)\n    return 3 * alpha1 * alpha2\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = someFunc(tqdm(alpha1Array))\nprint(alpha1Score)\n\nfig, ax = plt.subplots(ncols=2, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot([1e-3, 1e-2, 1e-1, 1], [1, 0.5, 0.3, 2])\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot([1e-3, 1e-2, 1e-1, 1], [1, 0.5, 0.3, 2])\nplt.show()\n14/10:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    #reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    #return reg.score(x_test, y_test)\n    return 3 * alpha1 * alpha2\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = scoreFunc(tqdm(alpha1Array))\nprint(alpha1Score)\n\nfig, ax = plt.subplots(ncols=2, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot([1e-3, 1e-2, 1e-1, 1], [1, 0.5, 0.3, 2])\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot([1e-3, 1e-2, 1e-1, 1], [1, 0.5, 0.3, 2])\nplt.show()\n14/11:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    #reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    #return reg.score(x_test, y_test)\n    return 3 * alpha1 * alpha2\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array))\nprint(alpha1Score)\n\nfig, ax = plt.subplots(ncols=2, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot([1e-3, 1e-2, 1e-1, 1], [1, 0.5, 0.3, 2])\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot([1e-3, 1e-2, 1e-1, 1], [1, 0.5, 0.3, 2])\nplt.show()\n14/12:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport math\nfrom tqdm import tqdm\n14/13:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    #reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    #return reg.score(x_test, y_test)\n    return 3 * alpha1 * alpha2\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array))\nprint(alpha1Score)\n\nfig, ax = plt.subplots(ncols=2, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot([1e-3, 1e-2, 1e-1, 1], [1, 0.5, 0.3, 2])\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot([1e-3, 1e-2, 1e-1, 1], [1, 0.5, 0.3, 2])\nplt.show()\n14/14:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    #reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    #return reg.score(x_test, y_test)\n    return 3 * alpha1 * alpha2\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = map(lambda alpha: scoreFunc(alpha, 0), alpha1Array)\nprint(alpha1Score)\n\nfig, ax = plt.subplots(ncols=2, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot([1e-3, 1e-2, 1e-1, 1], [1, 0.5, 0.3, 2])\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot([1e-3, 1e-2, 1e-1, 1], [1, 0.5, 0.3, 2])\nplt.show()\n14/15:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    #reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    #return reg.score(x_test, y_test)\n    return 3 * alpha1 * alpha2\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), alpha1Array))\nprint(alpha1Score)\n\nfig, ax = plt.subplots(ncols=2, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot([1e-3, 1e-2, 1e-1, 1], [1, 0.5, 0.3, 2])\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot([1e-3, 1e-2, 1e-1, 1], [1, 0.5, 0.3, 2])\nplt.show()\n14/16:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    #reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    #return reg.score(x_test, y_test)\n    return 3 + alpha1 + alpha2\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array))\nprint(alpha1Score)\n\nfig, ax = plt.subplots(ncols=2, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot([1e-3, 1e-2, 1e-1, 1], [1, 0.5, 0.3, 2])\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot([1e-3, 1e-2, 1e-1, 1], [1, 0.5, 0.3, 2])\nplt.show()\n14/17:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    #reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    #return reg.score(x_test, y_test)\n    return 3 + alpha1 + alpha2\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array)))\nprint(alpha1Score)\n\nfig, ax = plt.subplots(ncols=2, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot([1e-3, 1e-2, 1e-1, 1], [1, 0.5, 0.3, 2])\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot([1e-3, 1e-2, 1e-1, 1], [1, 0.5, 0.3, 2])\nplt.show()\n14/18:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n    #return 3 + alpha1 + alpha2\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array)))\nprint(alpha1Score)\n\nfig, ax = plt.subplots(ncols=2, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot([1e-3, 1e-2, 1e-1, 1], [1, 0.5, 0.3, 2])\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot([1e-3, 1e-2, 1e-1, 1], [1, 0.5, 0.3, 2])\nplt.show()\n14/19:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\nalpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nfig, ax = plt.subplots(ncols=2, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\nplt.show()\n14/20:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(self, x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = self.h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps, position=4):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n14/21:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\nalpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nfig, ax = plt.subplots(ncols=2, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\nplt.show()\n14/22:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport math\nfrom tqdm.notebook import tqdm, trange\n14/23:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport math\nfrom tqdm.notebook import tqdm, trange\n14/24:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(self, x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = self.h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps, position=4):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n14/25:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\nalpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nfig, ax = plt.subplots(ncols=2, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\nplt.show()\n16/1:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport math\nfrom tqdm.notebook import tqdm, trange\n16/2:\ndata = pd.read_csv('data.csv')\ndata\n16/3:\nx, y = data.values[:, :-2].astype(np.float32), data.values[:, -2:-1].astype(np.float32)\n\nnp.random.seed(1337)\nis_train = np.random.uniform(size=(x.shape[0],)) < 0.95\n\nx_train, y_train = x[is_train], y[is_train]\nx_test, y_test = x[~is_train], y[~is_train]\n\nprint(f'Train samples: {len(x_train)}')\nprint(f'Test samples: {len(x_test)}')\n16/4:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(self, x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = self.h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps, position=4):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n16/5:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\nalpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nfig, ax = plt.subplots(ncols=2, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\nplt.show()\n16/6:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(self, x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = self.h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps, position=100, leave=False):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n16/7:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\nalpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nfig, ax = plt.subplots(ncols=2, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\nplt.show()\n16/8:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(self, x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = self.h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps, leave=False):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n17/1:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n17/2:\ndata = pd.read_csv('data.csv')\nx, y = data.values[:, :-2].astype(np.float32), data.values[:, -2:-1].astype(np.float32)\n\nnp.random.seed(1337)\nis_train = np.random.uniform(size=(x.shape[0],)) < 0.95\n\nx_train, y_train = x[is_train], y[is_train]\nx_test, y_test = x[~is_train], y[~is_train]\n\nprint(f'Train samples: {len(x_train)}')\nprint(f'Test samples: {len(x_test)}')\n17/3:\ndef toPowMatrix(matrix, p):\n    res = list()\n    res.append(np.ones(matrix.shape))\n    res.append(matrix)   \n    for i in range(2, p + 1):\n        res.append(np.multiply(res[-1], matrix))\n    return np.concatenate(res, axis = 1)\n\ntoPowMatrix(np.matrix([[1, 2, 3],\n                       [4, 5, 6]]), 2)\n18/1:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport math\nfrom tqdm.notebook import tqdm, trange\n18/2:\ndata = pd.read_csv('data.csv')\ndata\n18/3:\nx, y = data.values[:, :-2].astype(np.float32), data.values[:, -2:-1].astype(np.float32)\n\nnp.random.seed(1337)\nis_train = np.random.uniform(size=(x.shape[0],)) < 0.95\n\nx_train, y_train = x[is_train], y[is_train]\nx_test, y_test = x[~is_train], y[~is_train]\n\nprint(f'Train samples: {len(x_train)}')\nprint(f'Test samples: {len(x_test)}')\n18/4:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(self, x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = self.h(x, theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps, leave=False):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n18/5:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\nalpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1 in trange(alpha1Array, desc='Alpha1'):\n    for alpha1 in tqdm(alpha1Array, desc='Alpha2'):\n        alpha1_2_score[] scoreFunc()\n\nfig, ax = plt.subplots(ncols=2, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\nplt.show()\n18/6:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\nalpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n\nfig, ax = plt.subplots(ncols=2, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\nplt.show()\n18/7:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n\nfig, ax = plt.subplots(ncols=2, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\nplt.show()\n18/8:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = alpha1Index * alpha2Index#scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].set_xscale('log')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/9:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = alpha1Index * alpha2Index#scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].set_xscale('log')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/10:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = alpha1Index * alpha2Index#scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/11:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/12:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\npd.DataFrame(alpha1Array).to_csv(\"./alpha1Array.csv\")\nprint(pd.read_csv(\"/alpha1Array.scv\"))\n\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/13:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\npd.DataFrame(alpha1Array).to_csv(\"./alpha1Array.csv\")\nprint(pd.read_csv(\"./alpha1Array.scv\"))\n\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/14:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\npd.DataFrame(alpha1Array).to_csv(\"./alpha1Array.csv\")\nprint(pd.read_csv(\"./alpha1Array.csv\"))\n\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/15:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\npd.DataFrame(alpha1Array).to_csv(\"./alpha1Array.csv\", index=False)\nprint(pd.read_csv(\"./alpha1Array.csv\"))\n\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/16:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\npd.DataFrame(alpha1Array).to_csv(\"./alpha1Array.csv\", index=False)\nprint(pd.read_csv(\"./alpha1Array.csv\")[:1])\n\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/17:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\npd.DataFrame(alpha1Array).to_csv(\"./alpha1Array.csv\", index=False)\nprint(pd.read_csv(\"./alpha1Array.csv\").values)\n\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/18:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\npd.DataFrame(alpha1Array).to_csv(\"./alpha1Array.csv\", index=False)\nprint(pd.read_csv(\"./alpha1Array.csv\").values.astype(np.float32))\n\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/19:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\npd.DataFrame(alpha1Array).to_csv(\"./alpha1Array.csv\", index=False)\nprint(pd.read_csv(\"./alpha1Array.csv\").values.flaten().astype(np.float32))\n\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/20:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\npd.DataFrame(alpha1Array).to_csv(\"./alpha1Array.csv\", index=False)\nprint(pd.read_csv(\"./alpha1Array.csv\").values.flatten().astype(np.float32))\n\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/21:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\npd.DataFrame(np.arange(16).reshape(4, 4))).to_csv(\"./alpha1Array.csv\", index=False)\nprint(pd.read_csv(\"./alpha1Array.csv\").values.flatten().astype(np.float32))\n\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/22:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\npd.DataFrame(np.arange(16).reshape(4, 4)).to_csv(\"./alpha1Array.csv\", index=False)\nprint(pd.read_csv(\"./alpha1Array.csv\").values.flatten().astype(np.float32))\n\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/23:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\npd.DataFrame(np.arange(16).reshape(4, 4)).to_csv(\"./alpha1Array.csv\", index=False)\nprint(pd.read_csv(\"./alpha1Array.csv\").values.astype(np.float32))\n\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/24:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores.csv\", index=False)\nprint(pd.read_csv(\"./alphaScores.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/25:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4.csv\", index=False)\nprint(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/26:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None || alpha1_2_score[alpha1Index, alpha2Index] < min_score)\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/27:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score)\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/28:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/29:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-3, 2048, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/30:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-3, 2048, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Array[0] = 0\nalpha2Array[0] = 0\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/31:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(self, x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = self.h(x, theta)\n        print(theta)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps, leave=False):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n18/32:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-3, 2048, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Array[0] = 0\nalpha2Array[0] = 0\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n17/4:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n17/5: np.ones((4, 1))\n17/6: np.ones((4, 1)).delete(0)\n17/7: np.delete(np.ones((4, 1)), 0)\n17/8: np.delete(np.arange(4).shape((4, 1)), 0)\n17/9: np.delete(np.arange(4).reshape((4, 1)), 0)\n17/10: np.delete(np.arange(4).reshape((4, 1)), 0)\n17/11:\nnp.delete(np.arange(4).reshape((4, 1)), 0)\nnp.arange(4).reshape((4, 1))\n17/12: np.delete(np.arange(4).reshape((4, 1)), 0)\n17/13: np.delete(np.arange(4).reshape((4, 1)), 0, axis=0)\n18/33:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(self, x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = self.h(x, theta)\n        theta = np.delete(theta, 0, axis=0)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps, leave=False):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n18/34:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Array[0] = 0\nalpha2Array[0] = 0\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/35:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Array[0] = 1e-3\nalpha2Array[0] = 1e-3\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/36:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 3, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Array[0] = 1e-3\nalpha2Array[0] = 1e-3\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/37:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/38:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-2, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/39:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-2, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/40:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-4, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/41:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-3, 2048, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/42:\n#reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-3, 2048, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Array[0] = 1e-3\nalpha2Array[0] = 1e-3\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4_2048steps.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/43:\n# reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-3, 2048, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Array[0] = 1e-2\nalpha2Array[0] = 1e-2\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4_2048steps.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/44:\n# reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Array[0] = 1e-2\nalpha2Array[0] = 1e-2\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4_2048steps.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/45:\n# reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-3, 1024, 2000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Array[0] = 1e-2\nalpha2Array[0] = 1e-2\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4_2048steps.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/46:\n# reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-3, 1024, 2000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Array[0] = 1e-3\nalpha2Array[0] = 1e-3\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4_2048steps.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/47:\n# reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 3, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Array[0] = 1e-4\nalpha2Array[0] = 1e-4\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4_2048steps.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/48:\n# reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 3, 1e-3, 2048, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Array[0] = 1e-4\nalpha2Array[0] = 1e-4\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores4_2048steps.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/49:\n# reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 3, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_3poly_1024steps.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/50:\nreg = PolynomialRegression(1e-3, 1e-3, 3, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n18/51:\nreg = PolynomialRegression(1e-3, 1e-3, 3, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n18/52:\nreg = PolynomialRegression(0, 0, 3, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n18/53:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(self, x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = self.h(x, theta)\n        theta = np.delete(theta, 0, axis=0)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.square(theta).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps, leave=False):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n18/54:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(self, x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = self.h(x, theta)\n        theta = np.delete(theta, 0, axis=0)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.square(theta).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps, leave=False):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n18/55:\nreg = PolynomialRegression(1e-3, 1e-3, 3, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n18/56:\nreg = PolynomialRegression(0, 0, 3, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n18/57:\nreg = PolynomialRegression(1e-2, 1e-2, 3, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n18/58:\nreg = PolynomialRegression(1e-2, 1e-2, 3, 1e-3, 2048, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n18/59:\nreg = PolynomialRegression(1e-2, 1e-2, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n18/60:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(self, x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = self.h(x, theta)\n        theta = np.delete(theta, 0, axis=0)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.square(theta).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps, leave=False):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n18/61:\n# reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 3, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-7, -1, 7)\nalpha2Array = 10 ** np.linspace(-7, -1, 7)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_3poly_1024steps.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/62:\n# reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 3, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_3poly_1024steps.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/63:\n# reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 3, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_3poly_1024batch.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/64:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(self, x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = self.h(x, theta)\n        theta = np.delete(theta, 0, axis=0)\n        return np.square(y - y_pred).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * np.power(theta, 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps, leave=False):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n18/65:\n# reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 3, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_3poly_1024batch.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/66:\n# reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 3, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Array[0] = 1e-2\nalpha2Array[0] = 1e-2\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_3poly_1024batch.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/67:\n# reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Array[0] = 1e-2\nalpha2Array[0] = 1e-2\n\n#alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\n#alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('\\rCurrent min score = ' + str(min_score))\n\nprint(alpha1_2_score)\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_3poly_1024batch.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\n#ax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\n#ax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/68:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(self, x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = self.h(x, theta)\n        theta = np.delete(theta, 0, axis=0)\n        return ((y - y_pred) ** 2).mean() + self.alpha1*np.absolute(theta).sum() + self.alpha2 * (theta ** 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps, leave=False):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n18/69:\nreg = PolynomialRegression(1e-3, 1e-3, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n18/70:\nreg = PolynomialRegression(1e-4, 1e-4, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n18/71:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(self, x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = self.h(x, theta)\n        theta = np.delete(theta, 0, axis=0)\n        return ((y - y_pred) ** 2).mean() + self.alpha1 * np.abs(theta).sum() + self.alpha2 * (theta ** 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps, leave=False):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n18/72:\nreg = PolynomialRegression(1e-4, 1e-4, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n17/14: np.arange(4).reshape((4, 1)) ** 2\n17/15: np.arange(8).reshape((4, 2)) ** 2\n17/16: np.square(np.arange(8).reshape((4, 2)))\n18/73:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(self, x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = self.h(x, theta)\n        theta = np.delete(theta, 0, axis=0)\n        return ((y - y_pred) ** 2).mean() + self.alpha1 * np.abs(theta).sum() + self.alpha2 * (theta ** 2).sum()\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps, leave=False):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n18/74:\nreg = PolynomialRegression(1e-4, 1e-4, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n18/75:\n# reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\nalpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha1Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('Current min score = ' + str(min_score))\n\nprint(alpha1_2_score)\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_4poly_1024batch.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/76:\n# reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Alpha1')))\nalpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha2Array, desc='Alpha2')))\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='Alpha1'):\n    for alpha2Index in trange(len(alpha2Array), desc='Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('Current min score = ' + str(min_score))\n\nprint(alpha1_2_score)\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_4poly_1024batch.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/77:\n# reg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\n#print(f'Test MAE: {reg.score(x_test, y_test)}')\n\ndef scoreFunc(alpha1, alpha2):\n    reg = PolynomialRegression(alpha1, alpha2, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Single Alpha1', leave=False)))\npd.DataFrame(alpha1Score).to_csv(\"./alpha1Score_4poly_1024batch.csv\", index=False)\n\nalpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha2Array, desc='Single Alpha2', leave=False)))\npd.DataFrame(alpha2Score).to_csv(\"./alpha2Score_4poly_1024batch.csv\", index=False)\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='For Alpha1', leave=False):\n    for alpha2Index in trange(len(alpha2Array), desc='For Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('Current min score = ' + str(min_score))\n\nprint(alpha1_2_score)\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_4poly_1024batch.csv\", index=False)\n#print(pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32))\n        \nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n18/78:\nreg = PolynomialRegression(0, 0, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n18/79:\nreg = PolynomialRegression(0, 0, 4, 1e-3, 1024, 2000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n18/80:\nreg = PolynomialRegression(1e-3, 1e-3, 4, 1e-3, 1024, 2000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n18/81:\nreg = PolynomialRegression(0, 0, 4, 1e-3, 1024, 2000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n18/82:\nreg = PolynomialRegression(0, 0, 3, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n18/83:\nreg = PolynomialRegression(1e-3, 1e-3, 3, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n#11.61\n18/84:\nreg = PolynomialRegression(1e-4, 1e-4, 3, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n#11.61\n18/85:\nreg = PolynomialRegression(1e-2, 1e-2, 3, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n#11.61\n18/86:\nreg = PolynomialRegression(1e-3, 1e-3, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n18/87:\nreg = PolynomialRegression(1e-3, 1e-3, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n#13.82\n18/88:\nreg = PolynomialRegression(0, 0, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n#13.79\n18/89:\nreg = PolynomialRegression(1e-3, 1e-3, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n#13.79\n#11.61\n18/90:\nreg = PolynomialRegression(1e-3, 1e-3, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n#13.79\n#11.61\n18/91:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n#13.79\n#11.61\n18/92:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n#13.79\n#11.61\n18/93:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nfig, ax = plt.subplots(ncols=1, figsize=(10, 10))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].plot(x_test, y_test)\nax[0].plot(x_test, reg.predict(x_test), color=green)\nplt.show()\n18/94:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nplt.plot(x_test, y_test, 'r')\nplt.plot(x_test, reg.predict(x_test), 'g')\nplt.show()\n18/95:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nplt.scatter(x_test, y_test, 'r')\nplt.scatter(x_test, reg.predict(x_test), 'g')\nplt.show()\n18/96:\nreg = PolynomialRegression(0, 0, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\nplt.scatter(x_test, y_test, marker='.', c='r')\nplt.scatter(x_test, reg.predict(x_test), marker='.', c='g')\nplt.show()\n17/17: p.delete(np.square(np.arange(8).reshape((4, 2))), 0, axis = 0)\n17/18: np.delete(np.square(np.arange(8).reshape((4, 2))), 0, axis = 0)\n18/97:\nreg = PolynomialRegression(1e-5, 1e-5, 1, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n#13.79\n#11.61\n18/98:\nreg = PolynomialRegression(1e-5, 1e-5, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n18/99:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(self, x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = self.h(x, theta)\n        theta_ = np.delete(theta, 0, axis=0)\n        return ((y - y_pred) ** 2).mean() + self.alpha1 * np.sum(np.abs(theta_)) + self.alpha2 * np.sum(theta_ ** 2)\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps, leave=False):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n18/100:\nreg = PolynomialRegression(1e-5, 1e-5, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n18/101:\nreg = PolynomialRegression(1e-3, 1e-3, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n18/102:\nreg = PolynomialRegression(1e-5, 1e-5, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n21/1:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport math\nfrom tqdm.notebook import tqdm, trange\n21/2:\ndata = pd.read_csv('data.csv')\ndata\n21/3:\nx, y = data.values[:, :-2].astype(np.float32), data.values[:, -2:-1].astype(np.float32)\n\nnp.random.seed(1337)\nis_train = np.random.uniform(size=(x.shape[0],)) < 0.95\n\nx_train, y_train = x[is_train], y[is_train]\nx_test, y_test = x[~is_train], y[~is_train]\n\nprint(f'Train samples: {len(x_train)}')\nprint(f'Test samples: {len(x_test)}')\n21/4:\nclass PolynomialRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        poly_deg,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.poly_deg = poly_deg\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def h(self, x, theta):\n        return x @ theta\n    \n    def preprocess(self, x):\n        res = list()\n        res.append(np.ones((x.shape[0], 1)))\n        res.append(x)\n        for i in range(2, self.poly_deg + 1):\n            res.append(np.multiply(res[-1], x))\n        return np.concatenate(res, axis = 1)\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n    \n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        y_pred = self.h(x, theta)\n        theta_ = np.delete(theta, 0, axis=0)\n        return ((y - y_pred) ** 2).mean() + self.alpha1 * np.sum(np.abs(theta_)) + self.alpha2 * np.sum(theta_ ** 2)\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        return (-2 / x.size * x.transpose()\n                @ (y - x @ theta)\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        (m, N), (_, k) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        try:\n            assert np.allclose(x[:, 1:].mean(axis=0), 0, atol=1e-3)\n            assert np.all((np.abs(x[:, 1:].std(axis=0)) < 1e-2) | (np.abs(x[:, 1:].std(axis=0) - 1) < 1e-2))\n        except AssertionError as e:\n            print('Something wrong with normalization')\n            raise e\n            \n        x_batch, y_batch = self.get_batch(x, y)\n        try:\n            assert x_batch.shape[0] == self.batch_size\n            assert y_batch.shape[0] == self.batch_size\n        except AssertionError as e:\n            print('Something wrong with get_batch')\n            raise e\n        \n        theta = np.zeros(shape=(N, k))\n        v_1 = np.zeros(shape=(N, k))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps, leave=False):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n            \n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad # * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n            \n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return x @ self.theta\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return np.abs(y - y_pred).mean()\n21/5:\nreg = PolynomialRegression(1e-3, 1e-3, 4, 1e-3, 1024, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\nprint(f'Test MAE: {reg.score(x_test, y_test)}')\n\nplt.figure(figsize=(10, 10))\nplt.scatter(y_test[:, 0], y_test_pred[:, 0], marker='.', c='r')\nplt.xlabel('True Y')\nplt.ylabel('Predicted Y')\nplt.show()\n23/1: import numpy as np\n23/2: import numpy as np\n23/3: np.ones(10)\n23/4: np.ones(10, axis=1)\n23/5: np.ones(sahep(1, 10))\n23/6: np.ones(sahpe=(1, 10))\n23/7: np.ones(1, 10)\n23/8: np.ones((1, 10))\n23/9: np.ones((10, 1))\n23/10:\n\nnp.ones((10, 1))\n23/11: np.arange(10)\n23/12: np.arange(11)\n23/13: np.arange(18, shape=(6, 3))\n23/14: np.arange(18).reshape(6, 3)\n23/15: np.concatenate(np.ones((6, 1)), np.arange(18).reshape(6, 3))\n23/16: np.concatenate(np.ones((6, 1)), np.arange(18).reshape(6, 3))\n23/17: np.concatenate(np.ones((6, 1)), np.arange(18).reshape(6, 3), axis = 1)\n23/18: np.concatenate(np.ones((6, 1)), np.arange(18).reshape(6, 3), axis=1)\n23/19: np.concatenate((np.ones((6, 1)), np.arange(18).reshape(6, 3)), axis=1)\n23/20: np.concatenate((np.ones((6, 1)), np.arange(18).reshape(6, 3)))\n24/1: np.concatenate((np.ones((6, 1)), np.arange(18).reshape(6, 3)))\n24/2: np.concatenate((np.ones((6, 1)), np.arange(18).reshape(6, 3)), axis=1)\n24/3: import numpy as np\n24/4: np.concatenate((np.ones((6, 1)), np.arange(18).reshape(6, 3)), axis=1)\n25/1:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n24/5:\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\n24/6: np.concatenate((np.ones((6, 1)), np.arange(18).reshape(6, 3)), axis=1)\n24/7:\nohe = OneHotEndcoder(sparse=false)\nohe.fit_transform(np.arange(18).reshape(6,3))\n24/8:\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\n24/9:\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\n24/10:\nohe = OneHotEncoder(sparse=false)\nohe.fit_transform(np.arange(18).reshape(6,3))\n24/11:\nohe = OneHotEncoder(sparse=False)\nohe.fit_transform(np.arange(18).reshape(6,3))\n24/12:\nohe = OneHotEncoder(sparse=False)\nohe.fit_transform(np.arange(8).reshape(4,2))\n24/13:\nohe = OneHotEncoder(sparse=False)\nohe.transform(np.arange(8).reshape(4,2))\n24/14:\nohe = OneHotEncoder(sparse=False)\nohe.fit_transform(np.arange(8).reshape(4,2))\n24/15:\nohe = OneHotEncoder(sparse=False)\nohe.fit_transform(np.arange(8).reshape(4,2))\n24/16: np.zeros((10, 3))\n24/17: np.zeros((10, 3))\n24/18: np.zeros((10, 3))\n24/19: np.arange(8).reshape(4, 2).max()\n24/20: np.arange(8).reshape(4, 2)\n25/2:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max()))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        # TODO\n        hyp = h(x)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 - np.exp(x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = h(x)\n        return (-1 * (x.transpose() @ (y - hyp)).mean()\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad # * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n25/3:\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\nprint(f'Test accuracy: {reg.score(x_test, y_test) * 100}%')\n25/4:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n25/5:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n25/6:\nwith np.load('mnist.npz') as npz:\n    x_train, y_train, x_test, y_test = [npz[k] for k in ['x_train', 'y_train', 'x_test', 'y_test']]\n\nfig, ax = plt.subplots(figsize=(20, 4),  ncols=5)\nfor a in ax:\n    i = np.random.randint(x_train.shape[0])\n    a.matshow(x_train[i], cmap='gray')\n    a.set_title(f'Label: {y_train[i]}')\n    a.axis('off')\n    \nprint(f'x_train shape: {x_train.shape}')\nprint(f'x_test shape: {x_test.shape}')\n25/7:\nx_train = x_train.reshape(-1, 28 * 28)\nx_test = x_test.reshape(-1, 28 * 28)\n\nprint(f'x_train shape after reshape: {x_train.shape}')\nprint(f'x_test shape after reshape: {x_test.shape}')\n25/8:\nx_train = x_train.astype(np.float32)\nx_test = x_test.astype(np.float32)\n25/9:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max()))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        # TODO\n        hyp = h(x)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 - np.exp(x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = h(x)\n        return (-1 * (x.transpose() @ (y - hyp)).mean()\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad # * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n25/10:\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\nprint(f'Test accuracy: {reg.score(x_test, y_test) * 100}%')\n25/11:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max() + 1))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        # TODO\n        hyp = h(x)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 - np.exp(x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = h(x)\n        return (-1 * (x.transpose() @ (y - hyp)).mean()\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        for step in range(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad # * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n25/12:\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\nprint(f'Test accuracy: {reg.score(x_test, y_test) * 100}%')\n25/13:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tqdm.notebook import tqdm, trange\n25/14:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max() + 1))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        # TODO\n        hyp = h(x)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 - np.exp(x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = h(x)\n        return (-1 * (x.transpose() @ (y - hyp)).mean()\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        for step in trange(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad # * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n25/15:\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\nprint(f'Test accuracy: {reg.score(x_test, y_test) * 100}%')\n25/16:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max() + 1))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        hyp = self.h(x)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 - np.exp(x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = self.h(x)\n        return (-1 * (x.transpose() @ (y - hyp)).mean()\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        for step in trange(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad # * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n25/17:\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\nprint(f'Test accuracy: {reg.score(x_test, y_test) * 100}%')\n25/18:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max() + 1))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        hyp = self.h(x. theta)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 - np.exp(x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = self.h(x, theta)\n        return (-1 * (x.transpose() @ (y - hyp)).mean()\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        for step in trange(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad # * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n25/19:\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\nprint(f'Test accuracy: {reg.score(x_test, y_test) * 100}%')\n25/20:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max() + 1))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        hyp = self.h(x. theta)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 - np.exp(x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = self.h(x, theta)\n        return (-1 * (x.transpose() @ (y - hyp)).mean()\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad # * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n25/21:\n# pip install scikit-learn\nfrom sklearn.metrics import confusion_matrix\n\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\n\ncm = confusion_matrix(y_test, y_test_pred)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(cm)\nplt.xlabel('Predicted class')\nplt.ylabel('True class')\nplt.xticks(range(10))\nplt.yticks(range(10))\nplt.show()\n\nprint(f'Final accuracy {reg.score(x_test, y_test) * 100}%')\n25/22:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max() + 1))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        hyp = self.h(x. theta)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 - np.exp(x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = self.h(x, theta)\n        return (-1 * (x.transpose() @ (y - hyp)).mean()\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad # * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n25/23:\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\nprint(f'Test accuracy: {reg.score(x_test, y_test) * 100}%')\n27/1:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tqdm.notebook import tqdm, trange\n27/2:\nwith np.load('mnist.npz') as npz:\n    x_train, y_train, x_test, y_test = [npz[k] for k in ['x_train', 'y_train', 'x_test', 'y_test']]\n\nfig, ax = plt.subplots(figsize=(20, 4),  ncols=5)\nfor a in ax:\n    i = np.random.randint(x_train.shape[0])\n    a.matshow(x_train[i], cmap='gray')\n    a.set_title(f'Label: {y_train[i]}')\n    a.axis('off')\n    \nprint(f'x_train shape: {x_train.shape}')\nprint(f'x_test shape: {x_test.shape}')\n27/3:\nx_train = x_train.reshape(-1, 28 * 28)\nx_test = x_test.reshape(-1, 28 * 28)\n\nprint(f'x_train shape after reshape: {x_train.shape}')\nprint(f'x_test shape after reshape: {x_test.shape}')\n27/4:\nx_train = x_train.astype(np.float32)\nx_test = x_test.astype(np.float32)\n27/5:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max() + 1))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        hyp = self.h(x. theta)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 - np.exp(x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = self.h(x, theta)\n        return (-1 * (x.transpose() @ (y - hyp)).mean()\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad # * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n27/6:\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\nprint(f'Test accuracy: {reg.score(x_test, y_test) * 100}%')\n27/7:\n# pip install scikit-learn\nfrom sklearn.metrics import confusion_matrix\n\n\ndef scoreFunc(alpha1, alpha2):\n    reg = LogisticRegression(alpha1, alpha2, 1e-3, 32, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Single Alpha1', leave=False)))\npd.DataFrame(alpha1Score).to_csv(\"./alpha1Score_4poly_1024batch.csv\", index=False)\n\nalpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha2Array, desc='Single Alpha2', leave=False)))\npd.DataFrame(alpha2Score).to_csv(\"./alpha2Score_4poly_1024batch.csv\", index=False)\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='For Alpha1', leave=False):\n    for alpha2Index in trange(len(alpha2Array), desc='For Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('Current min score = ' + str(min_score))\n\nprint(alpha1_2_score)\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_4poly_1024batch.csv\", index=False)\n\n\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\n\ncm = confusion_matrix(y_test, y_test_pred)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(cm)\nplt.xlabel('Predicted class')\nplt.ylabel('True class')\nplt.xticks(range(10))\nplt.yticks(range(10))\nplt.show()\n\nprint(f'Final accuracy {reg.score(x_test, y_test) * 100}%')\n27/8:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max() + 1))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        hyp = self.h(x. theta)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 + np.exp(-x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = self.h(x, theta)\n        return (-1 * (x.transpose() @ (y - hyp)).mean()\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad # * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n27/9:\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\nprint(f'Test accuracy: {reg.score(x_test, y_test) * 100}%')\n27/10:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max() + 1))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        hyp = self.h(x. theta)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 + np.exp(-x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = self.h(x, theta)\n        return (-1 * (x.transpose() @ (y - hyp)).mean()\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n27/11:\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\nprint(f'Test accuracy: {reg.score(x_test, y_test) * 100}%')\n27/12:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max() + 1))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        hyp = self.h(x. theta)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 + np.exp(-x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = self.h(x, theta)\n        return (-1 * (x.transpose() @ (y - hyp)).mean()\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad #* self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n27/13:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max() + 1))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        hyp = self.h(x, theta)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 + np.exp(-x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = self.h(x, theta)\n        return (-1 * (x.transpose() @ (y - hyp)).mean()\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad * self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n27/14:\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\nprint(f'Test accuracy: {reg.score(x_test, y_test) * 100}%')\n27/15:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max() + 1))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        hyp = self.h(x, theta)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 + np.exp(-x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = self.h(x, theta)\n        return (-1 * (x.transpose() @ (y - hyp)).mean()\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad #* self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n27/16:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max() + 1))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        hyp = self.h(x, theta)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 + np.exp(-x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = self.h(x, theta)\n        return (-1 * (x.transpose() @ (y - hyp)).mean()\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        print(\"Mu.shape = \" + self.mu.shape)\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad #* self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n27/17:\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\nprint(f'Test accuracy: {reg.score(x_test, y_test) * 100}%')\n27/18:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max() + 1))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        hyp = self.h(x, theta)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 + np.exp(-x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = self.h(x, theta)\n        return (-1 * (x.transpose() @ (y - hyp)).mean()\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        print(\"Mu.shape = \" + str(self.mu.shape) + \"; sigma.shape=\" + str(self.sigma.shape) + \"; x.shape = \" str(x.shape))\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad #* self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n27/19:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max() + 1))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        hyp = self.h(x, theta)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 + np.exp(-x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = self.h(x, theta)\n        return (-1 * (x.transpose() @ (y - hyp)).mean()\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        print(\"Mu.shape = \" + str(self.mu.shape) + \"; sigma.shape=\" + str(self.sigma.shape) + \"; x.shape = \" + str(x.shape))\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad #* self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n27/20:\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\nprint(f'Test accuracy: {reg.score(x_test, y_test) * 100}%')\n27/21:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max() + 1))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        hyp = self.h(x, theta)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 + np.exp(-x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = self.h(x, theta)\n        return (-1 * (x.transpose() @ (y - hyp)).mean()\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad #* self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n27/22:\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\nprint(f'Test accuracy: {reg.score(x_test, y_test) * 100}%')\n27/23:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max() + 1))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        hyp = self.h(x, theta)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 + np.exp(-x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = self.h(x, theta)\n        return (-1 * (x.transpose() @ (y - hyp)) / x.shape[0]\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad #* self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n27/24:\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\nprint(f'Test accuracy: {reg.score(x_test, y_test) * 100}%')\n27/25:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max() + 1))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        hyp = self.h(x, theta)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 + np.exp(-x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = self.h(x, theta)\n        return (-1 * (x.transpose() @ (y - hyp)) / x.shape[0]\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad #* self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n27/26:\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\nprint(f'Test accuracy: {reg.score(x_test, y_test) * 100}%')\n27/27:\n# pip install scikit-learn\nfrom sklearn.metrics import confusion_matrix\n\n\ndef scoreFunc(alpha1, alpha2):\n    reg = LogisticRegression(alpha1, alpha2, 1e-3, 32, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Single Alpha1', leave=False)))\npd.DataFrame(alpha1Score).to_csv(\"./alpha1Score_4poly_1024batch.csv\", index=False)\n\nalpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha2Array, desc='Single Alpha2', leave=False)))\npd.DataFrame(alpha2Score).to_csv(\"./alpha2Score_4poly_1024batch.csv\", index=False)\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='For Alpha1', leave=False):\n    for alpha2Index in trange(len(alpha2Array), desc='For Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('Current min score = ' + str(min_score))\n\nprint(alpha1_2_score)\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_4poly_1024batch.csv\", index=False)\n\n\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\n\ncm = confusion_matrix(y_test, y_test_pred)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(cm)\nplt.xlabel('Predicted class')\nplt.ylabel('True class')\nplt.xticks(range(10))\nplt.yticks(range(10))\nplt.show()\n\nprint(f'Final accuracy {reg.score(x_test, y_test) * 100}%')\n27/28:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max() + 1))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        hyp = self.h(x, theta)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 + np.exp(-x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = self.h(x, theta)\n        return (-1 * (x.transpose() @ (y - hyp)) / x.shape[0]\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps, leave=False):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad #* self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n27/29:\n# pip install scikit-learn\nfrom sklearn.metrics import confusion_matrix\n\n\ndef scoreFunc(alpha1, alpha2):\n    reg = LogisticRegression(alpha1, alpha2, 1e-3, 32, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\nalpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Single Alpha1', leave=False)))\npd.DataFrame(alpha1Score).to_csv(\"./alpha1Score_4poly_1024batch.csv\", index=False)\n\nalpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha2Array, desc='Single Alpha2', leave=False)))\npd.DataFrame(alpha2Score).to_csv(\"./alpha2Score_4poly_1024batch.csv\", index=False)\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='For Alpha1', leave=False):\n    for alpha2Index in trange(len(alpha2Array), desc='For Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            print('Current min score = ' + str(min_score))\n\nprint(alpha1_2_score)\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_4poly_1024batch.csv\", index=False)\n\n\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\n\ncm = confusion_matrix(y_test, y_test_pred)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(cm)\nplt.xlabel('Predicted class')\nplt.ylabel('True class')\nplt.xticks(range(10))\nplt.yticks(range(10))\nplt.show()\n\nprint(f'Final accuracy {reg.score(x_test, y_test) * 100}%')\n32/1:\n# pip install scikit-learn\nfrom sklearn.metrics import confusion_matrix\n\n\ndef scoreFunc(alpha1, alpha2):\n    reg = LogisticRegression(alpha1, alpha2, 1e-3, 32, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n# alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Single Alpha1', leave=False)))\n# pd.DataFrame(alpha1Score).to_csv(\"./alpha1Score_4poly_1024batch.csv\", index=False)\nalpha2Score = pd.read_csv(\"./alpha1Score_4poly_1024batch.csv\").values.astype(np.float32)\n\n# alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha2Array, desc='Single Alpha2', leave=False)))\n# pd.DataFrame(alpha2Score).to_csv(\"./alpha2Score_4poly_1024batch.csv\", index=False)\nalpha2Score = pd.read_csv(\"./alpha2Score_4poly_1024batch.csv\").values.astype(np.float32)\n\n# min_score = None\n# alpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\n# for alpha1Index in trange(len(alpha1Array), desc='For Alpha1', leave=False):\n#     for alpha2Index in trange(len(alpha2Array), desc='For Alpha2', leave=False):\n#         alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n#         if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n#             min_score = alpha1_2_score[alpha1Index, alpha2Index]\n#             print('Current min score = ' + str(min_score))\n\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_4poly_1024batch.csv\", index=False)\nalpha1_2_score = pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32)\nprint(alpha1_2_score)\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n\n\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\n\ncm = confusion_matrix(y_test, y_test_pred)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(cm)\nplt.xlabel('Predicted class')\nplt.ylabel('True class')\nplt.xticks(range(10))\nplt.yticks(range(10))\nplt.show()\n\nprint(f'Final accuracy {reg.score(x_test, y_test) * 100}%')\n32/2:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tqdm.notebook import tqdm, trange\n32/3:\nwith np.load('mnist.npz') as npz:\n    x_train, y_train, x_test, y_test = [npz[k] for k in ['x_train', 'y_train', 'x_test', 'y_test']]\n\nfig, ax = plt.subplots(figsize=(20, 4),  ncols=5)\nfor a in ax:\n    i = np.random.randint(x_train.shape[0])\n    a.matshow(x_train[i], cmap='gray')\n    a.set_title(f'Label: {y_train[i]}')\n    a.axis('off')\n    \nprint(f'x_train shape: {x_train.shape}')\nprint(f'x_test shape: {x_test.shape}')\n32/4:\nx_train = x_train.reshape(-1, 28 * 28)\nx_test = x_test.reshape(-1, 28 * 28)\n\nprint(f'x_train shape after reshape: {x_train.shape}')\nprint(f'x_test shape after reshape: {x_test.shape}')\n32/5:\nx_train = x_train.astype(np.float32)\nx_test = x_test.astype(np.float32)\n32/6:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max() + 1))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        hyp = self.h(x, theta)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 + np.exp(-x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = self.h(x, theta)\n        return (-1 * (x.transpose() @ (y - hyp)) / x.shape[0]\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps, leave=False):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad #* self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n32/7:\n# pip install scikit-learn\nfrom sklearn.metrics import confusion_matrix\n\n\ndef scoreFunc(alpha1, alpha2):\n    reg = LogisticRegression(alpha1, alpha2, 1e-3, 32, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n# alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Single Alpha1', leave=False)))\n# pd.DataFrame(alpha1Score).to_csv(\"./alpha1Score_4poly_1024batch.csv\", index=False)\nalpha2Score = pd.read_csv(\"./alpha1Score_4poly_1024batch.csv\").values.astype(np.float32)\n\n# alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha2Array, desc='Single Alpha2', leave=False)))\n# pd.DataFrame(alpha2Score).to_csv(\"./alpha2Score_4poly_1024batch.csv\", index=False)\nalpha2Score = pd.read_csv(\"./alpha2Score_4poly_1024batch.csv\").values.astype(np.float32)\n\n# min_score = None\n# alpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\n# for alpha1Index in trange(len(alpha1Array), desc='For Alpha1', leave=False):\n#     for alpha2Index in trange(len(alpha2Array), desc='For Alpha2', leave=False):\n#         alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n#         if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n#             min_score = alpha1_2_score[alpha1Index, alpha2Index]\n#             print('Current min score = ' + str(min_score))\n\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_4poly_1024batch.csv\", index=False)\nalpha1_2_score = pd.read_csv(\"./alphaScores4.csv\").values.astype(np.float32)\nprint(alpha1_2_score)\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n\n\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\n\ncm = confusion_matrix(y_test, y_test_pred)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(cm)\nplt.xlabel('Predicted class')\nplt.ylabel('True class')\nplt.xticks(range(10))\nplt.yticks(range(10))\nplt.show()\n\nprint(f'Final accuracy {reg.score(x_test, y_test) * 100}%')\n32/8:\n# pip install scikit-learn\nfrom sklearn.metrics import confusion_matrix\n\n\ndef scoreFunc(alpha1, alpha2):\n    reg = LogisticRegression(alpha1, alpha2, 1e-3, 32, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n# alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Single Alpha1', leave=False)))\n# pd.DataFrame(alpha1Score).to_csv(\"./alpha1Score_4poly_1024batch.csv\", index=False)\nalpha2Score = pd.read_csv(\"./alpha1Score_4poly_1024batch.csv\").values.astype(np.float32)\n\n# alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha2Array, desc='Single Alpha2', leave=False)))\n# pd.DataFrame(alpha2Score).to_csv(\"./alpha2Score_4poly_1024batch.csv\", index=False)\nalpha2Score = pd.read_csv(\"./alpha2Score_4poly_1024batch.csv\").values.astype(np.float32)\n\n# min_score = None\n# alpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\n# for alpha1Index in trange(len(alpha1Array), desc='For Alpha1', leave=False):\n#     for alpha2Index in trange(len(alpha2Array), desc='For Alpha2', leave=False):\n#         alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n#         if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n#             min_score = alpha1_2_score[alpha1Index, alpha2Index]\n#             print('Current min score = ' + str(min_score))\n\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_4poly_1024batch.csv\", index=False)\nalpha1_2_score = pd.read_csv(\"./alphaScores_4poly_1024batch.csv\").values.astype(np.float32)\nprint(alpha1_2_score)\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n\n\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\n\ncm = confusion_matrix(y_test, y_test_pred)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(cm)\nplt.xlabel('Predicted class')\nplt.ylabel('True class')\nplt.xticks(range(10))\nplt.yticks(range(10))\nplt.show()\n\nprint(f'Final accuracy {reg.score(x_test, y_test) * 100}%')\n32/9:\n# pip install scikit-learn\nfrom sklearn.metrics import confusion_matrix\n\n\ndef scoreFunc(alpha1, alpha2):\n    reg = LogisticRegression(alpha1, alpha2, 1e-3, 32, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n# alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Single Alpha1', leave=False)))\n# pd.DataFrame(alpha1Score).to_csv(\"./alpha1Score_4poly_1024batch.csv\", index=False)\nalpha1Score = pd.read_csv(\"./alpha1Score_4poly_1024batch.csv\").values.astype(np.float32)\n\n# alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha2Array, desc='Single Alpha2', leave=False)))\n# pd.DataFrame(alpha2Score).to_csv(\"./alpha2Score_4poly_1024batch.csv\", index=False)\nalpha2Score = pd.read_csv(\"./alpha2Score_4poly_1024batch.csv\").values.astype(np.float32)\n\n# min_score = None\n# alpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\n# for alpha1Index in trange(len(alpha1Array), desc='For Alpha1', leave=False):\n#     for alpha2Index in trange(len(alpha2Array), desc='For Alpha2', leave=False):\n#         alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n#         if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n#             min_score = alpha1_2_score[alpha1Index, alpha2Index]\n#             print('Current min score = ' + str(min_score))\n\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_4poly_1024batch.csv\", index=False)\nalpha1_2_score = pd.read_csv(\"./alphaScores_4poly_1024batch.csv\").values.astype(np.float32)\nprint(alpha1_2_score)\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n\n\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\n\ncm = confusion_matrix(y_test, y_test_pred)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(cm)\nplt.xlabel('Predicted class')\nplt.ylabel('True class')\nplt.xticks(range(10))\nplt.yticks(range(10))\nplt.show()\n\nprint(f'Final accuracy {reg.score(x_test, y_test) * 100}%')\n32/10:\n# pip install scikit-learn\nfrom sklearn.metrics import confusion_matrix\n\n\ndef scoreFunc(alpha1, alpha2):\n    reg = LogisticRegression(alpha1, alpha2, 1e-3, 32, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-5, -1, 5)\nalpha2Array = 10 ** np.linspace(-5, -1, 5)\n\n# alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Single Alpha1', leave=False)))\n# pd.DataFrame(alpha1Score).to_csv(\"./alpha1Score_4poly_1024batch.csv\", index=False)\nalpha1Score = pd.read_csv(\"./alpha1Score_4poly_1024batch.csv\").values.astype(np.float32)\n\n# alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha2Array, desc='Single Alpha2', leave=False)))\n# pd.DataFrame(alpha2Score).to_csv(\"./alpha2Score_4poly_1024batch.csv\", index=False)\nalpha2Score = pd.read_csv(\"./alpha2Score_4poly_1024batch.csv\").values.astype(np.float32)\n\n# min_score = None\n# alpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\n# for alpha1Index in trange(len(alpha1Array), desc='For Alpha1', leave=False):\n#     for alpha2Index in trange(len(alpha2Array), desc='For Alpha2', leave=False):\n#         alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n#         if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n#             min_score = alpha1_2_score[alpha1Index, alpha2Index]\n#             print('Current min score = ' + str(min_score))\n\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_4poly_1024batch.csv\", index=False)\nalpha1_2_score = pd.read_csv(\"./alphaScores_4poly_1024batch.csv\").values.astype(np.float32)\nprint(alpha1_2_score)\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n\n\nreg = LogisticRegression(1e-5, 1e-4, 1e-3, 32, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\n\ncm = confusion_matrix(y_test, y_test_pred)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(cm)\nplt.xlabel('Predicted class')\nplt.ylabel('True class')\nplt.xticks(range(10))\nplt.yticks(range(10))\nplt.show()\n\nprint(f'Final accuracy {reg.score(x_test, y_test) * 100}%')\n32/11:\n# pip install scikit-learn\nfrom sklearn.metrics import confusion_matrix\n\n\ndef scoreFunc(alpha1, alpha2):\n    reg = LogisticRegression(alpha1, alpha2, 1e-3, 32, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-6, -3, 4)\nalpha2Array = 10 ** np.linspace(-6, -3, 4)\n\ndef findAlpha1():\n    alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Single Alpha1', leave=False)))\n    pd.DataFrame(alpha1Score).to_csv(\"./alpha1Score_2.csv\", index=False)\n    return alpha1Score\n\ndef findAlpha2():\n    alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha2Array, desc='Single Alpha2', leave=False)))\n    pd.DataFrame(alpha2Score).to_csv(\"./alpha2Score_2.csv\", index=False)\n    return alpha2Score\n\nalpha1Score = findAlpha1()\n#alpha1Score = pd.read_csv(\"./alpha1Score_4poly_1024batch.csv\").values.astype(np.float32)\n\nalpha2Score = findAlpha2()\n#alpha2Score = pd.read_csv(\"./alpha2Score_4poly_1024batch.csv\").values.astype(np.float32)\n\n# min_score = None\n# alpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\n# for alpha1Index in trange(len(alpha1Array), desc='For Alpha1', leave=False):\n#     for alpha2Index in trange(len(alpha2Array), desc='For Alpha2', leave=False):\n#         alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n#         if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n#             min_score = alpha1_2_score[alpha1Index, alpha2Index]\n#             print('Current min score = ' + str(min_score))\n\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_4poly_1024batch.csv\", index=False)\nalpha1_2_score = pd.read_csv(\"./alphaScores_4poly_1024batch.csv\").values.astype(np.float32)\nprint(alpha1_2_score)\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n\n\nreg = LogisticRegression(1e-5, 1e-4, 1e-3, 32, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\n\ncm = confusion_matrix(y_test, y_test_pred)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(cm)\nplt.xlabel('Predicted class')\nplt.ylabel('True class')\nplt.xticks(range(10))\nplt.yticks(range(10))\nplt.show()\n\nprint(f'Final accuracy {reg.score(x_test, y_test) * 100}%')\n32/12:\n# pip install scikit-learn\nfrom sklearn.metrics import confusion_matrix\n\n\ndef scoreFunc(alpha1, alpha2):\n    reg = LogisticRegression(alpha1, alpha2, 1e-3, 32, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-6, -3, 4)\nalpha2Array = 10 ** np.linspace(-6, -3, 4)\n\ndef findAlpha1():\n    alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Single Alpha1', leave=False)))\n    pd.DataFrame(alpha1Score).to_csv(\"./alpha1Score_2.csv\", index=False)\n    return alpha1Score\n\ndef findAlpha2():\n    alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha2Array, desc='Single Alpha2', leave=False)))\n    pd.DataFrame(alpha2Score).to_csv(\"./alpha2Score_2.csv\", index=False)\n    return alpha2Score\n\n# alpha1Score = findAlpha1()\nalpha1Score = pd.read_csv(\"./alpha1Score_2.csv\").values.astype(np.float32)\n\n# alpha2Score = findAlpha2()\nalpha2Score = pd.read_csv(\"./alpha2Score_2.csv\").values.astype(np.float32)\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='For Alpha1', leave=False):\n    for alpha2Index in trange(len(alpha2Array), desc='For Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            # print('Current min score = ' + str(min_score))\n\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_2.csv\", index=False)\n# alpha1_2_score = pd.read_csv(\"./alphaScores_4poly_1024batch.csv\").values.astype(np.float32)\nprint(alpha1_2_score)\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n\n\nreg = LogisticRegression(1e-5, 1e-4, 1e-3, 32, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\n\ncm = confusion_matrix(y_test, y_test_pred)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(cm)\nplt.xlabel('Predicted class')\nplt.ylabel('True class')\nplt.xticks(range(10))\nplt.yticks(range(10))\nplt.show()\n\nprint(f'Final accuracy {reg.score(x_test, y_test) * 100}%')\n32/13:\n# pip install scikit-learn\nfrom sklearn.metrics import confusion_matrix\n\n\ndef scoreFunc(alpha1, alpha2):\n    reg = LogisticRegression(alpha1, alpha2, 1e-3, 32, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-6, -3, 4)\nalpha2Array = 10 ** np.linspace(-6, -3, 4)\n\ndef findAlpha1():\n    alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Single Alpha1', leave=False)))\n    pd.DataFrame(alpha1Score).to_csv(\"./alpha1Score_2.csv\", index=False)\n    return alpha1Score\n\ndef findAlpha2():\n    alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha2Array, desc='Single Alpha2', leave=False)))\n    pd.DataFrame(alpha2Score).to_csv(\"./alpha2Score_2.csv\", index=False)\n    return alpha2Score\n\n# alpha1Score = findAlpha1()\nalpha1Score = pd.read_csv(\"./alpha1Score_2.csv\").values.astype(np.float32)\n\n# alpha2Score = findAlpha2()\nalpha2Score = pd.read_csv(\"./alpha2Score_2.csv\").values.astype(np.float32)\n\n# min_score = None\n# alpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\n# for alpha1Index in trange(len(alpha1Array), desc='For Alpha1', leave=False):\n#     for alpha2Index in trange(len(alpha2Array), desc='For Alpha2', leave=False):\n#         alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n#         if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n#             min_score = alpha1_2_score[alpha1Index, alpha2Index]\n#             # print('Current min score = ' + str(min_score))\n\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_2.csv\", index=False)\nalpha1_2_score = pd.read_csv(\"./alphaScores_2.csv\").values.astype(np.float32)\nprint(alpha1_2_score)\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n\n\nreg = LogisticRegression(1e-5, 1e-5, 1e-3, 32, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\n\ncm = confusion_matrix(y_test, y_test_pred)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(cm)\nplt.xlabel('Predicted class')\nplt.ylabel('True class')\nplt.xticks(range(10))\nplt.yticks(range(10))\nplt.show()\n\nprint(f'Final accuracy {reg.score(x_test, y_test) * 100}%')\n32/14:\nwith np.load('mnist.npz') as npz:\n    x_train, y_train, x_test, y_test = [npz[k] for k in ['x_train', 'y_train', 'x_test', 'y_test']]\n\nfig, ax = plt.subplots(figsize=(20, 4),  ncols=5)\nfor a in ax:\n    i = np.random.randint(x_train.shape[0])\n    a.matshow(x_train[i], cmap='gray')\n    a.set_title(f'Label: {y_train[i]}')\n    a.axis('off')\n    \nprint(f'x_train shape: {x_train.shape}')\nprint(f'x_test shape: {x_test.shape}')\n32/15:\nx_train = x_train.reshape(-1, 28 * 28)\nx_test = x_test.reshape(-1, 28 * 28)\n\nprint(f'x_train shape after reshape: {x_train.shape}')\nprint(f'x_test shape after reshape: {x_test.shape}')\n32/16:\nx_train = x_train.astype(np.float32)\nx_test = x_test.astype(np.float32)\n32/17:\n# pip install scikit-learn\nfrom sklearn.metrics import confusion_matrix\n\n\ndef scoreFunc(alpha1, alpha2):\n    reg = LogisticRegression(alpha1, alpha2, 1e-3, 32, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-6, -3, 4)\nalpha2Array = 10 ** np.linspace(-6, -3, 4)\n\ndef findAlpha1():\n    alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Single Alpha1', leave=False)))\n    pd.DataFrame(alpha1Score).to_csv(\"./alpha1Score_2.csv\", index=False)\n    return alpha1Score\n\ndef findAlpha2():\n    alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha2Array, desc='Single Alpha2', leave=False)))\n    pd.DataFrame(alpha2Score).to_csv(\"./alpha2Score_2.csv\", index=False)\n    return alpha2Score\n\n# alpha1Score = findAlpha1()\nalpha1Score = pd.read_csv(\"./alpha1Score_2.csv\").values.astype(np.float32)\n\n# alpha2Score = findAlpha2()\nalpha2Score = pd.read_csv(\"./alpha2Score_2.csv\").values.astype(np.float32)\n\n# min_score = None\n# alpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\n# for alpha1Index in trange(len(alpha1Array), desc='For Alpha1', leave=False):\n#     for alpha2Index in trange(len(alpha2Array), desc='For Alpha2', leave=False):\n#         alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n#         if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n#             min_score = alpha1_2_score[alpha1Index, alpha2Index]\n#             # print('Current min score = ' + str(min_score))\n\n#pd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_2.csv\", index=False)\nalpha1_2_score = pd.read_csv(\"./alphaScores_2.csv\").values.astype(np.float32)\nprint(alpha1_2_score)\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n\n\nreg = LogisticRegression(1e-5, 1e-5, 1e-3, 32, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\n\ncm = confusion_matrix(y_test, y_test_pred)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(cm)\nplt.xlabel('Predicted class')\nplt.ylabel('True class')\nplt.xticks(range(10))\nplt.yticks(range(10))\nplt.show()\n\nprint(f'Final accuracy {reg.score(x_test, y_test) * 100}%')\n32/18:\n# pip install scikit-learn\nfrom sklearn.metrics import confusion_matrix\n\n\ndef scoreFunc(alpha1, alpha2):\n    reg = LogisticRegression(alpha1, alpha2, 1e-3, 32, 1000).fit(x_train, y_train)\n    return reg.score(x_test, y_test)\n\nalpha1Array = 10 ** np.linspace(-6, -3, 4)\nalpha2Array = 10 ** np.linspace(-6, -3, 4)\n\ndef findAlpha1():\n    alpha1Score = list(map(lambda alpha: scoreFunc(alpha, 0), tqdm(alpha1Array, desc='Single Alpha1', leave=False)))\n    pd.DataFrame(alpha1Score).to_csv(\"./alpha1Score_3.csv\", index=False)\n    return alpha1Score\n\ndef findAlpha2():\n    alpha2Score = list(map(lambda alpha: scoreFunc(0, alpha), tqdm(alpha2Array, desc='Single Alpha2', leave=False)))\n    pd.DataFrame(alpha2Score).to_csv(\"./alpha2Score_3.csv\", index=False)\n    return alpha2Score\n\nalpha1Score = findAlpha1()\n#alpha1Score = pd.read_csv(\"./alpha1Score_2.csv\").values.astype(np.float32)\n\nalpha2Score = findAlpha2()\n#alpha2Score = pd.read_csv(\"./alpha2Score_2.csv\").values.astype(np.float32)\n\nmin_score = None\nalpha1_2_score = np.empty((alpha1Array.size, alpha2Array.size))\nfor alpha1Index in trange(len(alpha1Array), desc='For Alpha1', leave=False):\n    for alpha2Index in trange(len(alpha2Array), desc='For Alpha2', leave=False):\n        alpha1_2_score[alpha1Index, alpha2Index] = scoreFunc(alpha1Array[alpha1Index], alpha2Array[alpha2Index])\n        if(min_score == None or alpha1_2_score[alpha1Index, alpha2Index] < min_score):\n            min_score = alpha1_2_score[alpha1Index, alpha2Index]\n            # print('Current min score = ' + str(min_score))\n\npd.DataFrame(alpha1_2_score).to_csv(\"./alphaScores_3.csv\", index=False)\n#alpha1_2_score = pd.read_csv(\"./alphaScores_2.csv\").values.astype(np.float32)\nprint(alpha1_2_score)\n\nfig, ax = plt.subplots(ncols=3, figsize=(16, 6))\nfig.suptitle('Poly deg. = 5')\nax[0].set_xlabel('Alpha 1')\nax[0].set_ylabel('Score')\nax[0].set_xscale('log')\nax[0].plot(alpha1Array, alpha1Score)\nax[1].set_xlabel('Alpha 2')\nax[1].set_ylabel('Score')\nax[1].set_xscale('log')\nax[1].plot(alpha2Array, alpha2Score)\n\nax[2].set_xlabel('Alpha 1')\nax[2].set_ylabel('Alpha 2')\nax[2].matshow(alpha1_2_score)\n\nplt.show()\n\n\nreg = LogisticRegression(1e-5, 1e-5, 1e-3, 32, 1000).fit(x_train, y_train)\ny_test_pred = reg.predict(x_test)\n\ncm = confusion_matrix(y_test, y_test_pred)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(cm)\nplt.xlabel('Predicted class')\nplt.ylabel('True class')\nplt.xticks(range(10))\nplt.yticks(range(10))\nplt.show()\n\nprint(f'Final accuracy {reg.score(x_test, y_test) * 100}%')\n35/1:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tqdm.notebook import tqdm, trange\n35/2:\nwith np.load('mnist.npz') as npz:\n    x_train, y_train, x_test, y_test = [npz[k] for k in ['x_train', 'y_train', 'x_test', 'y_test']]\n\nfig, ax = plt.subplots(figsize=(20, 4),  ncols=5)\nfor a in ax:\n    i = np.random.randint(x_train.shape[0])\n    a.matshow(x_train[i], cmap='gray')\n    a.set_title(f'Label: {y_train[i]}')\n    a.axis('off')\n    \nprint(f'x_train shape: {x_train.shape}')\nprint(f'x_test shape: {x_test.shape}')\n35/3:\nx_train = x_train.reshape(-1, 28 * 28)\nx_test = x_test.reshape(-1, 28 * 28)\n\nprint(f'x_train shape after reshape: {x_train.shape}')\nprint(f'x_test shape after reshape: {x_test.shape}')\n35/4:\nx_train = x_train.astype(np.float32)\nx_test = x_test.astype(np.float32)\n35/5:\nclass LogisticRegression:\n    def __init__(\n        self,\n        alpha1,\n        alpha2,\n        learning_rate,\n        batch_size,\n        train_steps\n    ):\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.train_steps = train_steps\n    \n    def preprocess(self, x):\n        return np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n    \n    def onehot(self, y):\n        Y = np.zeros((y.size, y.max() + 1))\n        Y[np.arange(y.size), y] = 1\n        return Y\n    \n    def normalize(self, x):\n        sigma = self.sigma + 1e-8\n        X = np.delete(x, 0, axis=1)\n        X = (X - self.mu) / sigma\n        return np.concatenate((np.ones(shape=(X.shape[0],1)), X), axis=1)\n\n    def moments(self, x):\n        X = np.delete(x, 0, axis=1)\n        return (X.mean(axis = 0), np.std(X, axis = 0))\n    \n    def J(self, x, y, theta):\n        hyp = self.h(x, theta)\n        theta_ = np.delete(theta, 0, axis=0)\n        return (-(y*np.log(hyp) - (1-y)*np.log(1-hyp)).mean()\n                + self.alpha1 * np.sum(np.abs(theta_))\n                + self.alpha2 * np.sum(theta_ ** 2))\n    \n    def h(self, x, theta):\n        return 1 / (1 + np.exp(-x @ theta))\n    \n    def grad(self, x, y, theta):\n        eye = np.eye(theta.shape[0])\n        eye[0,0] = 0\n        hyp = self.h(x, theta)\n        return (-1 * (x.transpose() @ (y - hyp)) / x.shape[0]\n                + eye @ (self.alpha1 * np.sign(theta)\n                   + 2 * self.alpha2 * theta))\n    \n    def get_batch(self, x, y):\n        rows_indices = np.random.choice(x.shape[0], size=self.batch_size, replace=False)\n        x_rows = list()\n        y_rows = list()\n        for i in rows_indices:\n            x_rows.append([x[i]])\n            y_rows.append([y[i]])\n        return (np.concatenate(x_rows), np.concatenate(y_rows))\n    \n    def fit(self, x, y):\n        x = self.preprocess(x)\n        y = self.onehot(y)\n\n        (m, n), (_, c) = x.shape, y.shape\n        \n        self.mu, self.sigma = self.moments(x)\n        x = self.normalize(x)\n        \n        theta = np.zeros(shape=(n, c))\n        v_1 = np.zeros(shape=(n, c))\n        v_t = v_1\n        gama = 0.9\n        for step in trange(self.train_steps, leave=False):\n            x_batch, y_batch = self.get_batch(x, y)\n            theta_grad = self.grad(x_batch, y_batch, theta)\n\n            # TODO Update v_t and theta\n            v_t = gama * v_t + self.learning_rate * theta_grad #* self.J(x_batch, y_batch, theta)\n            theta = theta - v_t\n\n        self.theta = theta\n        \n        return self\n\n    def predict(self, x):\n        x = self.preprocess(x)\n        x = self.normalize(x)\n        return self.h(x, self.theta).argmax(axis=1)\n    \n    def score(self, x, y):\n        y_pred = self.predict(x)\n        return (y == y_pred).mean()\n35/6:\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\nprint(f'Test accuracy: {reg.score(x_test, y_test) * 100}%')",
      "\n35/7:\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\nprint(f'Test accuracy: {reg.score(x_test, y_test) * 100}%')\n35/8:\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\nprint(f'Test accuracy: {reg.score(x_test, y_test) * 100}%')\n35/9:\nreg = LogisticRegression(0, 0, 1e-3, 32, 1000).fit(x_train, y_train)\nprint(f'Test accuracy: {reg.score(x_test, y_test) * 100}%')\n38/1:\n# File location and type\nfile_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"false\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf_train = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df_train)\n38/2:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n38/3:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n38/4:\n# File location and type\nfile_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"false\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf_train = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df_train)\n39/1:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n39/2:\n# File location and type\nfile_location = \"/FileStore/tables/diabetic_data.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf_train = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df_train)\n38/5:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n38/6:\n# File location and type\nfile_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"false\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf_train = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df_train)\n38/7:\n# File location and type\nfile_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"false\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf_train = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df_train)\n38/8:\nfrom pyspark.sql.functions import udf\n\n\ndef isSick(x):\n    if x == \"Yes\":\n        return 1\n    else:\n        return 0\ndef gender(x):\n    if x == \"Male\":\n        return 0\n    else:\n        return 1\ndef a1ResultToInt(x):\n    if x == \"None\":\n        return 0\n    if x == \"Norm\":\n      return 1\n    else:\n      numb = str(x).replace('>','')\n      print(numb)\n      return int(numb)\n    \ndef diagToFloat(x):\n    if x == \"?\":\n        return 0\n    if \"V\" in str(x):\n      numb = str(x).replace('V','')\n      return float(numb)\n    if \"E\" in str(x):\n      numb = str(x).replace('E','')\n      return float(numb)\n    else:\n      return float(x)\n\ndiabetesToSick = udf(lambda z: isSick(z), IntegerType())\ngenderToInt = udf(lambda z: gender(z), IntegerType())\naiResultInt = udf(lambda z: a1ResultToInt(z), IntegerType())\ndiafFloat = udf(lambda z: diagToFloat(z), FloatType())\ndf_train = df_train.withColumn('diabetesMed', diabetesToSick('diabetesMed'))\ndf_train = df_train.withColumn('gender', genderToInt('gender'))\ndf_train = df_train.withColumn('A1Cresult', aiResultInt('A1Cresult'))\ndf_train = df_train.withColumn('diag_1', diafFloat('diag_1'))\ndf_train = df_train.withColumn('diag_2', diafFloat('diag_2'))\ndf_train = df_train.withColumn('diag_3', diafFloat('diag_3'))\ndf_train.write.mode(\"overwrite\").saveAsTable(\"TrainData\")\n\n#display(df_train)\n38/9:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder\n    .master(\"local\")\n    .appName(\"Word Count\")\n    .config(\"spark.some.config.option\", \"some-value\")\n    .getOrCreate()\n38/10:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n38/11:\n# File location and type\nfile_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"false\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf_train = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df_train)\n38/12:\n# File location and type\nfile_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"false\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf_train = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df_train)\n38/13:\n# File location and type\nfile_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"false\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf_train = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df_train[0,0])\n38/14:\n# File location and type\nfile_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"false\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf_train = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\nprint(df_train[0,0])\n38/15:\n# File location and type\nfile_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"false\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf_train = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\nprint(df_train[0][0])\n38/16:\n# File location and type\nfile_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"false\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf_train = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\nprint(df_train[1][999])\n38/17:\n# File location and type\nfile_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"false\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf_train = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\nprint(df_train[1][999])\n38/18: print(df_train[1][999][0])\n38/19: print(df_train[1][999][1])\n38/20: print(df_train[1][999][1][0])\n38/21: print(df_train)\n38/22:\n# File location and type\nfile_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"false\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\n# df_train = spark.read.format(file_type) \\\n#   .option(\"inferSchema\", infer_schema) \\\n#   .option(\"header\", first_row_is_header) \\\n#   .option(\"sep\", delimiter) \\\n#   .load(file_location)\ndf_train = spark.read.csv(file_location)\n\nprint(df_train[1][999])\n38/23:\n# File location and type\nfile_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"false\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\n# df_train = spark.read.format(file_type) \\\n#   .option(\"inferSchema\", infer_schema) \\\n#   .option(\"header\", first_row_is_header) \\\n#   .option(\"sep\", delimiter) \\\n#   .load(file_location)\ndf_train = spark.read.csv(file_location)\n\nprint(df_train.count)\n38/24:\n# File location and type\nfile_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"false\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf_train = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n   .load(file_location)\n#df_train = spark.read.csv(file_location)\n\nprint(df_train.count())\n38/25:\n# File location and type\nfile_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf_train = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n   .load(file_location)\n#df_train = spark.read.csv(file_location)\n\nprint(df_train.count())\n38/26: df_train.describe()\n38/27: display(df_train)\n38/28: df_train.describe(['age']).show()\n38/29: df_train.describe(['c0']).show()\n38/30: display(df_train)\n38/31: df_train.show(n=100)\n38/32: df_train.show(n=3)\n38/33: df_train.show(n=3)\n38/34:\nval kmeans = new KMeans().setK(2).setSeed(1L)\nval model = kmeans.fit(dataset)\n38/35:\nval kmeans = new KMeans().setK(2).setSeed(1L)\nval model = kmeans.fit(df_train)\n38/36:\nkmeans = KMeans().setK(k).setSeed(1)\nmodel = kmeans.fit(df_train)\n38/37:\nfrom pyspark.ml.clustering import KMeans\nkmeans = KMeans().setK(k).setSeed(1)\nmodel = kmeans.fit(df_train)\n38/38:\nfrom pyspark.ml.clustering import KMeans\nkmeans = KMeans().setK(4).setSeed(1)\nmodel = kmeans.fit(df_train)\n38/39:\nfrom pyspark.ml.feature import VectorAssembler\n\nvecAssembler = VectorAssembler(inputCols=[\"lat\", \"long\"], outputCol=\"features\")\nnew_df = vecAssembler.transform(df)\nnew_df.show()\n\nfrom pyspark.ml.clustering import KMeans\n\nkmeans = KMeans(k=2, seed=1)  # 2 clusters here\nmodel = kmeans.fit(new_df.select('features'))\n38/40:\nkmeans = KMeans().setK(k).setSeed(1)\nmodel = kmeans.fit(df_train)\nkmeans = KMeans().setK(k).setSeed(1)\nmodel = kmeans.fit(df_train)\n38/41:\n\nkmeans = KMeans().setK(4).setSeed(1)\nmodel = kmeans.fit(df_train)\n38/42:\n\nkmeans = KMeans().setK(4).setSeed(1)\nmodel = kmeans.fit(df_train)\n38/43:\nfrom pyspark.ml.feature import VectorAssembler\n\nvecAssembler = VectorAssembler(inputCols=[\"c0\", \"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\"], outputCol=\"c10\")\nnew_df = vecAssembler.transform(df_train)\nnew_df.show()\n\nfrom pyspark.ml.clustering import KMeans\n\nkmeans = KMeans(k=2, seed=1)  # 2 clusters here\nmodel = kmeans.fit(new_df.select('c10'))\n38/44:\nfrom pyspark.ml.feature import VectorAssembler\n\nvecAssembler = VectorAssembler(inputCols=[\"c0\", \"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\", \"c10\"], outputCol=\"feature\")\nnew_df = vecAssembler.transform(df_train)\nnew_df.show()\n\nfrom pyspark.ml.clustering import KMeans\n\nkmeans = KMeans(k=2, seed=1)  # 2 clusters here\nmodel = kmeans.fit(new_df.select('feature'))\n38/45:\nfrom pyspark.ml.feature import VectorAssembler\n\nvecAssembler = VectorAssembler(inputCols=[\"c0\", \"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\", \"c10\"], outputCol=\"features\")\nnew_df = vecAssembler.transform(df_train)\nnew_df.show()\n\nfrom pyspark.ml.clustering import KMeans\n\nkmeans = KMeans(k=2, seed=1)  # 2 clusters here\nmodel = kmeans.fit(new_df.select('features'))\n38/46:\nfrom pyspark.ml.feature import VectorAssembler\n\nvecAssembler = VectorAssembler(inputCols=[\"c0\", \"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\", \"c10\"], outputCol=\"c10\")\nnew_df = vecAssembler.transform(df_train)\nnew_df.show()\n\nfrom pyspark.ml.clustering import KMeans\n\nkmeans = KMeans(k=2, seed=1)  # 2 clusters here\nmodel = kmeans.fit(new_df.select('features'))\n38/47:\nfrom pyspark.ml.feature import VectorAssembler\n\nvecAssembler = VectorAssembler(inputCols=[\"c0\", \"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\", \"c10\"], outputCol=\"feature\")\nnew_df = vecAssembler.transform(df_train)\nnew_df.show()\n\nfrom pyspark.ml.clustering import KMeans\n\nkmeans = KMeans(k=2, seed=1)  # 2 clusters here\nmodel = kmeans.fit(new_df.select('features'))\n38/48:\nfrom pyspark.ml.feature import VectorAssembler\n\nvecAssembler = VectorAssembler(inputCols=[\"c0\", \"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\", \"c10\"], outputCol=\"features\")\nnew_df = vecAssembler.transform(df_train)\nnew_df.show()\n\nfrom pyspark.ml.clustering import KMeans\n\nkmeans = KMeans(k=2, seed=1)  # 2 clusters here\nmodel = kmeans.fit(new_df.select('features'))\n38/49:\nfrom pyspark.ml.feature import VectorAssembler\n\nvecAssembler = VectorAssembler(inputCols=[\"c0\", \"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\", \"c10\"], outputCol=\"features\")\nnew_df = vecAssembler.transform(df_train)\nnew_df.show()\n\nfrom pyspark.ml.clustering import KMeans\n\nkmeans = KMeans(k=2, seed=1)  # 2 clusters here\nmodel = kmeans.fit(new_df.select('features'))\n38/50:\ntransformed = model.transform(new_df)\ntransformed.show()\n38/51:\ntransformed = model.transform(new_df)\ntransformed.show()\n38/52:\nfrom pyspark.ml.feature import VectorAssembler\n\nvecAssembler = VectorAssembler(inputCols=[\"c0\", \"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\"], outputCol=\"features\")\nnew_df = vecAssembler.transform(df_train)\nnew_df.show()\n\nfrom pyspark.ml.clustering import KMeans\n\nkmeans = KMeans(k=10, seed=1)  # 2 clusters here\nmodel = kmeans.fit(new_df.select('features'))\n38/53:\ntransformed = model.transform(new_df)\ntransformed.show()\n38/54:\ntransformed = model.transform(new_df)\ntransformed.show()\n38/55:\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Load the data stored in LIBSVM format as a DataFrame.file_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndata = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n   .load(file_location)\n\nfrom pyspark.ml.feature import VectorAssembler\n\nvecAssembler = VectorAssembler(inputCols=[\"c0\", \"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\"], outputCol=\"label\")\nnew_df = vecAssembler.transform(data)\nnew_df.show()\n\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n# Automatically identify categorical features, and index them.\n# We specify maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"c10\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = data.randomSplit([0.7, 0.3])\n\n# Train a DecisionTree model.\ndt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n\n# Chain indexers and tree in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"c10\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g \" % (1.0 - accuracy))\n\ntreeModel = model.stages[2]\n# summary only\nprint(treeModel)\n38/56:\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Load the data stored in LIBSVM format as a DataFrame.file_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndata = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n   .load(file_location)\n\nfrom pyspark.ml.feature import VectorAssembler\n\nvecAssembler = VectorAssembler(inputCols=[\"c0\", \"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\"], outputCol=\"label\")\nnew_df = vecAssembler.transform(data)\nnew_df.show()\n\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"indexedLabel\", outputCol=\"label\").fit(data)\n# Automatically identify categorical features, and index them.\n# We specify maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"c10\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = data.randomSplit([0.7, 0.3])\n\n# Train a DecisionTree model.\ndt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n\n# Chain indexers and tree in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"c10\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g \" % (1.0 - accuracy))\n\ntreeModel = model.stages[2]\n# summary only\nprint(treeModel)\n38/57:\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Load the data stored in LIBSVM format as a DataFrame.file_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndata = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n   .load(file_location)\n\nfrom pyspark.ml.feature import VectorAssembler\n\nvecAssembler = VectorAssembler(inputCols=[\"c0\", \"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\"], outputCol=\"label\")\nnew_df = vecAssembler.transform(data)\nnew_df.show()\n\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(new_df)\n# Automatically identify categorical features, and index them.\n# We specify maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"c10\", outputCol=\"indexedFeatures\", maxCategories=4).fit(new_df)\n\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = data.randomSplit([0.7, 0.3])\n\n# Train a DecisionTree model.\ndt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n\n# Chain indexers and tree in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"c10\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g \" % (1.0 - accuracy))\n\ntreeModel = model.stages[2]\n# summary only\nprint(treeModel)\n38/58:\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Load the data stored in LIBSVM format as a DataFrame.file_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndata = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n   .load(file_location)\n\n# from pyspark.ml.feature import VectorAssembler\n\n# vecAssembler = VectorAssembler(inputCols=[\"c0\", \"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\"], outputCol=\"label\")\n# new_df = vecAssembler.transform(data)\n# new_df.show()\n\nnew_df = train.rdd.map(lambda x: [Vectors.dense(x[0:10]), x[10]]).toDF(['label','features'])\nnew_df.show(n=10)\n\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(new_df)\n# Automatically identify categorical features, and index them.\n# We specify maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"c10\", outputCol=\"indexedFeatures\", maxCategories=4).fit(new_df)\n\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = data.randomSplit([0.7, 0.3])\n\n# Train a DecisionTree model.\ndt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n\n# Chain indexers and tree in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"c10\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g \" % (1.0 - accuracy))\n\ntreeModel = model.stages[2]\n# summary only\nprint(treeModel)\n38/59:\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Load the data stored in LIBSVM format as a DataFrame.file_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndata = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n   .load(file_location)\n\n# from pyspark.ml.feature import VectorAssembler\n\n# vecAssembler = VectorAssembler(inputCols=[\"c0\", \"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\"], outputCol=\"label\")\n# new_df = vecAssembler.transform(data)\n# new_df.show()\n\nnew_df = data.rdd.map(lambda x: [Vectors.dense(x[0:10]), x[10]]).toDF(['label','features'])\nnew_df.show(n=10)\n\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(new_df)\n# Automatically identify categorical features, and index them.\n# We specify maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"c10\", outputCol=\"indexedFeatures\", maxCategories=4).fit(new_df)\n\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = data.randomSplit([0.7, 0.3])\n\n# Train a DecisionTree model.\ndt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n\n# Chain indexers and tree in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"c10\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g \" % (1.0 - accuracy))\n\ntreeModel = model.stages[2]\n# summary only\nprint(treeModel)\n38/60:\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.mllib.linalg import Vectors, DenseMatrix\n\n# Load the data stored in LIBSVM format as a DataFrame.file_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndata = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n   .load(file_location)\n\n# from pyspark.ml.feature import VectorAssembler\n\n# vecAssembler = VectorAssembler(inputCols=[\"c0\", \"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\"], outputCol=\"label\")\n# new_df = vecAssembler.transform(data)\n# new_df.show()\n\nnew_df = data.rdd.map(lambda x: [Vectors.dense(x[0:10]), x[10]]).toDF(['label','features'])\nnew_df.show(n=10)\n\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(new_df)\n# Automatically identify categorical features, and index them.\n# We specify maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"c10\", outputCol=\"indexedFeatures\", maxCategories=4).fit(new_df)\n\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = data.randomSplit([0.7, 0.3])\n\n# Train a DecisionTree model.\ndt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n\n# Chain indexers and tree in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"c10\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g \" % (1.0 - accuracy))\n\ntreeModel = model.stages[2]\n# summary only\nprint(treeModel)\n38/61:\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.mllib.linalg import Vectors, DenseMatrix\n\n# Load the data stored in LIBSVM format as a DataFrame.file_location = \"./poker-hand-testing.data\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndata = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n   .load(file_location)\n\n# from pyspark.ml.feature import VectorAssembler\n\n# vecAssembler = VectorAssembler(inputCols=[\"c0\", \"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\"], outputCol=\"label\")\n# new_df = vecAssembler.transform(data)\n# new_df.show()\n\nnew_df = data.rdd.map(lambda x: [Vectors.dense(x[0:10]), x[10]]).toDF(['label','features'])\nnew_df.show(n=10)\n\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(new_df)\n# Automatically identify categorical features, and index them.\n# We specify maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"c10\", outputCol=\"indexedFeatures\", maxCategories=4).fit(new_df)\n\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = data.randomSplit([0.7, 0.3])\n\n# Train a DecisionTree model.\ndt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n\n# Chain indexers and tree in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"c10\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g \" % (1.0 - accuracy))\n\ntreeModel = model.stages[2]\n# summary only\nprint(treeModel)\n40/1:\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType\n\nschema = StructType([\n  StructField(\"age\", DoubleType(), False),\n  StructField(\"workclass\", StringType(), False),\n  StructField(\"fnlwgt\", DoubleType(), False),\n  StructField(\"education\", StringType(), False),\n  StructField(\"education_num\", DoubleType(), False),\n  StructField(\"marital_status\", StringType(), False),\n  StructField(\"occupation\", StringType(), False),\n  StructField(\"relationship\", StringType(), False),\n  StructField(\"race\", StringType(), False),\n  StructField(\"sex\", StringType(), False),\n  StructField(\"capital_gain\", DoubleType(), False),\n  StructField(\"capital_loss\", DoubleType(), False),\n  StructField(\"hours_per_week\", DoubleType(), False),\n  StructField(\"native_country\", StringType(), False),\n  StructField(\"income\", StringType(), False)\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"/databricks-datasets/adult/adult.data\")\ncols = dataset.columns\n40/2:\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType\n\nschema = StructType([\n  StructField(\"age\", DoubleType(), False),\n  StructField(\"workclass\", StringType(), False),\n  StructField(\"fnlwgt\", DoubleType(), False),\n  StructField(\"education\", StringType(), False),\n  StructField(\"education_num\", DoubleType(), False),\n  StructField(\"marital_status\", StringType(), False),\n  StructField(\"occupation\", StringType(), False),\n  StructField(\"relationship\", StringType(), False),\n  StructField(\"race\", StringType(), False),\n  StructField(\"sex\", StringType(), False),\n  StructField(\"capital_gain\", DoubleType(), False),\n  StructField(\"capital_loss\", DoubleType(), False),\n  StructField(\"hours_per_week\", DoubleType(), False),\n  StructField(\"native_country\", StringType(), False),\n  StructField(\"income\", StringType(), False)\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"/databricks-datasets/adult/adult.data\")\ncols = dataset.columns\n40/3:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType\n40/4:\nschema = StructType([\n  StructField(\"age\", DoubleType(), False),\n  StructField(\"workclass\", StringType(), False),\n  StructField(\"fnlwgt\", DoubleType(), False),\n  StructField(\"education\", StringType(), False),\n  StructField(\"education_num\", DoubleType(), False),\n  StructField(\"marital_status\", StringType(), False),\n  StructField(\"occupation\", StringType(), False),\n  StructField(\"relationship\", StringType(), False),\n  StructField(\"race\", StringType(), False),\n  StructField(\"sex\", StringType(), False),\n  StructField(\"capital_gain\", DoubleType(), False),\n  StructField(\"capital_loss\", DoubleType(), False),\n  StructField(\"hours_per_week\", DoubleType(), False),\n  StructField(\"native_country\", StringType(), False),\n  StructField(\"income\", StringType(), False)\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"/databricks-datasets/adult/adult.data\")\ncols = dataset.columns\n40/5:\nschema = StructType([\n  StructField(\"id\", DoubleType(), False)\n  StructField(\"age\", DoubleType(), False),\n  StructField(\"sex\", StringType(), False)\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./hospital_data.csv\")\ncols = dataset.columns\n40/6:\nschema = StructType([\n  StructField(\"id\", DoubleType(), False),\n  StructField(\"age\", DoubleType(), False),\n  StructField(\"sex\", StringType(), False)\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./hospital_data.csv\")\ncols = dataset.columns\n40/7:\nschema = StructType([\n  StructField(\"id\", DoubleType(), False),\n  StructField(\"age\", DoubleType(), False),\n  StructField(\"sex\", StringType(), False)\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./hospital_data.csv\")\ncols = dataset.columns\n40/8: display(dataset)\n40/9: print(dataset.count())\n40/10: print(dataset.count())\n40/11:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n40/12:\nschema = StructType([\n    StructField(\"id\", IntegerType(), False),\n    StructField(\"age\", IntegerType(), False),\n    StructField(\"sex\", StringType(), False),\n    StructField(\"CASTE_NAME\", StringType(), False),\n    StructField(\"CATEGORY_CODE\", StringType(), False),\n    StructField(\"CATEGORY_NAME\", StringType(), False),\n    StructField(\"SURGERY_CODE\", StringType(), False),\n    StructField(\"SURGERY\", StringType(), False),\n    StructField(\"DISTRICT_NAME\", StringType(), False),\n    StructField(\"PREAUTH_DATE\", DateType(), False),\n    StructField(\"PREAUTH_AMT\", StringType(), False),\n    StructField(\"CLAIM_DATE\", DateType (), False),\n    StructField(\"CLAIM_AMOUNT\", StringType(), False),\n    StructField(\"HOSP_NAME\", StringType(), False),\n    StructField(\"HOSP_TYPE\", StringType(), False),\n    StructField(\"HOSP_DISTRICT\", StringType(), False),\n    StructField(\"SURGERY_DATE\", DateType(), False),\n    StructField(\"DISCHARGE_DATE\", DateType(), False),\n    StructField(\"Mortality Y / N\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./hospital_data.csv\")\ncols = dataset.columns\n40/13: print(dataset.count())\n40/14:\ncategoricalColumns = [\"AGE\",\n    \"SEX\",\n    \"CASTE_NAME\",\n    \"CATEGORY_CODE\",\n    \"CATEGORY_NAME\",\n    \"SURGERY_CODE\",\n    \"SURGERY\",\n    \"DISTRICT_NAME\",\n    \"PREAUTH_DATE\",\n    \"PREAUTH_AMT\",\n    \"CLAIM_DATE\",\n    \"CLAIM_AMOUNT\",\n    \"HOSP_NAME\",\n    \"HOSP_TYPE\",\n    \"HOSP_DISTRICT\",\n    \"SURGERY_DATE\",\n    \"DISCHARGE_DATE\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n40/15:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n40/16:\ncategoricalColumns = [\"AGE\",\n    \"SEX\",\n    \"CASTE_NAME\",\n    \"CATEGORY_CODE\",\n    \"CATEGORY_NAME\",\n    \"SURGERY_CODE\",\n    \"SURGERY\",\n    \"DISTRICT_NAME\",\n    \"PREAUTH_DATE\",\n    \"PREAUTH_AMT\",\n    \"CLAIM_DATE\",\n    \"CLAIM_AMOUNT\",\n    \"HOSP_NAME\",\n    \"HOSP_TYPE\",\n    \"HOSP_DISTRICT\",\n    \"SURGERY_DATE\",\n    \"DISCHARGE_DATE\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n40/17:\ncategoricalColumns = [\"AGE\",\n    \"SEX\",\n    \"CASTE_NAME\",\n    \"CATEGORY_CODE\",\n    \"CATEGORY_NAME\",\n    \"SURGERY_CODE\",\n    \"SURGERY\",\n    \"DISTRICT_NAME\",\n    \"PREAUTH_DATE\",\n    \"PREAUTH_AMT\",\n    \"CLAIM_DATE\",\n    \"CLAIM_AMOUNT\",\n    \"HOSP_NAME\",\n    \"HOSP_TYPE\",\n    \"HOSP_DISTRICT\",\n    \"SURGERY_DATE\",\n    \"DISCHARGE_DATE\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n40/18:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"income\", outputCol=\"label\")\nstages += [label_stringIdx]\n40/19:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"Mortality Y / N\", outputCol=\"label\")\nstages += [label_stringIdx]\n40/20:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"Mortality Y / N\", outputCol=\"label\")\nstages += [label_stringIdx]\n40/21:\ncategoricalColumns = [\n    \"SEX\",\n    \"CASTE_NAME\",\n    \"CATEGORY_CODE\",\n    \"SURGERY_CODE\",\n    \"DISTRICT_NAME\",\n    \"HOSP_NAME\",\n    \"HOSP_TYPE\",\n    \"HOSP_DISTRICT\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n40/22:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"Mortality Y / N\", outputCol=\"label\")\nstages += [label_stringIdx]\n40/23:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"AGE\",\n\"PREAUTH_AMT\",\n\"CLAIM_AMOUNT\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]\n40/24:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"AGE\",\n\"PREAUTH_AMT\",\n\"CLAIM_AMOUNT\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]\n40/25:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n40/26:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n40/27:\nschema = StructType([\n    StructField(\"ID\", IntegerType(), False),\n    StructField(\"AGE\", IntegerType(), False),\n    StructField(\"SEX\", StringType(), False),\n    StructField(\"CASTE_NAME\", StringType(), False),\n    StructField(\"CATEGORY_CODE\", StringType(), False),\n    StructField(\"CATEGORY_NAME\", StringType(), False),\n    StructField(\"SURGERY_CODE\", StringType(), False),\n    StructField(\"SURGERY\", StringType(), False),\n    StructField(\"DISTRICT_NAME\", StringType(), False),\n    StructField(\"PREAUTH_DATE\", DateType(), False),\n    StructField(\"PREAUTH_AMT\", IntegerType(), False),\n    StructField(\"CLAIM_DATE\", DateType (), False),\n    StructField(\"CLAIM_AMOUNT\", IntegerType(), False),\n    StructField(\"HOSP_NAME\", StringType(), False),\n    StructField(\"HOSP_TYPE\", StringType(), False),\n    StructField(\"HOSP_DISTRICT\", StringType(), False),\n    StructField(\"SURGERY_DATE\", DateType(), False),\n    StructField(\"DISCHARGE_DATE\", DateType(), False),\n    StructField(\"Mortality Y / N\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./hospital_data.csv\")\ncols = dataset.columns\n40/28:\ncategoricalColumns = [\n    \"SEX\",\n    \"CASTE_NAME\",\n    \"CATEGORY_CODE\",\n    \"SURGERY_CODE\",\n    \"DISTRICT_NAME\",\n    \"HOSP_NAME\",\n    \"HOSP_TYPE\",\n    \"HOSP_DISTRICT\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n40/29:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"Mortality Y / N\", outputCol=\"label\")\nstages += [label_stringIdx]\n40/30:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"AGE\",\n\"PREAUTH_AMT\",\n\"CLAIM_AMOUNT\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]\n40/31:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n40/32:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n40/33:\n# Fit model to prepped data\nlrModel = LogisticRegression().fit(preppedDataDF)\n\n# ROC for training data\ndisplay(lrModel, preppedDataDF, \"ROC\")\n40/34:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"AGE\",\n\"PREAUTH_AMT\",\n\"CLAIM_AMOUNT\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n40/35:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n40/36:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n40/37:\nschema = StructType([\n    StructField(\"ID\", IntegerType(), False),\n    StructField(\"AGE\", IntegerType(), False),\n    StructField(\"SEX\", StringType(), False),\n    StructField(\"CASTE_NAME\", StringType(), False),\n    StructField(\"CATEGORY_CODE\", StringType(), False),\n    StructField(\"CATEGORY_NAME\", StringType(), False),\n    StructField(\"SURGERY_CODE\", StringType(), False),\n    StructField(\"SURGERY\", StringType(), False),\n    StructField(\"DISTRICT_NAME\", StringType(), False),\n    StructField(\"PREAUTH_DATE\", DateType(), False),\n    StructField(\"PREAUTH_AMT\", IntegerType(), False),\n    StructField(\"CLAIM_DATE\", DateType (), False),\n    StructField(\"CLAIM_AMOUNT\", IntegerType(), False),\n    StructField(\"HOSP_NAME\", StringType(), False),\n    StructField(\"HOSP_TYPE\", StringType(), False),\n    StructField(\"HOSP_DISTRICT\", StringType(), False),\n    StructField(\"SURGERY_DATE\", DateType(), False),\n    StructField(\"DISCHARGE_DATE\", DateType(), False),\n    StructField(\"Mortality Y / N\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./hospital_data.csv\")\ncols = dataset.columns\n40/38:\ncategoricalColumns = [\n    \"SEX\",\n    \"CASTE_NAME\",\n    \"CATEGORY_CODE\",\n    \"SURGERY_CODE\",\n    \"DISTRICT_NAME\",\n    \"HOSP_NAME\",\n    \"HOSP_TYPE\",\n    \"HOSP_DISTRICT\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n40/39:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"Mortality Y / N\", outputCol=\"label\")\nstages += [label_stringIdx]\n40/40:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"AGE\",\n\"PREAUTH_AMT\",\n\"CLAIM_AMOUNT\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n40/41:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n40/42:\n# Fit model to prepped data\nlrModel = LogisticRegression().fit(preppedDataDF)\n\n# ROC for training data\ndisplay(lrModel, preppedDataDF, \"ROC\")\n40/43:\n# Fit model to prepped data\nlrModel = LogisticRegression().fit(preppedDataDF)\n\n# ROC for training data\ndisplay(lrModel, preppedDataDF, \"ROC\")\n40/44: from pyspark.sql import *\n40/45:\nfrom pyspark.sql import *\ndisplay(lrModel, preppedDataDF, \"ROC\")\n40/46:\nfrom pyspark.sql import *\ndisplay(lrModel, preppedDataDF, \"ROC\")\n40/47:\nfrom pyspark.sql import *\ndisplay(lrModel.toPandas(), preppedDataDF, \"ROC\")\n40/48:\nfrom pyspark.sql import *\ndisplay(lrModel.toPandas(), preppedDataDF, \"ROC\")\n40/49: type(dataset)\n40/50: dataset\n40/51: print(dataset)\n40/52: print(dataset)\n40/53:\nimport pandas as pd\ntype(dataset)\n40/54:\nimport pandas as pd\ndataset.limit(10).toPandas()\n40/55:\nimport pandas as pd\ntype(lrModel)\n40/56:\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(5,5))\nplt.plot([0, 1], [0, 1], 'r--')\nplt.plot(lrModel.summary.roc.select('FPR').collect(),\n         lrModel.summary.roc.select('TPR').collect())\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()\n40/57:\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(5,5))\nplt.plot([0, 1], [0, 1], 'r--')\nplt.plot(lrModel.summary.roc.select('FPR').collect(),\n         lrModel.summary.roc.select('TPR').collect())\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()\n40/58:\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(5,5))\nplt.plot([0, 1], [0, 1], 'r--')\nplt.plot(lrModel.summary.roc.select('FPR').collect(),\n         lrModel.summary.roc.select('TPR').collect())\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()\n40/59: type(lrModel.summary)\n40/60:\n# Extract the summary from the returned LogisticRegressionModel instance trained\n# in the earlier example\ntrainingSummary = lrModel.summary\n\n# Obtain the objective per iteration\nobjectiveHistory = trainingSummary.objectiveHistory\nprint(\"objectiveHistory:\")\nfor objective in objectiveHistory:\n    print(objective)\n\n# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\ntrainingSummary.roc.show()\nprint(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n\n# Set the model threshold to maximize F-Measure\nfMeasure = trainingSummary.fMeasureByThreshold\nmaxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\nbestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n    .select('threshold').head()['threshold']\nlr.setThreshold(bestThreshold)\n40/61:\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(5,5))\nplt.plot([0, 1], [0, 1], 'r--')\nplt.plot(lrModel.summary.areaUnderROC.select('FPR').collect(),\n         lrModel.summary.areaUnderROC.select('TPR').collect())\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()\n40/62: print(dataset[0][\"Mortality Y / N\"])\n40/63: print(dataset.limit(10).toPandas()[0][\"Mortality Y / N\"])\n40/64: print(dataset.limit(10).toPandas())\n40/65: dataset.limit(10).toPandas()\n40/66:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n40/67:\nschema = StructType([\n    StructField(\"ID\", IntegerType(), False),\n    StructField(\"AGE\", IntegerType(), False),\n    StructField(\"SEX\", StringType(), False),\n    StructField(\"CASTE_NAME\", StringType(), False),\n    StructField(\"CATEGORY_CODE\", StringType(), False),\n    StructField(\"CATEGORY_NAME\", StringType(), False),\n    StructField(\"SURGERY_CODE\", StringType(), False),\n    StructField(\"SURGERY\", StringType(), False),\n    StructField(\"DISTRICT_NAME\", StringType(), False),\n    StructField(\"PREAUTH_DATE\", DateType(), False),\n    StructField(\"PREAUTH_AMT\", IntegerType(), False),\n    StructField(\"CLAIM_DATE\", DateType (), False),\n    StructField(\"CLAIM_AMOUNT\", IntegerType(), False),\n    StructField(\"HOSP_NAME\", StringType(), False),\n    StructField(\"HOSP_TYPE\", StringType(), False),\n    StructField(\"HOSP_DISTRICT\", StringType(), False),\n    StructField(\"SURGERY_DATE\", DateType(), False),\n    StructField(\"DISCHARGE_DATE\", DateType(), False),\n    StructField(\"Mortality Y / N\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./hospital_data.csv\")\ncols = dataset.columns\n40/68:\ncategoricalColumns = [\n    \"SEX\",\n    \"CASTE_NAME\",\n    \"CATEGORY_CODE\",\n    \"SURGERY_CODE\",\n    \"DISTRICT_NAME\",\n    \"HOSP_NAME\",\n    \"HOSP_TYPE\",\n    \"HOSP_DISTRICT\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n40/69:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"Mortality Y / N\", outputCol=\"label\")\nstages += [label_stringIdx]\n40/70:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"AGE\",\n\"PREAUTH_AMT\",\n\"CLAIM_AMOUNT\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n40/71:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n40/72:\n# Fit model to prepped data\nlrModel = LogisticRegression().fit(preppedDataDF)\n\n# ROC for training data\ndisplay(lrModel, preppedDataDF, \"ROC\")\n40/73:\nfrom pyspark.sql import *\ndisplay(lrModel.toPandas(), preppedDataDF, \"ROC\")\n40/74:\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(5,5))\nplt.plot([0, 1], [0, 1], 'r--')\nplt.plot(lrModel.summary.roc.select('FPR').collect(),\n         lrModel.summary.roc.select('TPR').collect())\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()\n40/75:\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(5,5))\nplt.plot([0, 1], [0, 1], 'r--')\nplt.plot(lrModel.summary.roc.select('FPR').collect(),\n         lrModel.summary.roc.select('TPR').collect())\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()\n40/76:\n# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + cols\ndataset = preppedDataDF.select(selectedcols)\ndisplay(dataset)\n40/77:\n### Randomly split data into training and test sets. set seed for reproducibility\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\nprint(trainingData.count())\nprint(testData.count())\n40/78:\n# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + cols\ndataset = preppedDataDF.select(selectedcols)\ndisplay(dataset.limit(10).toPandas())\n40/79:\n### Randomly split data into training and test sets. set seed for reproducibility\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\nprint(trainingData.count())\nprint(testData.count())\n40/80:\nfrom pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(trainingData)\n40/81: predictions = lrModel.transform(testData)\n40/82: predictions = lrModel.transform(testData)\n40/83:\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\ndisplay(selected)\n40/84:\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"AGE\", \"SEX\")\ndisplay(selected.limit(20).toPandas())\n40/85:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)\n40/86: evaluator.getMetricName()\n40/87: print(lr.explainParams())\n40/88:\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Create ParamGrid for Cross Validation\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(lr.maxIter, [1, 5, 10])\n             .build())\n40/89:\n# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData)\n# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n42/1: sleep(3000)\n42/2:\nimport time\ntime.sleep(10000)\n43/1: print(\"1\")\n43/2:\nimport time\ntime.sleep(5)\nprint(\"2\")\n43/3: print(\"1\")\n40/90:\n# Use test set to measure the accuracy of our model on new data\npredictions = cvModel.transform(testData)\n40/91:\n# cvModel uses the best model found from the Cross Validation\n# Evaluate best model\nevaluator.evaluate(predictions)\n40/92: print('Model Intercept: ', cvModel.bestModel.intercept)\n40/93:\nweights = cvModel.bestModel.coefficients\nweights = [(float(w),) for w in weights]  # convert numpy type to float, and to tuple\nweightsDF = sqlContext.createDataFrame(weights, [\"Feature Weight\"])\ndisplay(weightsDF)\n40/94:\nsqlContext = SqlContext(spark)\nweights = cvModel.bestModel.coefficients\nweights = [(float(w),) for w in weights]  # convert numpy type to float, and to tuple\nweightsDF = sqlContext.createDataFrame(weights, [\"Feature Weight\"])\ndisplay(weightsDF)\n40/95:\nfrom pyspark.sql import SqlContext\nsqlContext = SqlContext(spark)\nweights = cvModel.bestModel.coefficients\nweights = [(float(w),) for w in weights]  # convert numpy type to float, and to tuple\nweightsDF = sqlContext.createDataFrame(weights, [\"Feature Weight\"])\ndisplay(weightsDF)\n40/96:\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\nweights = cvModel.bestModel.coefficients\nweights = [(float(w),) for w in weights]  # convert numpy type to float, and to tuple\nweightsDF = sqlContext.createDataFrame(weights, [\"Feature Weight\"])\ndisplay(weightsDF)\n40/97:\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\nweights = cvModel.bestModel.coefficients\nweights = [(float(w),) for w in weights]  # convert numpy type to float, and to tuple\nweightsDF = sqlContext.createDataFrame(weights, [\"Feature Weight\"])\ndisplay(weightsDF.limit(20).toPandas())\n40/98:\n# View best model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"AGE\", \"SEX\")\ndisplay(selected)\n40/99:\n# View best model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"AGE\", \"SEX\")\ndisplay(selected.limit(20).toPandas())\n40/100:\n# View best model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"AGE\", \"SEX\")\n#display(selected.limit(20).toPandas())\n40/101:\n\nfrom pyspark.ml.classification import DecisionTreeClassifier\n\n# Create initial Decision Tree Model\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n\n# Train model with Training Data\ndtModel = dt.fit(trainingData)\n40/102:\nprint(\"numNodes = \", dtModel.numNodes)\nprint(\"depth = \", dtModel.depth)\n40/103: display(dtModel.limit(20).toPandas())\n40/104: display(dtModel)\n40/105: predictions = dtModel.transform(testData)\n40/106: predictions.printSchema()\n40/107:\n# View model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\ndisplay(selected)\n40/108:\n# View model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"AGE\", \"SEX\")\ndisplay(selected)\n40/109:\n# View model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"AGE\", \"SEX\")\ndisplay(selected.limit(20).toPandas())\n40/110:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n# Evaluate model\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(predictions)\n40/111: dt.getImpurity()\n40/112:\n# Create ParamGrid for Cross Validation\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nparamGrid = (ParamGridBuilder()\n             .addGrid(dt.maxDepth, [1, 2, 6, 10])\n             .addGrid(dt.maxBins, [20, 40, 80])\n             .build())\n40/113:\n# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData)\n# Takes ~5 minutes\n48/1:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n48/2:\nschema = StructType([\n    StructField(\"ID\", IntegerType(), False),\n    StructField(\"AGE\", IntegerType(), False),\n    StructField(\"SEX\", StringType(), False),\n    StructField(\"CASTE_NAME\", StringType(), False),\n    StructField(\"CATEGORY_CODE\", StringType(), False),\n    StructField(\"CATEGORY_NAME\", StringType(), False),\n    StructField(\"SURGERY_CODE\", StringType(), False),\n    StructField(\"SURGERY\", StringType(), False),\n    StructField(\"DISTRICT_NAME\", StringType(), False),\n    StructField(\"PREAUTH_DATE\", DateType(), False),\n    StructField(\"PREAUTH_AMT\", IntegerType(), False),\n    StructField(\"CLAIM_DATE\", DateType (), False),\n    StructField(\"CLAIM_AMOUNT\", IntegerType(), False),\n    StructField(\"HOSP_NAME\", StringType(), False),\n    StructField(\"HOSP_TYPE\", StringType(), False),\n    StructField(\"HOSP_DISTRICT\", StringType(), False),\n    StructField(\"SURGERY_DATE\", DateType(), False),\n    StructField(\"DISCHARGE_DATE\", DateType(), False),\n    StructField(\"Mortality Y / N\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./hospital_data_low.csv\")\ncols = dataset.columns\n48/3: dataset.limit(10).toPandas()\n48/4:\ncategoricalColumns = [\n    \"SEX\",\n    \"CASTE_NAME\",\n    \"CATEGORY_CODE\",\n    \"SURGERY_CODE\",\n    \"DISTRICT_NAME\",\n    \"HOSP_NAME\",\n    \"HOSP_TYPE\",\n    \"HOSP_DISTRICT\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n48/5:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"Mortality Y / N\", outputCol=\"label\")\nstages += [label_stringIdx]\n48/6:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"AGE\",\n\"PREAUTH_AMT\",\n\"CLAIM_AMOUNT\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n48/7:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n48/8:\n# Fit model to prepped data\nlrModel = LogisticRegression().fit(preppedDataDF)\n\n# ROC for training data\ndisplay(lrModel, preppedDataDF, \"ROC\")\n48/9:\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(5,5))\nplt.plot([0, 1], [0, 1], 'r--')\nplt.plot(lrModel.summary.roc.select('FPR').collect(),\n         lrModel.summary.roc.select('TPR').collect())\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()\n48/10:\n# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + cols\ndataset = preppedDataDF.select(selectedcols)\ndisplay(dataset.limit(10).toPandas())\n48/11:\n### Randomly split data into training and test sets. set seed for reproducibility\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\nprint(trainingData.count())\nprint(testData.count())\n48/12:\nfrom pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(trainingData)\n48/13: predictions = lrModel.transform(testData)\n48/14:\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"AGE\", \"SEX\")\ndisplay(selected.limit(20).toPandas())\n48/15:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)\n48/16: evaluator.getMetricName()\n48/17: print(lr.explainParams())\n48/18:\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Create ParamGrid for Cross Validation\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(lr.maxIter, [1, 5, 10])\n             .build())\n48/19:\n# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData)\n# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n48/20:\n# Use test set to measure the accuracy of our model on new data\npredictions = cvModel.transform(testData)\n48/21:\n# cvModel uses the best model found from the Cross Validation\n# Evaluate best model\nevaluator.evaluate(predictions)\n48/22: print('Model Intercept: ', cvModel.bestModel.intercept)\n48/23:\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\nweights = cvModel.bestModel.coefficients\nweights = [(float(w),) for w in weights]  # convert numpy type to float, and to tuple\nweightsDF = sqlContext.createDataFrame(weights, [\"Feature Weight\"])\ndisplay(weightsDF.limit(20).toPandas())\n48/24:\n# View best model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"AGE\", \"SEX\")\ndisplay(selected.limit(20).toPandas())\n48/25:\n\nfrom pyspark.ml.classification import DecisionTreeClassifier\n\n# Create initial Decision Tree Model\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n\n# Train model with Training Data\ndtModel = dt.fit(trainingData)\n48/26:\nprint(\"numNodes = \", dtModel.numNodes)\nprint(\"depth = \", dtModel.depth)\n48/27: display(dtModel)\n48/28: predictions = dtModel.transform(testData)\n48/29: predictions.printSchema()\n48/30:\n# View model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"AGE\", \"SEX\")\ndisplay(selected.limit(20).toPandas())\n48/31:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n# Evaluate model\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(predictions)\n48/32: dt.getImpurity()\n48/33:\n# Create ParamGrid for Cross Validation\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nparamGrid = (ParamGridBuilder()\n             .addGrid(dt.maxDepth, [1, 2, 6, 10])\n             .addGrid(dt.maxBins, [20, 40, 80])\n             .build())\n48/34:\n# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData)\n# Takes ~5 minutes\n48/35:\nprint(\"numNodes = \", cvModel.bestModel.numNodes)\nprint(\"depth = \", cvModel.bestModel.depth)\n48/36: predictions = cvModel.transform(testData)\n48/37: evaluator.evaluate(predictions)\n48/38:\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"AGE\", \"SEX\")\ndisplay(selected.limit(20).toPandas())\n49/1:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n49/2:\nschema = StructType([\n    StructField(\"ID\", IntegerType(), False),\n    StructField(\"AGE\", IntegerType(), False),\n    StructField(\"SEX\", StringType(), False),\n    StructField(\"CASTE_NAME\", StringType(), False),\n    StructField(\"CATEGORY_CODE\", StringType(), False),\n    StructField(\"CATEGORY_NAME\", StringType(), False),\n    StructField(\"SURGERY_CODE\", StringType(), False),\n    StructField(\"SURGERY\", StringType(), False),\n    StructField(\"DISTRICT_NAME\", StringType(), False),\n    StructField(\"PREAUTH_DATE\", DateType(), False),\n    StructField(\"PREAUTH_AMT\", IntegerType(), False),\n    StructField(\"CLAIM_DATE\", DateType (), False),\n    StructField(\"CLAIM_AMOUNT\", IntegerType(), False),\n    StructField(\"HOSP_NAME\", StringType(), False),\n    StructField(\"HOSP_TYPE\", StringType(), False),\n    StructField(\"HOSP_DISTRICT\", StringType(), False),\n    StructField(\"SURGERY_DATE\", DateType(), False),\n    StructField(\"DISCHARGE_DATE\", DateType(), False),\n    StructField(\"Mortality Y / N\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./hospital_data.csv\")\ncols = dataset.columns\n49/3: dataset.limit(10).toPandas()\n49/4:\ncategoricalColumns = [\n    \"SEX\",\n    \"CASTE_NAME\",\n    \"CATEGORY_CODE\",\n    \"SURGERY_CODE\",\n    \"DISTRICT_NAME\",\n    \"HOSP_NAME\",\n    \"HOSP_TYPE\",\n    \"HOSP_DISTRICT\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n49/5:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"Mortality Y / N\", outputCol=\"label\")\nstages += [label_stringIdx]\n49/6:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"AGE\",\n\"PREAUTH_AMT\",\n\"CLAIM_AMOUNT\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n49/7:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n49/8:\n# Fit model to prepped data\nlrModel = LogisticRegression().fit(preppedDataDF)\n\n# ROC for training data\ndisplay(lrModel, preppedDataDF, \"ROC\")\n49/9:\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(5,5))\nplt.plot([0, 1], [0, 1], 'r--')\nplt.plot(lrModel.summary.roc.select('FPR').collect(),\n         lrModel.summary.roc.select('TPR').collect())\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()\n49/10:\n# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + cols\ndataset = preppedDataDF.select(selectedcols)\ndisplay(dataset.limit(10).toPandas())\n49/11:\n### Randomly split data into training and test sets. set seed for reproducibility\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\nprint(trainingData.count())\nprint(testData.count())\n49/12:\nfrom pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(trainingData)\n49/13: predictions = lrModel.transform(testData)\n49/14:\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"AGE\", \"SEX\")\ndisplay(selected.limit(20).toPandas())\n49/15:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)\n49/16: evaluator.getMetricName()\n49/17: print(lr.explainParams())\n49/18:\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Create ParamGrid for Cross Validation\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(lr.maxIter, [1, 5, 10])\n             .build())\n49/19:\n# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData)\n# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n49/20:\n# Use test set to measure the accuracy of our model on new data\npredictions = cvModel.transform(testData)\n49/21:\n# cvModel uses the best model found from the Cross Validation\n# Evaluate best model\nevaluator.evaluate(predictions)\n49/22: print('Model Intercept: ', cvModel.bestModel.intercept)\n49/23:\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\nweights = cvModel.bestModel.coefficients\nweights = [(float(w),) for w in weights]  # convert numpy type to float, and to tuple\nweightsDF = sqlContext.createDataFrame(weights, [\"Feature Weight\"])\ndisplay(weightsDF.limit(20).toPandas())\n49/24:\n# View best model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"AGE\", \"SEX\")\ndisplay(selected.limit(20).toPandas())\n49/25:\n\nfrom pyspark.ml.classification import DecisionTreeClassifier\n\n# Create initial Decision Tree Model\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n\n# Train model with Training Data\ndtModel = dt.fit(trainingData)\n49/26:\nprint(\"numNodes = \", dtModel.numNodes)\nprint(\"depth = \", dtModel.depth)\n49/27: display(dtModel)\n49/28: predictions = dtModel.transform(testData)\n49/29: predictions.printSchema()\n49/30:\n# View model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"AGE\", \"SEX\")\ndisplay(selected.limit(20).toPandas())\n49/31:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n# Evaluate model\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(predictions)\n49/32: dt.getImpurity()\n49/33:\n# Create ParamGrid for Cross Validation\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nparamGrid = (ParamGridBuilder()\n             .addGrid(dt.maxDepth, [1, 2, 6, 10])\n             .addGrid(dt.maxBins, [20, 40, 80])\n             .build())\n49/34:\n# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData)\n# Takes ~5 minutes\n49/35:\nprint(\"numNodes = \", cvModel.bestModel.numNodes)\nprint(\"depth = \", cvModel.bestModel.depth)\n49/36: predictions = cvModel.transform(testData)\n49/37: evaluator.evaluate(predictions)\n49/38:\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"AGE\", \"SEX\")\ndisplay(selected.limit(20).toPandas())\n50/1:\nimport numpy as np\nimport matplotlib.pyplot as plt\n50/2:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        return np.sqrt(np.sum((a-b)**2,axis=1))\n        \n    def fit(self, x):\n        mean = np.mean(data, axis = 0)\n        std = np.std(data, axis = 0)\n        centers = np.random.randn(x, self.n_clusters.shape[1])*std + mean #kmeans++\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(distances, axis = 1)\n            \n            for i in range(k):\n                new_cluster_centers[i] = np.mean(data[new_clusters == i], axis=0)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/3:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/4:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        return np.sqrt(np.sum((a-b)**2,axis=1))\n        \n    def fit(self, x):\n        mean = np.mean(self.n_clusters, axis = 0)\n        std = np.std(self.n_clusters, axis = 0)\n        centers = np.random.randn(x, self.n_clusters.shape[1])*std + mean #kmeans++\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(distances, axis = 1)\n            \n            for i in range(k):\n                new_cluster_centers[i] = np.mean(data[new_clusters == i], axis=0)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/5:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/6:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        return np.sqrt(np.sum((a-b)**2,axis=1))\n        \n    def fit(self, x):\n        print(self.n_clusters.shape)\n        mean = np.mean(self.n_clusters, axis = 0)\n        std = np.std(self.n_clusters, axis = 0)\n        centers = np.random.randn(x, self.n_clusters.shape[1])*std + mean #kmeans++\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(distances, axis = 1)\n            \n            for i in range(k):\n                new_cluster_centers[i] = np.mean(data[new_clusters == i], axis=0)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/7:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/8:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        return np.sqrt(np.sum((a-b)**2,axis=1))\n        \n    def fit(self, x):\n        print(self.x.shape)\n        mean = np.mean(self.n_clusters, axis = 0)\n        std = np.std(self.n_clusters, axis = 0)\n        centers = np.random.randn(x, self.n_clusters.shape[1])*std + mean #kmeans++\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(distances, axis = 1)\n            \n            for i in range(k):\n                new_cluster_centers[i] = np.mean(data[new_clusters == i], axis=0)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/9:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/10:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        return np.sqrt(np.sum((a-b)**2,axis=1))\n        \n    def fit(self, x):\n        print(x.shape)\n        mean = np.mean(self.n_clusters, axis = 0)\n        std = np.std(self.n_clusters, axis = 0)\n        centers = np.random.randn(x, self.n_clusters.shape[1])*std + mean #kmeans++\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(distances, axis = 1)\n            \n            for i in range(k):\n                new_cluster_centers[i] = np.mean(data[new_clusters == i], axis=0)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/11:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/12:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        return np.sqrt(np.sum((a-b)**2,axis=1))\n        \n    def fit(self, x):\n        print(x.shape)\n        mean = np.mean(x, axis = 0)\n        std = np.std(x, axis = 0)\n        centers = np.random.randn(x, x.shape[1])*std + mean #kmeans++\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(distances, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(data[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/13:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/14:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        return np.sqrt(np.sum((a-b)**2,axis=1))\n        \n    def fit(self, x):\n        print(x.shape)\n        #mean = np.mean(x, axis = 0)\n        #std = np.std(x, axis = 0)\n        #centers = np.random.randn(x, x.shape[1])*std + mean #kmeans++\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(distances, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(data[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/15:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/16:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        return np.sqrt(np.sum((p1 - p2)**2,axis=1))\n        \n    def fit(self, x):\n        print(x.shape)\n        #mean = np.mean(x, axis = 0)\n        #std = np.std(x, axis = 0)\n        #centers = np.random.randn(x, x.shape[1])*std + mean #kmeans++\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(distances, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(data[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/17:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/18:\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import distance_matrix\n50/19:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        return distance_matrix(p1, p2)\n        #return np.sqrt(np.sum((p1 - p2)**2,axis=1))\n        \n    def fit(self, x):\n        print(x.shape)\n        #mean = np.mean(x, axis = 0)\n        #std = np.std(x, axis = 0)\n        #centers = np.random.randn(x, x.shape[1])*std + mean #kmeans++\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(distances, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(data[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/20:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/21:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        return distance_matrix(p1, p2)\n        #return np.sqrt(np.sum((p1 - p2)**2,axis=1))\n        \n    def fit(self, x):\n        print(x.shape)\n        #mean = np.mean(x, axis = 0)\n        #std = np.std(x, axis = 0)\n        #centers = np.random.randn(x, x.shape[1])*std + mean #kmeans++\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(distances, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(data[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/22:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/23:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        return distance_matrix(p1, p2)\n        #return np.sqrt(np.sum((p1 - p2)**2,axis=1))\n        \n    def fit(self, x):\n        print(x.shape)\n        #mean = np.mean(x, axis = 0)\n        #std = np.std(x, axis = 0)\n        #centers = np.random.randn(x, x.shape[1])*std + mean #kmeans++\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(data[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/24:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/25:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        return distance_matrix(p1, p2)\n        #return np.sqrt(np.sum((p1 - p2)**2,axis=1))\n        \n    def fit(self, x):\n        print(x.shape)\n        #mean = np.mean(x, axis = 0)\n        #std = np.std(x, axis = 0)\n        #centers = np.random.randn(x, x.shape[1])*std + mean #kmeans++\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/26:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n51/1: import numpy as np\n51/2: import numpy as np\n51/3: ar = np.array([[0, 1], [0, 2], [3, 3]])\n51/4:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.repeat(ar, repeats=10, axis=0)\n51/5:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.repeat(ar, repeats=10, axis=1)\n51/6:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.repeat(ar, repeats=10, axis=0)\n51/7:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.repeat(ar, repeats=10, axis=0).reshape(10, 2)\n51/8:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.repeat(ar, repeats=10, axis=0).reshape(10)\n51/9:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.repeat(ar, repeats=10, axis=0).arange(10)\n51/10:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.repeat(ar, repeats=10, axis=0).reshape((10, 3))\n51/11:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.repeat(ar, repeats=10, axis=0).shape\n51/12:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.repeat(ar, repeats=10, axis=0).reshape((10, 3, 2))\n51/13:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.repeat(ar, repeats=10, axis=1).reshape((10, 3, 2))\n51/14:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.repeat(ar, repeats=10, axis=0).reshape((10, 3, 2))\n51/15:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.repeat(ar, repeats=10, axis=0).reshape((10, 2, 3))\n51/16:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.repeat(ar, repeats=10, axis=0)#.reshape((10, 2, 3))\n51/17:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.repeat(ar, repeats=10, axis=1)#.reshape((10, 2, 3))\n51/18:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.repeat(ar, repeats=10, axis=2)#.reshape((10, 2, 3))\n51/19:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.repeat(ar, repeats=10, axis=1)#.reshape((10, 2, 3))\n51/20:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.repeat(ar, repeats=10, axis=0)#.reshape((10, 2, 3))\n51/21:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.repeat(ar.transopse(), repeats=10, axis=0)#.reshape((10, 2, 3))\n51/22:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.repeat(ar.transpose(), repeats=10, axis=0)#.reshape((10, 2, 3))\n51/23:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.repeat(ar.transpose(), repeats=10, axis=1)#.reshape((10, 2, 3))\n51/24:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.repeat(ar.transpose(), repeats=10, axis=0)#.reshape((10, 2, 3))\n51/25:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nar.tile((1, 10))\n\nnp.repeat(ar.transpose(), repeats=10, axis=0)#.reshape((10, 2, 3))\n51/26:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.tile(ar, (1, 10))\n\nnp.repeat(ar.transpose(), repeats=10, axis=0)#.reshape((10, 2, 3))\n51/27:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.tile(ar, (1, 10))\n\n#np.repeat(ar.transpose(), repeats=10, axis=0)#.reshape((10, 2, 3))\n51/28:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.tile(ar, (10, 1))\n\n#np.repeat(ar.transpose(), repeats=10, axis=0)#.reshape((10, 2, 3))\n51/29:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.tile(ar, (10, 1)).reshape((10, 2, 3))\n\n#np.repeat(ar.transpose(), repeats=10, axis=0)#.reshape((10, 2, 3))\n51/30:\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.tile(ar, (10, 1)).reshape((10, 3, 2))\n\n#np.repeat(ar.transpose(), repeats=10, axis=0)#.reshape((10, 2, 3))\n51/31:\n\n\nar = np.array([[0, 1], [0, 2], [3, 3]])\nnp.tile(ar, (10, 1))#.reshape((10, 3, 2))\n\n#np.repeat(ar.transpose(), repeats=10, axis=0)#.reshape((10, 2, 3))\n50/27:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        #return distance_matrix(p1, p2)\n        np.tile(p2, (p1.shape[0], 1))\n        return np.sqrt(np.sum((p1 - p2)**2,axis=1))\n        \n    def fit(self, x):\n        #mean = np.mean(x, axis = 0)\n        #std = np.std(x, axis = 0)\n        #centers = np.random.randn(x, x.shape[1])*std + mean #kmeans++\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/28:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        #return distance_matrix(p1, p2)\n        tiledP2 = np.tile(p2, (p1.shape[0], 1))\n        return np.sqrt(np.sum((p1 - tiledP2)**2,axis=1))\n        \n    def fit(self, x):\n        #mean = np.mean(x, axis = 0)\n        #std = np.std(x, axis = 0)\n        #centers = np.random.randn(x, x.shape[1])*std + mean #kmeans++\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/29:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/30:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        #return distance_matrix(p1, p2)\n        print(p1.shape)\n        print(p2.shape)\n        tiledP2 = np.tile(p2, (p1.shape[0], 1))\n        print(tiledP2.shape)\n        return np.sqrt(np.sum((p1 - tiledP2)**2,axis=1))\n        \n    def fit(self, x):\n        #mean = np.mean(x, axis = 0)\n        #std = np.std(x, axis = 0)\n        #centers = np.random.randn(x, x.shape[1])*std + mean #kmeans++\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/31:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/32:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        #return distance_matrix(p1, p2)\n        print(p1.shape)\n        print(np.array([p2]).shape)\n        tiledP2 = np.tile(p2, (p1.shape[0], 1))\n        print(tiledP2.shape)\n        return np.sqrt(np.sum((p1 - tiledP2)**2,axis=1))\n        \n    def fit(self, x):\n        #mean = np.mean(x, axis = 0)\n        #std = np.std(x, axis = 0)\n        #centers = np.random.randn(x, x.shape[1])*std + mean #kmeans++\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/33:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/34:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        #return distance_matrix(p1, p2)\n        print(p1.shape)\n        print(np.array(p2).shape)\n        tiledP2 = np.tile(p2, (p1.shape[0], 1))\n        print(tiledP2.shape)\n        return np.sqrt(np.sum((p1 - tiledP2)**2,axis=1))\n        \n    def fit(self, x):\n        #mean = np.mean(x, axis = 0)\n        #std = np.std(x, axis = 0)\n        #centers = np.random.randn(x, x.shape[1])*std + mean #kmeans++\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/35:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/36:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        for i in range(p2.size):\n            distances[:,i] = np.linalg.norm(p1 - p2[i], axis=1)\n        return distances\n        #return distance_matrix(p1, p2)\n#         print(p1.shape)\n#         print(np.array(p2).shape)\n#         tiledP2 = np.tile(p2, (p1.shape[0], 1))\n#         print(tiledP2.shape)\n#         return np.sqrt(np.sum((p1 - tiledP2)**2,axis=1))\n        \n    def fit(self, x):\n        #mean = np.mean(x, axis = 0)\n        #std = np.std(x, axis = 0)\n        #centers = np.random.randn(x, x.shape[1])*std + mean #kmeans++\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/37:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/38:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        for i in range(len(p2)):\n            distances[:,i] = np.linalg.norm(p1 - p2[i], axis=1)\n        return distances\n        #return distance_matrix(p1, p2)\n#         print(p1.shape)\n#         print(np.array(p2).shape)\n#         tiledP2 = np.tile(p2, (p1.shape[0], 1))\n#         print(tiledP2.shape)\n#         return np.sqrt(np.sum((p1 - tiledP2)**2,axis=1))\n        \n    def fit(self, x):\n        #mean = np.mean(x, axis = 0)\n        #std = np.std(x, axis = 0)\n        #centers = np.random.randn(x, x.shape[1])*std + mean #kmeans++\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/39:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/40:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        distances = []\n        for i in range(len(p2)):\n            distances[:,i] = np.linalg.norm(p1 - p2[i], axis=1)\n        return distances\n        #return distance_matrix(p1, p2)\n#         print(p1.shape)\n#         print(np.array(p2).shape)\n#         tiledP2 = np.tile(p2, (p1.shape[0], 1))\n#         print(tiledP2.shape)\n#         return np.sqrt(np.sum((p1 - tiledP2)**2,axis=1))\n        \n    def fit(self, x):\n        #mean = np.mean(x, axis = 0)\n        #std = np.std(x, axis = 0)\n        #centers = np.random.randn(x, x.shape[1])*std + mean #kmeans++\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/41:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/42:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        distances = np.matrix()\n        for i in range(len(p2)):\n            distances[:,i] = np.linalg.norm(p1 - p2[i], axis=1)\n        return distances\n        #return distance_matrix(p1, p2)\n#         print(p1.shape)\n#         print(np.array(p2).shape)\n#         tiledP2 = np.tile(p2, (p1.shape[0], 1))\n#         print(tiledP2.shape)\n#         return np.sqrt(np.sum((p1 - tiledP2)**2,axis=1))\n        \n    def fit(self, x):\n        #mean = np.mean(x, axis = 0)\n        #std = np.std(x, axis = 0)\n        #centers = np.random.randn(x, x.shape[1])*std + mean #kmeans++\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/43:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/44:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        distances = np.zeros((p1.shape[0],len(p2)))\n        for i in range(len(p2)):\n            distances[:,i] = np.linalg.norm(p1 - p2[i], axis=1)\n        return distances\n        #return distance_matrix(p1, p2)\n#         print(p1.shape)\n#         print(np.array(p2).shape)\n#         tiledP2 = np.tile(p2, (p1.shape[0], 1))\n#         print(tiledP2.shape)\n#         return np.sqrt(np.sum((p1 - tiledP2)**2,axis=1))\n        \n    def fit(self, x):\n        #mean = np.mean(x, axis = 0)\n        #std = np.std(x, axis = 0)\n        #centers = np.random.randn(x, x.shape[1])*std + mean #kmeans++\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/45:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/46:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        distances = np.zeros((p1.shape[0],len(p2)))\n        for i in range(len(p2)):\n            distances[:,i] = np.sqrt((p1 - p2[i])**2, axis = 1)\n        return distances\n        #return distance_matrix(p1, p2)\n#         print(p1.shape)\n#         print(np.array(p2).shape)\n#         tiledP2 = np.tile(p2, (p1.shape[0], 1))\n#         print(tiledP2.shape)\n#         return np.sqrt(np.sum((p1 - tiledP2)**2,axis=1))\n        \n    def fit(self, x):\n        #mean = np.mean(x, axis = 0)\n        #std = np.std(x, axis = 0)\n        #centers = np.random.randn(x, x.shape[1])*std + mean #kmeans++\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/47:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/48:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        distances = np.zeros((p1.shape[0],len(p2)))\n        for i in range(len(p2)):\n            distances[:,i] = np.sqrt((p1 - p2[i])**2)\n        return distances\n        #return distance_matrix(p1, p2)\n#         print(p1.shape)\n#         print(np.array(p2).shape)\n#         tiledP2 = np.tile(p2, (p1.shape[0], 1))\n#         print(tiledP2.shape)\n#         return np.sqrt(np.sum((p1 - tiledP2)**2,axis=1))\n        \n    def fit(self, x):\n        #mean = np.mean(x, axis = 0)\n        #std = np.std(x, axis = 0)\n        #centers = np.random.randn(x, x.shape[1])*std + mean #kmeans++\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/49:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/50:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        distances = np.zeros((p1.shape[0],len(p2)))\n        for i in range(len(p2)):\n            distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n            #np.sqrt((p1 - p2[i])**2)\n        return distances\n        #return distance_matrix(p1, p2)\n#         print(p1.shape)\n#         print(np.array(p2).shape)\n#         tiledP2 = np.tile(p2, (p1.shape[0], 1))\n#         print(tiledP2.shape)\n#         return np.sqrt(np.sum((p1 - tiledP2)**2,axis=1))\n        \n    def fit(self, x):\n        #mean = np.mean(x, axis = 0)\n        #std = np.std(x, axis = 0)\n        #centers = np.random.randn(x, x.shape[1])*std + mean #kmeans++\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/51:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/52:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        distances = np.zeros((p1.shape[0],len(p2)))\n        for i in range(len(p2)):\n            distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n        return distances\n        \n    def fit(self, x):\n        mean = np.mean(x, axis = 0)\n        std = np.std(x, axis = 0)\n        centers = np.random.randn(x, x.shape[1])*std + mean #kmeans++\n        #centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/53:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/54:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        distances = np.zeros((p1.shape[0],len(p2)))\n        for i in range(len(p2)):\n            distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n        return distances\n        \n    def fit(self, x):\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/55:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/56:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('./eye-regular.png')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 3)\n50/57:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('./pencil-alt-solid.png')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 3)\n50/58:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('./pencil-alt-solid.png')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(1, 3)\n50/59:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('./pencil-alt-solid.png')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 3)\n50/60:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('./pencil-alt-solid.png')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 3, -1)\n50/61:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('./pencil-alt-solid.png')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 3)\n50/62:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('./add.png')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 3)\n50/63:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('./vs_icon.jpg')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 3)\n50/64:\nks = list(range(2, 17))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n50/65:\nks = list(range(2, 17))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n50/66:\nclass PCA:\n    \n    def __init__(self, n_components):\n        self.n_components = n_components\n        \n    def fit(self, X):\n        self.mean_ = X.mean(axis=0)\n        Xc = X - self.mean_\n        C = (Xc.transpose() @ Xc) / (X.shape[0] - 1)\n        \n        self.components_ = np.linalg.eig(C)\n\n        return self\n        \n    def transform(self, X):\n        return (X - self.mean_) @ self.components_.transpose()\n50/67:\nwith np.load('mnist.npz') as npz:\n    x, y = [npz[k] for k in ['x_train', 'y_train']]\n\nx = x.reshape(-1, 784).astype(np.float32)\n50/68:\npca = PCA(n_components=2).fit(x)\nx_proj = pca.transform(x)\n50/69:\nclass PCA:\n    \n    def __init__(self, n_components):\n        self.n_components = n_components\n        \n    def fit(self, X):\n        self.mean_ = X.mean(axis=0)\n        Xc = X - self.mean_\n        C = (Xc.transpose() @ Xc) / (X.shape[0] - 1)\n        \n        self.components_ = np.linalg.eig(C)\n\n        return self\n        \n    def transform(self, X):\n        return (X - self.mean_) @ self.components_.transpose()\n50/70:\npca = PCA(n_components=2).fit(x)\nx_proj = pca.transform(x)\n50/71:\nclass PCA:\n    \n    def __init__(self, n_components):\n        self.n_components = n_components\n        \n    def fit(self, X):\n        self.mean_ = X.mean(axis=0)\n        Xc = X - self.mean_\n        C = (Xc.transpose() @ Xc) / (X.shape[0] - 1)\n        \n        self.components_ = np.linalg.eig(C)\n\n        return self\n        \n    def transform(self, X):\n        return (X - self.mean_) @ self.components_.T\n50/72:\npca = PCA(n_components=2).fit(x)\nx_proj = pca.transform(x)\n50/73:\nclass PCA:\n    \n    def __init__(self, n_components):\n        self.n_components = n_components\n        \n    def fit(self, X):\n        self.mean_ = X.mean(axis=0)\n        Xc = X - self.mean_\n        C = (Xc.transpose() @ Xc) / (X.shape[0] - 1)\n        \n        (w, v) = np.linalg.eig(C)\n        self.components_ = v.transpose()[w.argsort()[-self.n_components:][::-1]]\n\n        return self\n        \n    def transform(self, X):\n        return (X - self.mean_) @ self.components_.transpose()\n50/74:\npca = PCA(n_components=2).fit(x)\nx_proj = pca.transform(x)\n50/75:\nplt.figure(figsize=(10, 8))\nplt.scatter(*x_proj.T, c=y, s=2)\nplt.colorbar()\n50/76:\nks = list(range(2, 17))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n50/77:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('./vs_icon.jpg')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 3)\n50/78:\nks = list(range(2, 12))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n50/79:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('./vs_icon.jpg')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 3)\n50/80:\nks = list(range(2, 12))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n50/81:\nks = list(range(2, 17))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n50/82:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('./vs_icon.jpg')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 3)\n50/83:\nks = list(range(2, 17))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n50/84:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('./vs_icon.jpg')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 3)\n50/85:\nks = list(range(2, 17))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n50/86:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('./vs_icon.jpg')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 3)\n50/87:\nks = list(range(2, 17))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n50/88:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('./add.png.jpg')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 4)[:,0:3]\n50/89:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('./add.png')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 4)[:,0:3]\n50/90:\nks = list(range(2, 17))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n50/91:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('./add.png')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 4)[:,0:3]\n50/92:\nks = list(range(2, 3))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n50/93:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('add.png')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 4)[:,0:3]\n50/94:\nks = list(range(2, 3))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n54/1:\nimport numpy as np\nimport matplotlib.pyplot as plt\n54/2:\ndef distance(p1, p2):\n    res = np.empty((p1.shape[0], p2.shape[0]))\n    for i in range(p1.shape[0]):\n        for j in range(p2.shape[0]):\n            res[i][j] = np.sqrt(np.sum((p1[i] - p2[j]) ** 2))\n    return res\n54/3:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n        pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n        return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n        \n    def newClasterCenters(self, x, D):\n        #return np.stack([x[D.argmin(axis=1) == i].mean(axis=0) for i in range(D.shape[1])])\n        classes = np.empty((D.shape[1], x.shape[1]))\n        for i in range(D.shape[1]):\n            currClass = x[D.argmin(axis=1) == i]\n            if currClass.size == 0:\n                currClass = self.cluster_centers_[i][np.newaxis]\n            classes[i] = currClass.mean(axis=0)\n        return classes\n        \n    def fit(self, x):\n        self.cluster_centers_ = x[np.random.choice(x.shape[0], self.n_clusters, replace=False)]\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            \n            new_cluster_centers = self.newClasterCenters(x, D)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n54/4:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n54/5:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('add.png')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 4)[:,0:3]\n54/6:\nks = list(range(2, 12))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n50/95:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n        pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n        return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n        distances = np.zeros((p1.shape[0],len(p2)))\n        for i in range(len(p2)):\n            distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n        return distances\n        \n    def fit(self, x):\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/96:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/97:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        distances = np.zeros((p1.shape[0],len(p2)))\n        for i in range(len(p2)):\n            distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n        return distances\n        \n    def fit(self, x):\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/98:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/99:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/100:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        distances = np.zeros((p1.shape[0],len(p2)))\n        for i in range(len(p2)):\n            distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n        return distances\n        \n    def fit(self, x):\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n#         while True:\n#             D = KMeans.distance(x, self.cluster_centers_)\n            \n#             new_clusters = np.argmin(D, axis = 1)\n            \n#             new_cluster_centers = []\n#             for i in range(self.n_clusters):\n#                 new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n#             if np.allclose(new_cluster_centers, self.cluster_centers_):\n#                 self.cluster_centers_ = new_cluster_centers\n#                 break\n                \n#             self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/101:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/102:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        distances = np.zeros((p1.shape[0],len(p2)))\n        for i in range(len(p2)):\n            distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n        return distances\n        \n    def fit(self, x):\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/103:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n54/7:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n        pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n        return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n        \n    def newClasterCenters(self, x, D):\n        #return np.stack([x[D.argmin(axis=1) == i].mean(axis=0) for i in range(D.shape[1])])\n        classes = np.empty((D.shape[1], x.shape[1]))\n        for i in range(D.shape[1]):\n            currClass = x[D.argmin(axis=1) == i]\n            if currClass.size == 0:\n                currClass = self.cluster_centers_[i][np.newaxis]\n            classes[i] = currClass.mean(axis=0)\n        return classes\n        \n    def fit(self, x):\n        self.cluster_centers_ = x[np.random.choice(x.shape[0], self.n_clusters, replace=False)]\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            \n            new_cluster_centers = self.newClasterCenters(x, D)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n54/8:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/104:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/105:\nx = np.concatenate([np.random.normal(size=(1000, 2)), np.random.normal(size=(1000, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/106:\nx = np.concatenate([np.random.normal(size=(100000, 2)), np.random.normal(size=(100000, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n54/9:\nx = np.concatenate([np.random.normal(size=(100000, 2)), np.random.normal(size=(100000, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/107:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        distances = np.zeros((p1.shape[0],len(p2)))\n        for i in range(len(p2)):\n            distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n        return distances\n        \n    def fit(self, x):\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/108:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/109:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('add.png')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 4)[:,0:3]\n50/110:\nks = list(range(2, 3))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n50/111:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('icon.jpg')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 3)\n50/112:\nks = list(range(2, 3))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n50/113:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        distances = np.zeros((p1.shape[0],len(p2)))\n        for i in range(len(p2)):\n            distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n        return distances\n        \n    def fit(self, x):\n        centers = [x[0], x[1]]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n50/114:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n50/115:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('icon.jpg')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 3)\n50/116:\nks = list(range(2, 3))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n54/10:\nkmeans = KMeans(n_clusters=5).fit(pixels)\npixels_clustered = kmeans.cluster_centers_[kmeans.predict(pixels)]\n\nfig, ax = plt.subplots(ncols=2, figsize=(10, 6))\nax[0].imshow(image.astype(np.uint8))\nax[1].imshow(pixels_clustered.reshape((64, 64, 3)).astype(np.uint8))\n54/11:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('icon.jpg')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 3)\n54/12:\nks = list(range(2, 12))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n54/13:\nkmeans = KMeans(n_clusters=5).fit(pixels)\npixels_clustered = kmeans.cluster_centers_[kmeans.predict(pixels)]\n\nfig, ax = plt.subplots(ncols=2, figsize=(10, 6))\nax[0].imshow(image.astype(np.uint8))\nax[1].imshow(pixels_clustered.reshape((64, 64, 3)).astype(np.uint8))\n55/1:\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import distance_matrix\n55/2:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        distances = np.zeros((p1.shape[0],len(p2)))\n        for i in range(len(p2)):\n            distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n        return distances\n        \n    def fit(self, x):\n        centers = x[0:self.n_clusters]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n55/3:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n55/4:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('icon.jpg')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 3)\n55/5:\nks = list(range(2, 3))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n55/6:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n        pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n        return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n#         distances = np.zeros((p1.shape[0],len(p2)))\n#         for i in range(len(p2)):\n#             distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n#         return distances\n        \n    def fit(self, x):\n        centers = x[0:self.n_clusters]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n55/7:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n55/8:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n        pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n        return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n#         distances = np.zeros((p1.shape[0],len(p2)))\n#         for i in range(len(p2)):\n#             distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n#         return distances\n        \n    def fit(self, x):\n        randomIndices = np.random.choice(x.shape[0], self.n_clusters, replace=False)\n        centers = x[randomIndices]\n        self.cluster_centers_ = centers\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n55/9:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n55/10:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n        pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n        return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n#         distances = np.zeros((p1.shape[0],len(p2)))\n#         for i in range(len(p2)):\n#             distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n#         return distances\n        \n    def fit(self, x):\n        randomIndices = np.random.choice(x.shape[0], self.n_clusters, replace=False)\n        self.cluster_centers_ = x[randomIndices]\n        \n        while True:\n            D = KMeans.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n55/11:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n56/1:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n        pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n        return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n        \n    def newClasterCenters(self, x, D):\n        #return np.stack([x[D.argmin(axis=1) == i].mean(axis=0) for i in range(D.shape[1])])\n        classes = np.empty((D.shape[1], x.shape[1]))\n        for i in range(D.shape[1]):\n            currClass = x[D.argmin(axis=1) == i]\n            if currClass.size == 0:\n                currClass = self.cluster_centers_[i][np.newaxis]\n            classes[i] = currClass.mean(axis=0)\n        return classes\n        \n    def fit(self, x):\n        self.cluster_centers_ = x[np.random.choice(x.shape[0], self.n_clusters, replace=False)]\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            \n            new_cluster_centers = self.newClasterCenters(x, D)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n56/2:\nx = np.concatenate([np.random.normal(size=(100000, 2)), np.random.normal(size=(100000, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n56/3:\nimport numpy as np\nimport matplotlib.pyplot as plt\n56/4:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n        pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n        return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n        \n    def newClasterCenters(self, x, D):\n        #return np.stack([x[D.argmin(axis=1) == i].mean(axis=0) for i in range(D.shape[1])])\n        classes = np.empty((D.shape[1], x.shape[1]))\n        for i in range(D.shape[1]):\n            currClass = x[D.argmin(axis=1) == i]\n            if currClass.size == 0:\n                currClass = self.cluster_centers_[i][np.newaxis]\n            classes[i] = currClass.mean(axis=0)\n        return classes\n        \n    def fit(self, x):\n        self.cluster_centers_ = x[np.random.choice(x.shape[0], self.n_clusters, replace=False)]\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            \n            new_cluster_centers = self.newClasterCenters(x, D)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n56/5:\nx = np.concatenate([np.random.normal(size=(100000, 2)), np.random.normal(size=(100000, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n55/12:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n        pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n        return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n#         distances = np.zeros((p1.shape[0],len(p2)))\n#         for i in range(len(p2)):\n#             distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n#         return distances\n        \n    def fit(self, x):\n        randomIndices = np.random.choice(x.shape[0], self.n_clusters, replace=False)\n        self.cluster_centers_ = x[randomIndices]\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = []\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n55/13:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n55/14:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n        pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n        return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n#         distances = np.zeros((p1.shape[0],len(p2)))\n#         for i in range(len(p2)):\n#             distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n#         return distances\n        \n    def fit(self, x):\n        randomIndices = np.random.choice(x.shape[0], self.n_clusters, replace=False)\n        self.cluster_centers_ = x[randomIndices]\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = np.empty()\n            for i in range(self.n_clusters):\n                new_cluster_centers.append(np.mean(x[new_clusters == i], axis=0))\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n55/15:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n55/16:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n        pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n        return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n#         distances = np.zeros((p1.shape[0],len(p2)))\n#         for i in range(len(p2)):\n#             distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n#         return distances\n        \n    def fit(self, x):\n        randomIndices = np.random.choice(x.shape[0], self.n_clusters, replace=False)\n        self.cluster_centers_ = x[randomIndices]\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = np.empty(i)\n            for i in range(self.n_clusters):\n                new_cluster_centers[i] = (np.mean(x[new_clusters == i], axis=0)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n55/17:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n        pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n        return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n#         distances = np.zeros((p1.shape[0],len(p2)))\n#         for i in range(len(p2)):\n#             distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n#         return distances\n        \n    def fit(self, x):\n        randomIndices = np.random.choice(x.shape[0], self.n_clusters, replace=False)\n        self.cluster_centers_ = x[randomIndices]\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = np.empty(i)\n            for i in range(self.n_clusters):\n                new_cluster_centers[i] = np.mean(x[new_clusters == i], axis=0)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n55/18:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n55/19:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n        pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n        return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n#         distances = np.zeros((p1.shape[0],len(p2)))\n#         for i in range(len(p2)):\n#             distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n#         return distances\n        \n    def fit(self, x):\n        randomIndices = np.random.choice(x.shape[0], self.n_clusters, replace=False)\n        self.cluster_centers_ = x[randomIndices]\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = np.empty(self.n_clusters)\n            for i in range(self.n_clusters):\n                new_cluster_centers[i] = np.mean(x[new_clusters == i], axis=0)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n55/20:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n55/21:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n        pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n        return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n#         distances = np.zeros((p1.shape[0],len(p2)))\n#         for i in range(len(p2)):\n#             distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n#         return distances\n        \n    def fit(self, x):\n        randomIndices = np.random.choice(x.shape[0], self.n_clusters, replace=False)\n        self.cluster_centers_ = x[randomIndices]\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = np.empty(self.n_clusters, x.shape[1])\n            for i in range(self.n_clusters):\n                new_cluster_centers[i] = np.mean(x[new_clusters == i], axis=0)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n55/22:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n55/23:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n        pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n        return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n#         distances = np.zeros((p1.shape[0],len(p2)))\n#         for i in range(len(p2)):\n#             distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n#         return distances\n        \n    def fit(self, x):\n        randomIndices = np.random.choice(x.shape[0], self.n_clusters, replace=False)\n        self.cluster_centers_ = x[randomIndices]\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            \n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = np.empty((self.n_clusters, x.shape[1]))\n            for i in range(self.n_clusters):\n                new_cluster_centers[i] = np.mean(x[new_clusters == i], axis=0)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n55/24:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n55/25:\nks = list(range(2, 10))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n55/26:\nks = list(range(2, 10))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n55/27:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n        pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n        pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n        return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n#         distances = np.zeros((p1.shape[0],len(p2)))\n#         for i in range(len(p2)):\n#             distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n#         return distances\n        \n    def fit(self, x):\n        randomIndices = np.random.choice(x.shape[0], self.n_clusters, replace=False)\n        self.cluster_centers_ = x[randomIndices]\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            print(D.shape)\n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = np.empty((self.n_clusters, x.shape[1]))\n            for i in range(self.n_clusters):\n                new_cluster_centers[i] = np.mean(x[new_clusters == i], axis=0)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n55/28:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n55/29:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n#         pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n#         pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n#         return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n        distances = np.zeros((p1.shape[0],len(p2)))\n        for i in range(len(p2)):\n            distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n        return distances\n        \n    def fit(self, x):\n        randomIndices = np.random.choice(x.shape[0], self.n_clusters, replace=False)\n        self.cluster_centers_ = x[randomIndices]\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            print(D.shape)\n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = np.empty((self.n_clusters, x.shape[1]))\n            for i in range(self.n_clusters):\n                new_cluster_centers[i] = np.mean(x[new_clusters == i], axis=0)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n55/30:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n55/31:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n#         pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n#         pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n#         return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n        distances = np.zeros((p1.shape[0],len(p2)))\n        for i in range(len(p2)):\n            distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n        return distances\n        \n    def fit(self, x):\n        randomIndices = np.random.choice(x.shape[0], self.n_clusters, replace=False)\n        self.cluster_centers_ = x[randomIndices]\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            print(D.shape)\n            new_clusters = np.argmin(D, axis = 0)\n            \n            new_cluster_centers = np.empty((self.n_clusters, x.shape[1]))\n            for i in range(self.n_clusters):\n                new_cluster_centers[i] = np.mean(x[new_clusters == i], axis=0)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n55/32:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n55/33:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n#         pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n#         pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n#         return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n        distances = np.zeros((p1.shape[0],len(p2)))\n        for i in range(len(p2)):\n            distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n        return distances\n        \n    def fit(self, x):\n        randomIndices = np.random.choice(x.shape[0], self.n_clusters, replace=False)\n        self.cluster_centers_ = x[randomIndices]\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            print(D.shape)\n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = np.empty((self.n_clusters, x.shape[1]))\n            for i in range(self.n_clusters):\n                new_cluster_centers[i] = np.mean(x[new_clusters == i], axis=0)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n55/34:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n55/35:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('icon.jpg')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 3)\n55/36:\nks = list(range(2, 10))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n55/37:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n#         pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n#         pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n#         return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n        distances = np.zeros((p1.shape[0],len(p2)))\n        for i in range(len(p2)):\n            distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n        return distances\n        \n    def fit(self, x):\n        randomIndices = np.random.choice(x.shape[0], self.n_clusters, replace=False)\n        self.cluster_centers_ = x[randomIndices]\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = np.empty((self.n_clusters, x.shape[1]))\n            for i in range(self.n_clusters):\n                new_cluster_centers[i] = np.mean(x[new_clusters == i], axis=0)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n55/38:\nks = list(range(2, 10))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n55/39:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n#         pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n#         pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n#         return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n        distances = np.zeros((p1.shape[0],len(p2)))\n        for i in range(len(p2)):\n            distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n        return distances\n        \n    def fit(self, x):\n        randomIndices = np.random.choice(x.shape[0], self.n_clusters, replace=False)\n        self.cluster_centers_ = x[randomIndices]\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = np.empty((self.n_clusters, x.shape[1]))\n            for i in range(self.n_clusters):\n                pretendent = x[new_clusters == i]\n                if(pretendent.size == 0)\n                    pretendent = self.cluster_centers_[i][np.newaxis]\n                new_cluster_centers[i] = np.mean(pretendent, axis=0)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n55/40:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n#         pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n#         pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n#         return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n        distances = np.zeros((p1.shape[0],len(p2)))\n        for i in range(len(p2)):\n            distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n        return distances\n        \n    def fit(self, x):\n        randomIndices = np.random.choice(x.shape[0], self.n_clusters, replace=False)\n        self.cluster_centers_ = x[randomIndices]\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = np.empty((self.n_clusters, x.shape[1]))\n            for i in range(self.n_clusters):\n                pretendent = x[new_clusters == i]\n                if(pretendent.size == 0):\n                    pretendent = self.cluster_centers_[i][np.newaxis]\n                new_cluster_centers[i] = np.mean(pretendent, axis=0)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n55/41:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n55/42:\nks = list(range(2, 10))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n55/43:\nkmeans = KMeans(n_clusters=2).fit(pixels)\npixels_clustered = kmeans.cluster_centers_[kmeans.predict(pixels)]\n\n\nfig, ax = plt.subplots(ncols=2, figsize=(10, 6))\nax[0].imshow(image.astype(np.uint8))\nax[1].imshow(pixels_clustered.reshape(image.shape).astype(np.uint8))\n55/44:\nkmeans = KMeans(n_clusters=3).fit(pixels)\npixels_clustered = kmeans.cluster_centers_[kmeans.predict(pixels)]\n\n\nfig, ax = plt.subplots(ncols=2, figsize=(10, 6))\nax[0].imshow(image.astype(np.uint8))\nax[1].imshow(pixels_clustered.reshape(image.shape).astype(np.uint8))\n55/45:\nkmeans = KMeans(n_clusters=2).fit(pixels)\npixels_clustered = kmeans.cluster_centers_[kmeans.predict(pixels)]\n\n\nfig, ax = plt.subplots(ncols=2, figsize=(10, 6))\nax[0].imshow(image.astype(np.uint8))\nax[1].imshow(pixels_clustered.reshape(image.shape).astype(np.uint8))\n55/46:\nclass KMeans:\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        \n    @staticmethod\n    def distance(p1, p2):\n#         pp1 = np.tile(p1, (p2.shape[0], 1, 1))\n#         pp2 = np.tile(p2, (p1.shape[0], 1, 1)).swapaxes(0, 1)\n#         return np.sqrt(np.sum((pp1 - pp2) ** 2, axis=2)).T\n        distances = np.zeros((p1.shape[0],len(p2)))\n        for i in range(len(p2)):\n            distances[:,i] = np.sqrt(np.sum((p1 - p2[i])**2,axis=1))\n        return distances\n        \n    def fit(self, x):\n        #randomIndices = np.random.choice(x.shape[0], self.n_clusters, replace=False)\n        #self.cluster_centers_ = x[randomIndices]\n        self.cluster_centers_ = x[0:self.n_clusters]\n        \n        while True:\n            D = self.distance(x, self.cluster_centers_)\n            new_clusters = np.argmin(D, axis = 1)\n            \n            new_cluster_centers = np.empty((self.n_clusters, x.shape[1]))\n            for i in range(self.n_clusters):\n                pretendent = x[new_clusters == i]\n                if(pretendent.size == 0):\n                    pretendent = self.cluster_centers_[i][np.newaxis]\n                new_cluster_centers[i] = np.mean(pretendent, axis=0)\n            \n            if np.allclose(new_cluster_centers, self.cluster_centers_):\n                self.cluster_centers_ = new_cluster_centers\n                break\n                \n            self.cluster_centers_ = new_cluster_centers\n            \n        return self\n    \n    def predict(self, x):\n        D = self.distance(x, self.cluster_centers_)\n        return D.argmin(axis=1)\n55/47:\nx = np.concatenate([np.random.normal(size=(100, 2)), np.random.normal(size=(100, 2)) + 5])\nclust = KMeans(2).fit(x).predict(x)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(*x.T, c=clust)\nplt.show()\n55/48:\nks = list(range(2, 10))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n55/49:\nkmeans = KMeans(n_clusters=2).fit(pixels)\npixels_clustered = kmeans.cluster_centers_[kmeans.predict(pixels)]\n\n\nfig, ax = plt.subplots(ncols=2, figsize=(10, 6))\nax[0].imshow(image.astype(np.uint8))\nax[1].imshow(pixels_clustered.reshape(image.shape).astype(np.uint8))\n55/50:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('1.jpg')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 3)\n55/51:\nks = list(range(2, 10))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n55/52:\nkmeans = KMeans(n_clusters=2).fit(pixels)\npixels_clustered = kmeans.cluster_centers_[kmeans.predict(pixels)]\n\n\nfig, ax = plt.subplots(ncols=2, figsize=(10, 6))\nax[0].imshow(image.astype(np.uint8))\nax[1].imshow(pixels_clustered.reshape(image.shape).astype(np.uint8))\n55/53:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('Swords.jpg')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 3)\n55/54:\nks = list(range(2, 10))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n55/55:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('BorderTextures_2.jpg')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 3)\n55/56:\nks = list(range(2, 10))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n55/57:\nkmeans = KMeans(n_clusters=7).fit(pixels)\npixels_clustered = kmeans.cluster_centers_[kmeans.predict(pixels)]\n\n\nfig, ax = plt.subplots(ncols=2, figsize=(10, 6))\nax[0].imshow(image.astype(np.uint8))\nax[1].imshow(pixels_clustered.reshape(image.shape).astype(np.uint8))\n55/58:\n# pip install scikit-image\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import silhouette_score\n\nimage = imread('2.jpg')\n\n# Resize if necessary\nimage = resize(image, (64, 64), preserve_range=True)\n\npixels = image.reshape(-1, 3)\n55/59:\nks = list(range(2, 10))\nscores = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k).fit(pixels)\n    scores.append(silhouette_score(pixels, kmeans.predict(pixels), metric='euclidean'))\n    \nplt.plot(\n    ks,\n    scores\n)\n55/60:\nkmeans = KMeans(n_clusters=6).fit(pixels)\npixels_clustered = kmeans.cluster_centers_[kmeans.predict(pixels)]\n\n\nfig, ax = plt.subplots(ncols=2, figsize=(10, 6))\nax[0].imshow(image.astype(np.uint8))\nax[1].imshow(pixels_clustered.reshape(image.shape).astype(np.uint8))\n60/1:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tqdm.notebook import tqdm, trange\n60/2:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tqdm.notebook import tqdm, trange\n60/3:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tqdm.notebook import tqdm, trange\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D\nfrom tensorflow.keras import Model\n60/4:\nmnist = tf.keras.datasets.mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n# Add a channels dimension\nx_train = x_train[..., tf.newaxis].astype(\"float32\")\nx_test = x_test[..., tf.newaxis].astype(\"float32\")\n60/5:\nclass MyModel(Model):\n  def __init__(self):\n    super(MyModel, self).__init__()\n    self.conv1 = Conv2D(32, 3, activation='relu')\n    self.flatten = Flatten()\n    self.d1 = Dense(128, activation='relu')\n    self.d2 = Dense(10)\n\n  def call(self, x):\n    x = self.conv1(x)\n    x = self.flatten(x)\n    x = self.d1(x)\n    return self.d2(x)\n\n# Create an instance of the model\nmodel = MyModel()\n60/6:\nclass MyModel(Model):\n  def __init__(self):\n    super(MyModel, self).__init__()\n    self.conv1 = Conv2D(32, 3, activation='relu')\n    self.flatten = Flatten()\n    self.d1 = Dense(128, activation='relu')\n    self.d2 = Dense(10)\n\n  def call(self, x):\n    x = self.conv1(x)\n    x = self.flatten(x)\n    x = self.d1(x)\n    return self.d2(x)\n\n# Create an instance of the model\nmodel = MyModel()\n60/7:\nclass MyModel(Model):\n  def __init__(self):\n    super(MyModel, self).__init__()\n    self.conv1 = Conv2D(32, 3, activation='relu')\n    self.flatten = Flatten()\n    self.d1 = Dense(128, activation='relu')\n    self.d2 = Dense(10)\n\n  def call(self, x):\n    x = self.conv1(x)\n    x = self.flatten(x)\n    x = self.d1(x)\n    return self.d2(x)\n\n# Create an instance of the model\nmodel = MyModel()\n60/8:\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\noptimizer = tf.keras.optimizers.Adam()\n60/9:\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\noptimizer = tf.keras.optimizers.Adam()\n60/10:\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\noptimizer = tf.keras.optimizers.Adam()\n60/11:\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\ntest_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n60/12:\n@tf.function\ndef train_step(images, labels):\n  with tf.GradientTape() as tape:\n    # training=True is only needed if there are layers with different\n    # behavior during training versus inference (e.g. Dropout).\n    predictions = model(images, training=True)\n    loss = loss_object(labels, predictions)\n  gradients = tape.gradient(loss, model.trainable_variables)\n  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n  train_loss(loss)\n  train_accuracy(labels, predictions)\n60/13:\n@tf.function\ndef test_step(images, labels):\n  # training=False is only needed if there are layers with different\n  # behavior during training versus inference (e.g. Dropout).\n  predictions = model(images, training=False)\n  t_loss = loss_object(labels, predictions)\n\n  test_loss(t_loss)\n  test_accuracy(labels, predictions)\n60/14:\nEPOCHS = 5\n\nfor epoch in range(EPOCHS):\n  # Reset the metrics at the start of the next epoch\n  train_loss.reset_states()\n  train_accuracy.reset_states()\n  test_loss.reset_states()\n  test_accuracy.reset_states()\n\n  for images, labels in train_ds:\n    train_step(images, labels)\n\n  for test_images, test_labels in test_ds:\n    test_step(test_images, test_labels)\n\n  print(\n    f'Epoch {epoch + 1}, '\n    f'Loss: {train_loss.result()}, '\n    f'Accuracy: {train_accuracy.result() * 100}, '\n    f'Test Loss: {test_loss.result()}, '\n    f'Test Accuracy: {test_accuracy.result() * 100}'\n  )\n60/15:\n",
      "mnist = tf.keras.datasets.mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n# Add a channels dimension\nx_train = x_train[..., tf.newaxis].astype(\"float32\")\nx_test = x_test[..., tf.newaxis].astype(\"float32\")\n60/16:\ntrain_ds = tf.data.Dataset.from_tensor_slices(\n    (x_train, y_train)).shuffle(10000).batch(32)\n\ntest_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n60/17:\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\noptimizer = tf.keras.optimizers.Adam()\n60/18:\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\ntest_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n60/19:\n@tf.function\ndef train_step(images, labels):\n  with tf.GradientTape() as tape:\n    # training=True is only needed if there are layers with different\n    # behavior during training versus inference (e.g. Dropout).\n    predictions = model(images, training=True)\n    loss = loss_object(labels, predictions)\n  gradients = tape.gradient(loss, model.trainable_variables)\n  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n  train_loss(loss)\n  train_accuracy(labels, predictions)\n60/20:\nEPOCHS = 5\n\nfor epoch in range(EPOCHS):\n  # Reset the metrics at the start of the next epoch\n  train_loss.reset_states()\n  train_accuracy.reset_states()\n  test_loss.reset_states()\n  test_accuracy.reset_states()\n\n  for images, labels in train_ds:\n    train_step(images, labels)\n\n  for test_images, test_labels in test_ds:\n    test_step(test_images, test_labels)\n\n  print(\n    f'Epoch {epoch + 1}, '\n    f'Loss: {train_loss.result()}, '\n    f'Accuracy: {train_accuracy.result() * 100}, '\n    f'Test Loss: {test_loss.result()}, '\n    f'Test Accuracy: {test_accuracy.result() * 100}'\n  )\n61/1:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n61/2:\nv = sqlContext.createDataFrame([\n  (\"a\", \"Alice\", 34),\n  (\"b\", \"Bob\", 36),\n  (\"c\", \"Charlie\", 30),\n  (\"d\", \"David\", 29),\n  (\"e\", \"Esther\", 32),\n  (\"f\", \"Fanny\", 36),\n  (\"g\", \"Gabby\", 60)\n], [\"id\", \"name\", \"age\"])\n# Edge DataFrame\ne = sqlContext.createDataFrame([\n  (\"a\", \"b\", \"friend\"),\n  (\"b\", \"c\", \"follow\"),\n  (\"c\", \"b\", \"follow\"),\n  (\"f\", \"c\", \"follow\"),\n  (\"e\", \"f\", \"follow\"),\n  (\"e\", \"d\", \"friend\"),\n  (\"d\", \"a\", \"friend\"),\n  (\"a\", \"e\", \"friend\")\n], [\"src\", \"dst\", \"relationship\"])\n# Create a GraphFrame\ng = GraphFrame(v, e)\n61/3:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n61/4:\nv = sqlContext.createDataFrame([\n  (\"a\", \"Alice\", 34),\n  (\"b\", \"Bob\", 36),\n  (\"c\", \"Charlie\", 30),\n  (\"d\", \"David\", 29),\n  (\"e\", \"Esther\", 32),\n  (\"f\", \"Fanny\", 36),\n  (\"g\", \"Gabby\", 60)\n], [\"id\", \"name\", \"age\"])\n# Edge DataFrame\ne = sqlContext.createDataFrame([\n  (\"a\", \"b\", \"friend\"),\n  (\"b\", \"c\", \"follow\"),\n  (\"c\", \"b\", \"follow\"),\n  (\"f\", \"c\", \"follow\"),\n  (\"e\", \"f\", \"follow\"),\n  (\"e\", \"d\", \"friend\"),\n  (\"d\", \"a\", \"friend\"),\n  (\"a\", \"e\", \"friend\")\n], [\"src\", \"dst\", \"relationship\"])\n# Create a GraphFrame\ng = GraphFrame(v, e)\n61/5:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n61/6:\nv = sqlContext.createDataFrame([\n  (\"a\", \"Alice\", 34),\n  (\"b\", \"Bob\", 36),\n  (\"c\", \"Charlie\", 30),\n  (\"d\", \"David\", 29),\n  (\"e\", \"Esther\", 32),\n  (\"f\", \"Fanny\", 36),\n  (\"g\", \"Gabby\", 60)\n], [\"id\", \"name\", \"age\"])\n# Edge DataFrame\ne = sqlContext.createDataFrame([\n  (\"a\", \"b\", \"friend\"),\n  (\"b\", \"c\", \"follow\"),\n  (\"c\", \"b\", \"follow\"),\n  (\"f\", \"c\", \"follow\"),\n  (\"e\", \"f\", \"follow\"),\n  (\"e\", \"d\", \"friend\"),\n  (\"d\", \"a\", \"friend\"),\n  (\"a\", \"e\", \"friend\")\n], [\"src\", \"dst\", \"relationship\"])\n# Create a GraphFrame\ng = GraphFrame(v, e)\n61/7:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\nfrom graphframes import *\n61/8:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\nfrom graphframes import *\n61/9:\nv = sqlContext.createDataFrame([\n  (\"a\", \"Alice\", 34),\n  (\"b\", \"Bob\", 36),\n  (\"c\", \"Charlie\", 30),\n  (\"d\", \"David\", 29),\n  (\"e\", \"Esther\", 32),\n  (\"f\", \"Fanny\", 36),\n  (\"g\", \"Gabby\", 60)\n], [\"id\", \"name\", \"age\"])\n# Edge DataFrame\ne = sqlContext.createDataFrame([\n  (\"a\", \"b\", \"friend\"),\n  (\"b\", \"c\", \"follow\"),\n  (\"c\", \"b\", \"follow\"),\n  (\"f\", \"c\", \"follow\"),\n  (\"e\", \"f\", \"follow\"),\n  (\"e\", \"d\", \"friend\"),\n  (\"d\", \"a\", \"friend\"),\n  (\"a\", \"e\", \"friend\")\n], [\"src\", \"dst\", \"relationship\"])\n# Create a GraphFrame\ng = GraphFrame(v, e)\n62/1:\nv = sqlContext.createDataFrame([\n  (\"a\", \"Alice\", 34),\n  (\"b\", \"Bob\", 36),\n  (\"c\", \"Charlie\", 30),\n  (\"d\", \"David\", 29),\n  (\"e\", \"Esther\", 32),\n  (\"f\", \"Fanny\", 36),\n  (\"g\", \"Gabby\", 60)\n], [\"id\", \"name\", \"age\"])\n# Edge DataFrame\ne = sqlContext.createDataFrame([\n  (\"a\", \"b\", \"friend\"),\n  (\"b\", \"c\", \"follow\"),\n  (\"c\", \"b\", \"follow\"),\n  (\"f\", \"c\", \"follow\"),\n  (\"e\", \"f\", \"follow\"),\n  (\"e\", \"d\", \"friend\"),\n  (\"d\", \"a\", \"friend\"),\n  (\"a\", \"e\", \"friend\")\n], [\"src\", \"dst\", \"relationship\"])\n# Create a GraphFrame\ng = GraphFrame(v, e)\n62/2:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\nfrom graphframes import *\n63/1:\nfrom pyspark.python.pyspark.shell import sqlContext\nfrom pyspark.shell import spark\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql import *\nfrom pyspark.sql.functions import *\nfrom graphframes import *\nfrom pyspark.sql import functions as func\nfrom pyspark.sql.functions import explode\nimport math\n63/2:\n#sc = SparkContext('local')\n#sc = SparkSession.builder.appName('Lab5').master(\"local\").getOrCreate()\n#sqlContext = SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)\n#spark = SparkSession(sqlContext)\n63/3:\n#Read by from files by schemas\nairports = spark.read.format(\"csv\").option(\"header\", False).schema(airportsSchema).load(\"airports-extended.dat\")\nairlines = spark.read.format(\"csv\").option(\"header\", False).schema(airlinesSchema).load(\"airlines.dat\")\nroutes = spark.read.format(\"csv\").option(\"header\", False).schema(routesSchema).load(\"routes.dat\")\n63/4:\n# Layout schemas\nairportsSchema = StructType([\n    StructField(\"airport_id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"city\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"iata\", StringType(), True),\n    StructField(\"icao\", StringType(), True),\n    StructField(\"lat\", DoubleType(), True),\n    StructField(\"long\", DoubleType(), True),\n    StructField(\"alt\", IntegerType(), True),\n    StructField(\"tz\", DoubleType(), True),\n    StructField(\"dst\", StringType(), True),\n    StructField(\"tz_db\", StringType(), True),\n    StructField(\"type\", StringType(), True),\n    StructField(\"source\", StringType(), True)\n  ])\n\nairlinesSchema = StructType([\n    StructField(\"airline_id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"alias\", StringType(), True),\n    StructField(\"iata\", StringType(), True),\n    StructField(\"icao\", StringType(), True),\n    StructField(\"callsign\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"active\", StringType(), True)\n])\n\nroutesSchema = StructType([\n    StructField(\"airline\", StringType(), True),\n    StructField(\"airline_id\", IntegerType(), True),\n    StructField(\"source\", StringType(), True),\n    StructField(\"source_id\", IntegerType(), True),\n    StructField(\"dest\", StringType(), True),\n    StructField(\"dest_id\", IntegerType(), True),\n    StructField(\"sharecode\", StringType(), True),\n    StructField(\"stops\", IntegerType(), True),\n    StructField(\"equip\", StringType(), True)\n])\n63/5:\n#Read by from files by schemas\nairports = spark.read.format(\"csv\").option(\"header\", False).schema(airportsSchema).load(\"airports-extended.dat\")\nairlines = spark.read.format(\"csv\").option(\"header\", False).schema(airlinesSchema).load(\"airlines.dat\")\nroutes = spark.read.format(\"csv\").option(\"header\", False).schema(routesSchema).load(\"routes.dat\")\n63/6:\n# Task 1) Get companies with biggest and smallest sum of routes distances\ndef calcDist(srcLat, srcLong, destLat, destLong):\n    latDistance = math.radians(srcLat - destLat)\n    lngDistance = math.radians(srcLong - destLong)\n    sinLat = math.sin(latDistance / 2)\n    sinLng = math.sin(lngDistance / 2)\n    a = sinLat * sinLat + (math.cos(math.radians(srcLat)) * math.cos(math.radians(destLat)) * sinLng * sinLng)\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n    return int(c * 6371)\n\nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n\nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\"), col(\"airport_id\").alias(\"dest_id\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], (calcDist(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])))\n\n\nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"data\"]))\nairportGraph.cache()\n\nroutesDistancesSum = airportGraph \\\n    .edges.groupBy(col(\"data._2\").alias(\"airline_id\")) \\\n    .agg(func.sum(\"data._1\").alias(\"dist_sum\"))\n\n# Sorted to see biggest distances\nroutesDistancesSum.sort(col(\"dist_sum\").desc()).show()\n63/7:\nsc = SparkContext('local')\nsc = SparkSession.builder.appName('Lab5').master(\"local\").getOrCreate()\nsqlContext = SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)\nspark = SparkSession(sqlContext)\n63/8:\n# sc = SparkContext('local')\n# sc = SparkSession.builder.appName('Lab5').master(\"local\").getOrCreate()\n# sqlContext = SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)\n# spark = SparkSession(sqlContext)\n\nsc = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n63/9:\n# sc = SparkContext('local')\n# sc = SparkSession.builder.appName('Lab5').master(\"local\").getOrCreate()\n# sqlContext = SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)\n# spark = SparkSession(sqlContext)\n\nsc = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"local-1608057188473\") \\\n    .getOrCreate()\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n63/10:\n# Layout schemas\nairportsSchema = StructType([\n    StructField(\"airport_id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"city\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"iata\", StringType(), True),\n    StructField(\"icao\", StringType(), True),\n    StructField(\"lat\", DoubleType(), True),\n    StructField(\"long\", DoubleType(), True),\n    StructField(\"alt\", IntegerType(), True),\n    StructField(\"tz\", DoubleType(), True),\n    StructField(\"dst\", StringType(), True),\n    StructField(\"tz_db\", StringType(), True),\n    StructField(\"type\", StringType(), True),\n    StructField(\"source\", StringType(), True)\n  ])\n\nairlinesSchema = StructType([\n    StructField(\"airline_id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"alias\", StringType(), True),\n    StructField(\"iata\", StringType(), True),\n    StructField(\"icao\", StringType(), True),\n    StructField(\"callsign\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"active\", StringType(), True)\n])\n\nroutesSchema = StructType([\n    StructField(\"airline\", StringType(), True),\n    StructField(\"airline_id\", IntegerType(), True),\n    StructField(\"source\", StringType(), True),\n    StructField(\"source_id\", IntegerType(), True),\n    StructField(\"dest\", StringType(), True),\n    StructField(\"dest_id\", IntegerType(), True),\n    StructField(\"sharecode\", StringType(), True),\n    StructField(\"stops\", IntegerType(), True),\n    StructField(\"equip\", StringType(), True)\n])\n63/11:\n#Read by from files by schemas\nairports = spark.read.format(\"csv\").option(\"header\", False).schema(airportsSchema).load(\"airports-extended.dat\")\nairlines = spark.read.format(\"csv\").option(\"header\", False).schema(airlinesSchema).load(\"airlines.dat\")\nroutes = spark.read.format(\"csv\").option(\"header\", False).schema(routesSchema).load(\"routes.dat\")\n63/12:\n# Task 1) Get companies with biggest and smallest sum of routes distances\ndef calcDist(srcLat, srcLong, destLat, destLong):\n    latDistance = math.radians(srcLat - destLat)\n    lngDistance = math.radians(srcLong - destLong)\n    sinLat = math.sin(latDistance / 2)\n    sinLng = math.sin(lngDistance / 2)\n    a = sinLat * sinLat + (math.cos(math.radians(srcLat)) * math.cos(math.radians(destLat)) * sinLng * sinLng)\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n    return int(c * 6371)\n\nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n\nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\"), col(\"airport_id\").alias(\"dest_id\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], (calcDist(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])))\n\n\nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"data\"]))\nairportGraph.cache()\n\nroutesDistancesSum = airportGraph \\\n    .edges.groupBy(col(\"data._2\").alias(\"airline_id\")) \\\n    .agg(func.sum(\"data._1\").alias(\"dist_sum\"))\n\n# Sorted to see biggest distances\nroutesDistancesSum.sort(col(\"dist_sum\").desc()).show()\n63/13:\n# sc = SparkContext('local')\n# sc = SparkSession.builder.appName('Lab5').master(\"local\").getOrCreate()\n# sqlContext = SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)\n# spark = SparkSession(sqlContext)\nimport findspark\nfindspark.init()\n\nsc = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"local-1608057188473\") \\\n    .getOrCreate()\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n63/14:\n# sc = SparkContext('local')\n# sc = SparkSession.builder.appName('Lab5').master(\"local\").getOrCreate()\n# sqlContext = SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)\n# spark = SparkSession(sqlContext)\nimport findspark\nfindspark.init()\n\nsc = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"local-1608057188473\") \\\n    .getOrCreate()\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n65/1:\nfrom pyspark.python.pyspark.shell import sqlContext\nfrom pyspark.shell import spark\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql import *\nfrom pyspark.sql.functions import *\nfrom graphframes import *\nfrom pyspark.sql import functions as func\nfrom pyspark.sql.functions import explode\nimport math\n65/2:\n# sc = SparkContext('local')\n# sc = SparkSession.builder.appName('Lab5').master(\"local\").getOrCreate()\n# sqlContext = SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)\n# spark = SparkSession(sqlContext)\nimport findspark\nfindspark.init()\n\nsc = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"local-1608057188473\") \\\n    .getOrCreate()\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n66/1:\n#from pyspark.python.pyspark.shell import sqlContext\n#from pyspark.shell import spark\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql import *\nfrom pyspark.sql.functions import *\nfrom graphframes import *\nfrom pyspark.sql import functions as func\nfrom pyspark.sql.functions import explode\nimport math\n66/2:\n# sc = SparkContext('local')\n# sc = SparkSession.builder.appName('Lab5').master(\"local\").getOrCreate()\n# sqlContext = SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)\n# spark = SparkSession(sqlContext)\nimport findspark\nfindspark.init()\n\nsc = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"local-1608057188473\") \\\n    .getOrCreate()\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n68/1:\n#from pyspark.python.pyspark.shell import sqlContext\n#from pyspark.shell import spark\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql import *\nfrom pyspark.sql.functions import *\nfrom graphframes import *\nfrom pyspark.sql import functions as func\nfrom pyspark.sql.functions import explode\nimport math\n68/2:\n# sc = SparkContext('local')\n# sc = SparkSession.builder.appName('Lab5').master(\"local\").getOrCreate()\n# sqlContext = SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)\n# spark = SparkSession(sqlContext)\nimport findspark\nfindspark.init()\n\nsc = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"local-1608057188473\") \\\n    .getOrCreate()\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n68/3:\n# sc = SparkContext('local')\n# sc = SparkSession.builder.appName('Lab5').master(\"local\").getOrCreate()\n# sqlContext = SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)\n# spark = SparkSession(sqlContext)\nimport findspark\nfindspark.init()\n\nsc = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"local-1608057188473\") \\\n    .getOrCreate()\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n68/4:\n# Layout schemas\nairportsSchema = StructType([\n    StructField(\"airport_id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"city\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"iata\", StringType(), True),\n    StructField(\"icao\", StringType(), True),\n    StructField(\"lat\", DoubleType(), True),\n    StructField(\"long\", DoubleType(), True),\n    StructField(\"alt\", IntegerType(), True),\n    StructField(\"tz\", DoubleType(), True),\n    StructField(\"dst\", StringType(), True),\n    StructField(\"tz_db\", StringType(), True),\n    StructField(\"type\", StringType(), True),\n    StructField(\"source\", StringType(), True)\n  ])\n\nairlinesSchema = StructType([\n    StructField(\"airline_id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"alias\", StringType(), True),\n    StructField(\"iata\", StringType(), True),\n    StructField(\"icao\", StringType(), True),\n    StructField(\"callsign\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"active\", StringType(), True)\n])\n\nroutesSchema = StructType([\n    StructField(\"airline\", StringType(), True),\n    StructField(\"airline_id\", IntegerType(), True),\n    StructField(\"source\", StringType(), True),\n    StructField(\"source_id\", IntegerType(), True),\n    StructField(\"dest\", StringType(), True),\n    StructField(\"dest_id\", IntegerType(), True),\n    StructField(\"sharecode\", StringType(), True),\n    StructField(\"stops\", IntegerType(), True),\n    StructField(\"equip\", StringType(), True)\n])\n68/5:\n#Read by from files by schemas\nairports = spark.read.format(\"csv\").option(\"header\", False).schema(airportsSchema).load(\"airports-extended.dat\")\nairlines = spark.read.format(\"csv\").option(\"header\", False).schema(airlinesSchema).load(\"airlines.dat\")\nroutes = spark.read.format(\"csv\").option(\"header\", False).schema(routesSchema).load(\"routes.dat\")\n68/6:\n# sc = SparkContext('local')\n# sc = SparkSession.builder.appName('Lab5').master(\"local\").getOrCreate()\n# sqlContext = SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)\n# spark = SparkSession(sqlContext)\nimport findspark\nfindspark.init()\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"local-1608057188473\") \\\n    .getOrCreate()\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n68/7:\n# Layout schemas\nairportsSchema = StructType([\n    StructField(\"airport_id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"city\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"iata\", StringType(), True),\n    StructField(\"icao\", StringType(), True),\n    StructField(\"lat\", DoubleType(), True),\n    StructField(\"long\", DoubleType(), True),\n    StructField(\"alt\", IntegerType(), True),\n    StructField(\"tz\", DoubleType(), True),\n    StructField(\"dst\", StringType(), True),\n    StructField(\"tz_db\", StringType(), True),\n    StructField(\"type\", StringType(), True),\n    StructField(\"source\", StringType(), True)\n  ])\n\nairlinesSchema = StructType([\n    StructField(\"airline_id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"alias\", StringType(), True),\n    StructField(\"iata\", StringType(), True),\n    StructField(\"icao\", StringType(), True),\n    StructField(\"callsign\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"active\", StringType(), True)\n])\n\nroutesSchema = StructType([\n    StructField(\"airline\", StringType(), True),\n    StructField(\"airline_id\", IntegerType(), True),\n    StructField(\"source\", StringType(), True),\n    StructField(\"source_id\", IntegerType(), True),\n    StructField(\"dest\", StringType(), True),\n    StructField(\"dest_id\", IntegerType(), True),\n    StructField(\"sharecode\", StringType(), True),\n    StructField(\"stops\", IntegerType(), True),\n    StructField(\"equip\", StringType(), True)\n])\n68/8:\n#Read by from files by schemas\nairports = spark.read.format(\"csv\").option(\"header\", False).schema(airportsSchema).load(\"airports-extended.dat\")\nairlines = spark.read.format(\"csv\").option(\"header\", False).schema(airlinesSchema).load(\"airlines.dat\")\nroutes = spark.read.format(\"csv\").option(\"header\", False).schema(routesSchema).load(\"routes.dat\")\n68/9:\n# Task 1) Get companies with biggest and smallest sum of routes distances\ndef calcDist(srcLat, srcLong, destLat, destLong):\n    latDistance = math.radians(srcLat - destLat)\n    lngDistance = math.radians(srcLong - destLong)\n    sinLat = math.sin(latDistance / 2)\n    sinLng = math.sin(lngDistance / 2)\n    a = sinLat * sinLat + (math.cos(math.radians(srcLat)) * math.cos(math.radians(destLat)) * sinLng * sinLng)\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n    return int(c * 6371)\n\nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n\nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\"), col(\"airport_id\").alias(\"dest_id\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], (calcDist(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])))\n\n\nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"data\"]))\nairportGraph.cache()\n\nroutesDistancesSum = airportGraph \\\n    .edges.groupBy(col(\"data._2\").alias(\"airline_id\")) \\\n    .agg(func.sum(\"data._1\").alias(\"dist_sum\"))\n\n# Sorted to see biggest distances\nroutesDistancesSum.sort(col(\"dist_sum\").desc()).show()\n69/1:\n#from pyspark.python.pyspark.shell import sqlContext\n#from pyspark.shell import spark\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql import *\nfrom pyspark.sql.functions import *\nfrom graphframes import *\nfrom pyspark.sql import functions as func\nfrom pyspark.sql.functions import explode\nimport math\n69/2:\n# sc = SparkContext('local')\n# sc = SparkSession.builder.appName('Lab5').master(\"local\").getOrCreate()\n# sqlContext = SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)\n# spark = SparkSession(sqlContext)\n\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"local-1608057188473\") \\\n    .getOrCreate()\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n69/3:\n# Layout schemas\nairportsSchema = StructType([\n    StructField(\"airport_id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"city\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"iata\", StringType(), True),\n    StructField(\"icao\", StringType(), True),\n    StructField(\"lat\", DoubleType(), True),\n    StructField(\"long\", DoubleType(), True),\n    StructField(\"alt\", IntegerType(), True),\n    StructField(\"tz\", DoubleType(), True),\n    StructField(\"dst\", StringType(), True),\n    StructField(\"tz_db\", StringType(), True),\n    StructField(\"type\", StringType(), True),\n    StructField(\"source\", StringType(), True)\n  ])\n\nairlinesSchema = StructType([\n    StructField(\"airline_id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"alias\", StringType(), True),\n    StructField(\"iata\", StringType(), True),\n    StructField(\"icao\", StringType(), True),\n    StructField(\"callsign\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"active\", StringType(), True)\n])\n\nroutesSchema = StructType([\n    StructField(\"airline\", StringType(), True),\n    StructField(\"airline_id\", IntegerType(), True),\n    StructField(\"source\", StringType(), True),\n    StructField(\"source_id\", IntegerType(), True),\n    StructField(\"dest\", StringType(), True),\n    StructField(\"dest_id\", IntegerType(), True),\n    StructField(\"sharecode\", StringType(), True),\n    StructField(\"stops\", IntegerType(), True),\n    StructField(\"equip\", StringType(), True)\n])\n69/4:\n#Read by from files by schemas\nairports = spark.read.format(\"csv\").option(\"header\", False).schema(airportsSchema).load(\"airports-extended.dat\")\nairlines = spark.read.format(\"csv\").option(\"header\", False).schema(airlinesSchema).load(\"airlines.dat\")\nroutes = spark.read.format(\"csv\").option(\"header\", False).schema(routesSchema).load(\"routes.dat\")\n69/5:\n# Task 1) Get companies with biggest and smallest sum of routes distances\ndef calcDist(srcLat, srcLong, destLat, destLong):\n    latDistance = math.radians(srcLat - destLat)\n    lngDistance = math.radians(srcLong - destLong)\n    sinLat = math.sin(latDistance / 2)\n    sinLng = math.sin(lngDistance / 2)\n    a = sinLat * sinLat + (math.cos(math.radians(srcLat)) * math.cos(math.radians(destLat)) * sinLng * sinLng)\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n    return int(c * 6371)\n\nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n\nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\"), col(\"airport_id\").alias(\"dest_id\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], (calcDist(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])))\n\n\nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"data\"]))\nairportGraph.cache()\n\nroutesDistancesSum = airportGraph \\\n    .edges.groupBy(col(\"data._2\").alias(\"airline_id\")) \\\n    .agg(func.sum(\"data._1\").alias(\"dist_sum\"))\n\n# Sorted to see biggest distances\nroutesDistancesSum.sort(col(\"dist_sum\").desc()).show()\n70/1:\n#from pyspark.python.pyspark.shell import sqlContext\n#from pyspark.shell import spark\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql import *\nfrom pyspark.sql.functions import *\nfrom graphframes import *\nfrom pyspark.sql import functions as func\nfrom pyspark.sql.functions import explode\nimport math\n70/2:\n# sc = SparkContext('local')\n# sc = SparkSession.builder.appName('Lab5').master(\"local\").getOrCreate()\n# sqlContext = SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)\n# spark = SparkSession(sqlContext)\n\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"local-1608057188473\") \\\n    .getOrCreate()\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n70/3:\n# Layout schemas\nairportsSchema = StructType([\n    StructField(\"airport_id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"city\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"iata\", StringType(), True),\n    StructField(\"icao\", StringType(), True),\n    StructField(\"lat\", DoubleType(), True),\n    StructField(\"long\", DoubleType(), True),\n    StructField(\"alt\", IntegerType(), True),\n    StructField(\"tz\", DoubleType(), True),\n    StructField(\"dst\", StringType(), True),\n    StructField(\"tz_db\", StringType(), True),\n    StructField(\"type\", StringType(), True),\n    StructField(\"source\", StringType(), True)\n  ])\n\nairlinesSchema = StructType([\n    StructField(\"airline_id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"alias\", StringType(), True),\n    StructField(\"iata\", StringType(), True),\n    StructField(\"icao\", StringType(), True),\n    StructField(\"callsign\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"active\", StringType(), True)\n])\n\nroutesSchema = StructType([\n    StructField(\"airline\", StringType(), True),\n    StructField(\"airline_id\", IntegerType(), True),\n    StructField(\"source\", StringType(), True),\n    StructField(\"source_id\", IntegerType(), True),\n    StructField(\"dest\", StringType(), True),\n    StructField(\"dest_id\", IntegerType(), True),\n    StructField(\"sharecode\", StringType(), True),\n    StructField(\"stops\", IntegerType(), True),\n    StructField(\"equip\", StringType(), True)\n])\n70/4:\n#Read by from files by schemas\nairports = spark.read.format(\"csv\").option(\"header\", False).schema(airportsSchema).load(\"airports-extended.dat\")\nairlines = spark.read.format(\"csv\").option(\"header\", False).schema(airlinesSchema).load(\"airlines.dat\")\nroutes = spark.read.format(\"csv\").option(\"header\", False).schema(routesSchema).load(\"routes.dat\")\n70/5:\n# Task 1) Get companies with biggest and smallest sum of routes distances\ndef calcDist(srcLat, srcLong, destLat, destLong):\n    latDistance = math.radians(srcLat - destLat)\n    lngDistance = math.radians(srcLong - destLong)\n    sinLat = math.sin(latDistance / 2)\n    sinLng = math.sin(lngDistance / 2)\n    a = sinLat * sinLat + (math.cos(math.radians(srcLat)) * math.cos(math.radians(destLat)) * sinLng * sinLng)\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n    return int(c * 6371)\n\nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n\nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\"), col(\"airport_id\").alias(\"dest_id\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], (calcDist(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])))\n\n\nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"data\"]))\nairportGraph.cache()\n\nroutesDistancesSum = airportGraph \\\n    .edges.groupBy(col(\"data._2\").alias(\"airline_id\")) \\\n    .agg(func.sum(\"data._1\").alias(\"dist_sum\"))\n\n# Sorted to see biggest distances\nroutesDistancesSum.sort(col(\"dist_sum\").desc()).show()\n71/1:\n#from pyspark.python.pyspark.shell import sqlContext\n#from pyspark.shell import spark\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql import *\nfrom pyspark.sql.functions import *\nfrom graphframes import *\nfrom pyspark.sql import functions as func\nfrom pyspark.sql.functions import explode\nimport math\n71/2:\n# sc = SparkContext('local')\n# sc = SparkSession.builder.appName('Lab5').master(\"local\").getOrCreate()\n# sqlContext = SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)\n# spark = SparkSession(sqlContext)\n\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"local-1608057188473\") \\\n    .getOrCreate()\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n71/3:\n# Layout schemas\nairportsSchema = StructType([\n    StructField(\"airport_id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"city\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"iata\", StringType(), True),\n    StructField(\"icao\", StringType(), True),\n    StructField(\"lat\", DoubleType(), True),\n    StructField(\"long\", DoubleType(), True),\n    StructField(\"alt\", IntegerType(), True),\n    StructField(\"tz\", DoubleType(), True),\n    StructField(\"dst\", StringType(), True),\n    StructField(\"tz_db\", StringType(), True),\n    StructField(\"type\", StringType(), True),\n    StructField(\"source\", StringType(), True)\n  ])\n\nairlinesSchema = StructType([\n    StructField(\"airline_id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"alias\", StringType(), True),\n    StructField(\"iata\", StringType(), True),\n    StructField(\"icao\", StringType(), True),\n    StructField(\"callsign\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"active\", StringType(), True)\n])\n\nroutesSchema = StructType([\n    StructField(\"airline\", StringType(), True),\n    StructField(\"airline_id\", IntegerType(), True),\n    StructField(\"source\", StringType(), True),\n    StructField(\"source_id\", IntegerType(), True),\n    StructField(\"dest\", StringType(), True),\n    StructField(\"dest_id\", IntegerType(), True),\n    StructField(\"sharecode\", StringType(), True),\n    StructField(\"stops\", IntegerType(), True),\n    StructField(\"equip\", StringType(), True)\n])\n71/4:\n#Read by from files by schemas\nairports = spark.read.format(\"csv\").option(\"header\", False).schema(airportsSchema).load(\"airports-extended.dat\")\nairlines = spark.read.format(\"csv\").option(\"header\", False).schema(airlinesSchema).load(\"airlines.dat\")\nroutes = spark.read.format(\"csv\").option(\"header\", False).schema(routesSchema).load(\"routes.dat\")\n71/5:\n# Task 1) Get companies with biggest and smallest sum of routes distances\ndef calcDist(srcLat, srcLong, destLat, destLong):\n    latDistance = math.radians(srcLat - destLat)\n    lngDistance = math.radians(srcLong - destLong)\n    sinLat = math.sin(latDistance / 2)\n    sinLng = math.sin(lngDistance / 2)\n    a = sinLat * sinLat + (math.cos(math.radians(srcLat)) * math.cos(math.radians(destLat)) * sinLng * sinLng)\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n    return int(c * 6371)\n\nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n\nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\"), col(\"airport_id\").alias(\"dest_id\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], (calcDist(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])))\n\n\nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"data\"]))\nairportGraph.cache()\n\nroutesDistancesSum = airportGraph \\\n    .edges.groupBy(col(\"data._2\").alias(\"airline_id\")) \\\n    .agg(func.sum(\"data._1\").alias(\"dist_sum\"))\n\n# Sorted to see biggest distances\nroutesDistancesSum.sort(col(\"dist_sum\").desc()).show()\n71/6:\n# Sorted to see smallest distances\nroutesDistancesSum.sort(col(\"dist_sum\").asc()).show()\n71/7:\n# Task 2) Show all routes between Ukraine and Italy\nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n\nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\"), col(\"stops\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .filter(\"country = 'Ukraine' OR country = 'Italy'\") \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"airline_id\"),col(\"stops\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")).filter(\"country = 'Ukraine' OR country = 'Italy'\") \\\n    .select(col(\"source_id\"), col(\"airport_id\").alias(\"dest_id\"), col(\"airline_id\"), col(\"stops\")) \\\n    .filter(\"stops < 3\") \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], 1))\n\nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\"), col(\"stops\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .filter(\"country = 'Ukraine' OR country = 'Italy'\") \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"airline_id\"),col(\"stops\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")).filter(\"country = 'Ukraine' OR country = 'Italy'\") \\\n    .select(col(\"source_id\"), col(\"airport_id\").alias(\"dest_id\"), col(\"airline_id\"), col(\"stops\")) \\\n    .filter(\"stops < 3\") \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], row[\"airline_id\"]))\n\nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"data\"]))\nairportGraph.cache()\nairportGraph.edges.show()\n71/8:\n# Task 3) Show all airports with biggest and smallest routes count\nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n\nairportRoutesEdges = routes \\\n    .select(col(\"source_id\"), col(\"dest_id\")) \\\n    .rdd.map(lambda row:  (row[\"source_id\"], row[\"dest_id\"], 1))\n\nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"data\"]))\nairportGraph.cache()\n\ngroup = airportGraph.edges.groupBy(col(\"src\"))\nroutesCount = group.agg(func.sum(\"data\").alias(\"route_count\"))\n\n# Sorted to see most routes\nroutesCount.sort(col(\"route_count\").desc()).show()\n71/9:\n# Sorted to see least routes\nroutesCount.sort(col(\"route_count\").asc()).show()\n71/10:\n# Task 4) Find the shortest and longest route between the two designated airports.\nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n                                     \nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\").cast(\"long\"), col(\"airport_id\").alias(\"dest_id\").cast(\"long\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row:  (row[\"source_id\"], row[\"dest_id\"], (calcDist(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])[1]))\n    \nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"dist\"]))\nairportGraph.cache()\n\nshort_path_small = airportGraph.shortestPaths(landmarks=[221])\nshort_path_small.select(\"id\", explode(\"distances\")).show()\n71/11:\nfilteredPaths = airportGraph.bfs(fromExpr = 'id = 2900', toExpr = 'id = 221', maxPathLength = 3) \nfilteredPaths.show()\n71/12:\n# Task 4) Find the shortest and longest route between the two designated airports.\nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n                                     \nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\").cast(\"long\"), col(\"airport_id\").alias(\"dest_id\").cast(\"long\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row:  (row[\"source_id\"], row[\"dest_id\"], (calcDist(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])[1]))\n    \nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"dist\"]))\nairportGraph.cache()\n\nshort_path_small = airportGraph.shortestPaths(landmarks=[221])\nshort_path_small.select(\"id\", explode(\"distances\")).show()\n71/13:\nfilteredPaths = airportGraph.bfs(fromExpr = 'id = 2900', toExpr = 'id = 221', maxPathLength = 3) \nfilteredPaths.show()\n71/14:\nfrom pyspark import SparkConf\nfrom pyspark.context import SparkContext\nsc = SparkContext.getOrCreate(SparkConf())\n71/15:\n# Task 6) Find airports that have no connection \nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n\nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\").cast(\"long\"), col(\"airport_id\").alias(\"dest_id\").cast(\"long\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], (getDistance(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])))\n\nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"data\"]))\nairportGraph.persist()\n\nsc.setCheckpointDir(\"/tmp/\")\nresult = airportGraph.connectedComponents()\n71/16:\ninfo = result \\\n    .groupBy('component') \\\n    .agg({'id': 'count'}) \\\n    .withColumnRenamed('count(id)', 'component_size')\n\ninfo.filter('component_size == 1').join(\n  result, info.component == result.component, 'left'\n)\n71/17:\n# Task 6) Find airports that have no connection \ndef getDistance(srcLat, srcLong, destLat, destLong):\n    latDistance = math.radians(srcLat - destLat)\n    lngDistance = math.radians(srcLong - destLong)\n    sinLat = math.sin(latDistance / 2)\n    sinLng = math.sin(lngDistance / 2)\n    a = sinLat * sinLat + (math.cos(math.radians(srcLat)) * math.cos(math.radians(destLat)) * sinLng * sinLng)\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n    return int(c * 6371)\n\nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n\nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\").cast(\"long\"), col(\"airport_id\").alias(\"dest_id\").cast(\"long\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], (getDistance(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])))\n\nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"data\"]))\nairportGraph.persist()\n\nsc.setCheckpointDir(\"/tmp/\")\nresult = airportGraph.connectedComponents()\n71/18:\n# Task 6) Find airports that have no connection \n\nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n\nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\").cast(\"long\"), col(\"airport_id\").alias(\"dest_id\").cast(\"long\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], (calcDist(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])))\n\nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"data\"]))\nairportGraph.persist()\n\nsc.setCheckpointDir(\"./tmp/\")\nresult = airportGraph.connectedComponents()\n71/19:\nsc.setCheckpointDir(\"./\")\nresult = airportGraph.connectedComponents()\n73/1:\n#from pyspark.python.pyspark.shell import sqlContext\n#from pyspark.shell import spark\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql import *\nfrom pyspark.sql.functions import *\nfrom graphframes import *\nfrom pyspark.sql import functions as func\nfrom pyspark.sql.functions import explode\nimport math\n73/2:\n# sc = SparkContext('local')\n# sc = SparkSession.builder.appName('Lab5').master(\"local\").getOrCreate()\n# sqlContext = SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)\n# spark = SparkSession(sqlContext)\n\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"local-1608057188473\") \\\n    .getOrCreate()\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n73/3:\n# Layout schemas\nairportsSchema = StructType([\n    StructField(\"airport_id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"city\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"iata\", StringType(), True),\n    StructField(\"icao\", StringType(), True),\n    StructField(\"lat\", DoubleType(), True),\n    StructField(\"long\", DoubleType(), True),\n    StructField(\"alt\", IntegerType(), True),\n    StructField(\"tz\", DoubleType(), True),\n    StructField(\"dst\", StringType(), True),\n    StructField(\"tz_db\", StringType(), True),\n    StructField(\"type\", StringType(), True),\n    StructField(\"source\", StringType(), True)\n  ])\n\nairlinesSchema = StructType([\n    StructField(\"airline_id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"alias\", StringType(), True),\n    StructField(\"iata\", StringType(), True),\n    StructField(\"icao\", StringType(), True),\n    StructField(\"callsign\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"active\", StringType(), True)\n])\n\nroutesSchema = StructType([\n    StructField(\"airline\", StringType(), True),\n    StructField(\"airline_id\", IntegerType(), True),\n    StructField(\"source\", StringType(), True),\n    StructField(\"source_id\", IntegerType(), True),\n    StructField(\"dest\", StringType(), True),\n    StructField(\"dest_id\", IntegerType(), True),\n    StructField(\"sharecode\", StringType(), True),\n    StructField(\"stops\", IntegerType(), True),\n    StructField(\"equip\", StringType(), True)\n])\n73/4:\n#Read by from files by schemas\nairports = spark.read.format(\"csv\").option(\"header\", False).schema(airportsSchema).load(\"airports-extended.dat\")\nairlines = spark.read.format(\"csv\").option(\"header\", False).schema(airlinesSchema).load(\"airlines.dat\")\nroutes = spark.read.format(\"csv\").option(\"header\", False).schema(routesSchema).load(\"routes.dat\")\n73/5:\n# Task 1) Get companies with biggest and smallest sum of routes distances\ndef calcDist(srcLat, srcLong, destLat, destLong):\n    latDistance = math.radians(srcLat - destLat)\n    lngDistance = math.radians(srcLong - destLong)\n    sinLat = math.sin(latDistance / 2)\n    sinLng = math.sin(lngDistance / 2)\n    a = sinLat * sinLat + (math.cos(math.radians(srcLat)) * math.cos(math.radians(destLat)) * sinLng * sinLng)\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n    return int(c * 6371)\n\nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n\nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\"), col(\"airport_id\").alias(\"dest_id\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], (calcDist(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])))\n\n\nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"data\"]))\nairportGraph.cache()\n\nroutesDistancesSum = airportGraph \\\n    .edges.groupBy(col(\"data._2\").alias(\"airline_id\")) \\\n    .agg(func.sum(\"data._1\").alias(\"dist_sum\"))\n\n# Sorted to see biggest distances\nroutesDistancesSum.sort(col(\"dist_sum\").desc()).show()\n73/6:\n# Sorted to see smallest distances\nroutesDistancesSum.sort(col(\"dist_sum\").asc()).show()\n73/7:\n# Task 2) Show all routes between Ukraine and Italy\nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n\nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\"), col(\"stops\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .filter(\"country = 'Ukraine' OR country = 'Italy'\") \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"airline_id\"),col(\"stops\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")).filter(\"country = 'Ukraine' OR country = 'Italy'\") \\\n    .select(col(\"source_id\"), col(\"airport_id\").alias(\"dest_id\"), col(\"airline_id\"), col(\"stops\")) \\\n    .filter(\"stops < 3\") \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], 1))\n\nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\"), col(\"stops\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .filter(\"country = 'Ukraine' OR country = 'Italy'\") \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"airline_id\"),col(\"stops\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")).filter(\"country = 'Ukraine' OR country = 'Italy'\") \\\n    .select(col(\"source_id\"), col(\"airport_id\").alias(\"dest_id\"), col(\"airline_id\"), col(\"stops\")) \\\n    .filter(\"stops < 3\") \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], row[\"airline_id\"]))\n\nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"data\"]))\nairportGraph.cache()\nairportGraph.edges.show()\n73/8:\n# Task 3) Show all airports with biggest and smallest routes count\nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n\nairportRoutesEdges = routes \\\n    .select(col(\"source_id\"), col(\"dest_id\")) \\\n    .rdd.map(lambda row:  (row[\"source_id\"], row[\"dest_id\"], 1))\n\nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"data\"]))\nairportGraph.cache()\n\ngroup = airportGraph.edges.groupBy(col(\"src\"))\nroutesCount = group.agg(func.sum(\"data\").alias(\"route_count\"))\n\n# Sorted to see most routes\nroutesCount.sort(col(\"route_count\").desc()).show()\n73/9:\n# Sorted to see least routes\nroutesCount.sort(col(\"route_count\").asc()).show()\n73/10:\n# Task 4) Find the shortest and longest route between the two designated airports.\nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n                                     \nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\").cast(\"long\"), col(\"airport_id\").alias(\"dest_id\").cast(\"long\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row:  (row[\"source_id\"], row[\"dest_id\"], (calcDist(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])[1]))\n    \nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"dist\"]))\nairportGraph.cache()\n\nshort_path_small = airportGraph.shortestPaths(landmarks=[221])\nshort_path_small.select(\"id\", explode(\"distances\")).show()\n73/11:\n# Task 4) Find the shortest and longest route between the two designated airports.\nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n                                     \nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\").cast(\"long\"), col(\"airport_id\").alias(\"dest_id\").cast(\"long\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row:  (row[\"source_id\"], row[\"dest_id\"], (calcDist(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])[1]))\n    \nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"dist\"]))\nairportGraph.cache()\n\nshort_path_small = airportGraph.shortestPaths(landmarks=[221])\nshort_path_small.select(col(\"id\").cast(long), explode(\"distances\")).show()\n73/12:\n# Task 4) Find the shortest and longest route between the two designated airports.\nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n                                     \nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\").cast(\"long\"), col(\"airport_id\").alias(\"dest_id\").cast(\"long\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row:  (row[\"source_id\"], row[\"dest_id\"], (calcDist(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])[1]))\n    \nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"dist\"]))\nairportGraph.cache()\n\nshort_path_small = airportGraph.shortestPaths(landmarks=[221])\nshort_path_small.select(col(\"id\").cast(\"long\"), explode(\"distances\")).show()\n73/13:\n# Task 4) Find the shortest and longest route between the two designated airports.\nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n                                     \nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\").cast(\"long\"), col(\"airport_id\").alias(\"dest_id\").cast(\"long\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row:  (row[\"source_id\"], row[\"dest_id\"], (calcDist(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])[1]))\n    \nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"dist\"]))\nairportGraph.cache()\n\nshort_path_small = airportGraph.shortestPaths(landmarks=[221])\nshort_path_small.select(col(\"id\").cast(\"int\"), explode(\"distances\")).show()\n73/14:\n# Task 4) Find the shortest and longest route between the two designated airports.\nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\").cast(\"long\"), col(\"name\"))\n                                     \nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\").cast(\"long\"), col(\"airport_id\").alias(\"dest_id\").cast(\"long\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row:  (row[\"source_id\"], row[\"dest_id\"], (calcDist(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])[1]))\n    \nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"dist\"]))\nairportGraph.cache()\n\nshort_path_small = airportGraph.shortestPaths(landmarks=[221])\nshort_path_small.select(col(\"id\").cast(\"long\"), explode(\"distances\")).show()\n73/15:\nfilteredPaths = airportGraph.bfs(fromExpr = 'id = 2900', toExpr = 'id = 221', maxPathLength = 3) \nfilteredPaths.show()\n73/16:\nfrom pyspark import SparkConf\nfrom pyspark.context import SparkContext\nsc = SparkContext.getOrCreate(SparkConf())\n73/17:\n# Task 6) Find airports that have no connection \n\nairportRoutesVertices = airports \\\n    .select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n\nairportRoutesEdges = routes \\\n    .select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\").cast(\"long\"), col(\"airport_id\").alias(\"dest_id\").cast(\"long\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], (calcDist(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])))\n\nairportGraph = GraphFrame(airportRoutesVertices, sqlContext.createDataFrame(airportRoutesEdges, [\"src\", \"dst\", \"data\"]))\nairportGraph.persist()\n\nsc.setCheckpointDir(\"./\")\nresult = airportGraph.connectedComponents()\n73/18:\ninfo = result \\\n    .groupBy('component') \\\n    .agg({'id': 'count'}) \\\n    .withColumnRenamed('count(id)', 'component_size')\n\ninfo.filter('component_size == 1').join(\n  result, info.component == result.component, 'left'\n)\n73/19:\ninfo = result \\\n    .groupBy('component') \\\n    .agg({'id': 'count'}) \\\n    .withColumnRenamed('count(id)', 'component_size')\n\ninfo.filter('component_size == 1').join(\n  result, info.component == result.component, 'left'\n).show()\n73/20:\nnum_cc = result.select('component').distinct().count()\nprint(num_cc)\n75/1:\n#from pyspark.python.pyspark.shell import sqlContext\n#from pyspark.shell import spark\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql import *\nfrom pyspark.sql.functions import *\nfrom graphframes import *\nfrom pyspark.sql import functions as func\nfrom pyspark.sql.functions import explode\nimport math\n75/2:\n#sc = SparkSession.builder.appName('Lab4').master(\"local\").getOrCreate()\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"local-1608057188473\") \\\n    .getOrCreate()\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n75/3:\n# Preparing data\nairportsData = StructType([\n    StructField(\"airport_id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"city\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"iata\", StringType(), True),\n    StructField(\"icao\", StringType(), True),\n    StructField(\"lat\", DoubleType(), True),\n    StructField(\"long\", DoubleType(), True),\n    StructField(\"alt\", IntegerType(), True),\n    StructField(\"tz\", DoubleType(), True),\n    StructField(\"dst\", StringType(), True),\n    StructField(\"tz_db\", StringType(), True),\n    StructField(\"type\", StringType(), True),\n    StructField(\"source\", StringType(), True)\n])\n\nairlinesData = StructType([\n    StructField(\"airline_id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"alias\", StringType(), True),\n    StructField(\"iata\", StringType(), True),\n    StructField(\"icao\", StringType(), True),\n    StructField(\"callsign\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"active\", StringType(), True)\n])\n\nroutesData = StructType([\n    StructField(\"airline\", StringType(), True),\n    StructField(\"airline_id\", IntegerType(), True),\n    StructField(\"source\", StringType(), True),\n    StructField(\"source_id\", IntegerType(), True),\n    StructField(\"dest\", StringType(), True),\n    StructField(\"dest_id\", IntegerType(), True),\n    StructField(\"sharecode\", StringType(), True),\n    StructField(\"stops\", IntegerType(), True),\n    StructField(\"equip\", StringType(), True)\n])\n75/4:\nairports = spark.read.format(\"csv\").option(\"header\", False).schema(airportsData).load(\"airports-extended.dat\")\nairlines = spark.read.format(\"csv\").option(\"header\", False).schema(airlinesData).load(\"airlines.dat\")\nroutes = spark.read.format(\"csv\").option(\"header\", False).schema(routesData).load(\"routes.dat\")\n75/5:\ndef distance(srcLat, srcLong, destLat, destLong):\n    latDistance = math.radians(srcLat - destLat)\n    lngDistance = math.radians(srcLong - destLong)\n    sinLat = math.sin(latDistance / 2)\n    sinLng = math.sin(lngDistance / 2)\n    a = sinLat * sinLat + (math.cos(math.radians(srcLat)) * math.cos(math.radians(destLat)) * sinLng * sinLng)\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n    return int(c * 6371)\n75/6:\n# 1\nvertices = airports.select(col(\"airport_id\"), col(\"name\"))\n\nedges = routes.select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"source_lat\"), col(\"long\").alias(\"source_long\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\"), col(\"dest_id\"), col(\"source_lat\"), col(\"source_long\"), col(\"lat\").alias(\"dest_lat\"), col(\"long\").alias(\"dest_long\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], (distance(row[\"source_lat\"], row[\"source_long\"], row[\"dest_lat\"], row[\"dest_long\"]), row[\"airline_id\"])))\n\n\ngraph = GraphFrame(vertices, sqlContext.createDataFrame(edges))\ngraph.cache()\n\ndistancesSum = graph.edges.groupBy(col(\"data._2\").alias(\"airline\")).agg(func.sum(\"data._1\").alias(\"res\"))\n\n# Sorted to see biggest distances\ndistancesSum.sort(col(\"res\").desc()).show()\n75/7:\n# 1\nvertices = airports.select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n\nedges = routes.select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"source_lat\"), col(\"long\").alias(\"source_long\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\"), col(\"dest_id\"), col(\"source_lat\"), col(\"source_long\"), col(\"lat\").alias(\"dest_lat\"), col(\"long\").alias(\"dest_long\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], (distance(row[\"source_lat\"], row[\"source_long\"], row[\"dest_lat\"], row[\"dest_long\"]), row[\"airline_id\"])))\n\n\ngraph = GraphFrame(vertices, sqlContext.createDataFrame(edges))\ngraph.cache()\n\ndistancesSum = graph.edges.groupBy(col(\"data._2\").alias(\"airline\")).agg(func.sum(\"data._1\").alias(\"res\"))\n\n# Sorted to see biggest distances\ndistancesSum.sort(col(\"res\").desc()).show()\n75/8:\n# 1\nvertices = airports.select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n\nedges = routes.select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"source_lat\"), col(\"long\").alias(\"source_long\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\"), col(\"dest_id\"), col(\"source_lat\"), col(\"source_long\"), col(\"lat\").alias(\"dest_lat\"), col(\"long\").alias(\"dest_long\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], (distance(row[\"source_lat\"], row[\"source_long\"], row[\"dest_lat\"], row[\"dest_long\"]), row[\"airline_id\"])))\n\n\ngraph = GraphFrame(vertices, sqlContext.createDataFrame(edges, [\"src\", \"dst\", \"data\"]))\ngraph.cache()\n\ndistancesSum = graph.edges.groupBy(col(\"data._2\").alias(\"airline\")).agg(func.sum(\"data._1\").alias(\"res\"))\n\n# Sorted to see biggest distances\ndistancesSum.sort(col(\"res\").desc()).show()\n75/9:\n# Sorted to see smallest distances\ndistancesSum.sort(col(\"res\").asc()).show()\n75/10:\n# 2\nvertices = airports.select(col(\"airport_id\"), col(\"name\"))\n\nedges = routes.select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\"), col(\"stops\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .filter(\"country = 'Ukraine' OR country = 'Italy'\") \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"airline_id\"),col(\"stops\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")).filter(\"country = 'Ukraine' OR country = 'Italy'\") \\\n    .select(col(\"source_id\"), col(\"airport_id\").alias(\"dest_id\"), col(\"airline_id\"), col(\"stops\")) \\\n    .filter(\"stops < 3\") \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], row[\"airline_id\"]))\n\nairportGraph = GraphFrame(vertices, sqlContext.createDataFrame(edges))\nairportGraph.cache()\nairportGraph.edges.show()\n75/11:\n# 2\nvertices = airports.select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n\nedges = routes.select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\"), col(\"stops\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .filter(\"country = 'Ukraine' OR country = 'Italy'\") \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"airline_id\"),col(\"stops\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")).filter(\"country = 'Ukraine' OR country = 'Italy'\") \\\n    .select(col(\"source_id\"), col(\"airport_id\").alias(\"dest_id\"), col(\"airline_id\"), col(\"stops\")) \\\n    .filter(\"stops < 3\") \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], row[\"airline_id\"]))\n\nairportGraph = GraphFrame(vertices, sqlContext.createDataFrame(edges, [\"src\", \"dst\", \"data\"]))\nairportGraph.cache()\nairportGraph.edges.show()\n75/12:\n# 3\nvertices = airports.select(col(\"airport_id\"), col(\"name\"))\nedges = routes.select(col(\"source_id\"), col(\"dest_id\")).rdd.map(lambda row:  (row[\"source_id\"], row[\"dest_id\"], 1))\n\ngraph = GraphFrame(vertices, sqlContext.createDataFrame(edges, [\"source\", \"dest\", \"data\"]))\ngraph.cache()\n\ngroup = graph.edges.groupBy(col(\"source\"))\nroutesCount = group.agg(func.sum(\"data\").alias(\"res\"))\n\nroutesCount.sort(col(\"res\").desc()).show()\n75/13:\n# 3\nvertices = airports.select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\nedges = routes.select(col(\"source_id\"), col(\"dest_id\")).rdd.map(lambda row:  (row[\"source_id\"], row[\"dest_id\"], 1))\n\ngraph = GraphFrame(vertices, sqlContext.createDataFrame(edges, [\"source\", \"dest\", \"data\"]))\ngraph.cache()\n\ngroup = graph.edges.groupBy(col(\"source\"))\nroutesCount = group.agg(func.sum(\"data\").alias(\"res\"))\n\nroutesCount.sort(col(\"res\").desc()).show()\n75/14:\n# 3\nvertices = airports.select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\nedges = routes.select(col(\"source_id\"), col(\"dest_id\")).rdd.map(lambda row:  (row[\"source_id\"], row[\"dest_id\"], 1))\n\ngraph = GraphFrame(vertices, sqlContext.createDataFrame(edges, [\"src\", \"dst\", \"data\"]))\ngraph.cache()\n\ngroup = graph.edges.groupBy(col(\"source\"))\nroutesCount = group.agg(func.sum(\"data\").alias(\"res\"))\n\nroutesCount.sort(col(\"res\").desc()).show()\n75/15:\n# 3\nvertices = airports.select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\nedges = routes.select(col(\"source_id\"), col(\"dest_id\")).rdd.map(lambda row:  (row[\"source_id\"], row[\"dest_id\"], 1))\n\ngraph = GraphFrame(vertices, sqlContext.createDataFrame(edges, [\"src\", \"dst\", \"data\"]))\ngraph.cache()\n\ngroup = graph.edges.groupBy(col(\"src\"))\nroutesCount = group.agg(func.sum(\"data\").alias(\"res\"))\n\nroutesCount.sort(col(\"res\").desc()).show()\n75/16:\n# Sorted to see least routes\nroutesCount.sort(col(\"res\").asc()).show()\n75/17:\n# Sorted to see least routes\nroutesCount.sort(col(\"src\").asc()).show()\n75/18:\n# Sorted to see least routes\nroutesCount.sort(col(\"res\").asc()).show()\n75/19:\n# 4\nvertices = airports.select(col(\"airport_id\").cast(\"long\"), col(\"name\"))\n                                     \nedges = routes.select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\").cast(\"long\"), col(\"airport_id\").alias(\"dest_id\").cast(\"long\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row:  (row[\"source_id\"], row[\"dest_id\"], (distance(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])[1]))\n    \ngraph = GraphFrame(vertices, sqlContext.createDataFrame(edges, [\"src\", \"dst\", \"dist\"]))\ngraph.cache()\n\nshort_path_small = graph.shortestPaths(landmarks=[221])\nshort_path_small.select(col(\"airport_id\").cast(\"long\"), explode(\"distances\")).show()\n75/20:\n# 4\nvertices = airports.select(col(\"airport_id\").alias(\"id\").cast(\"long\"), col(\"name\"))\n                                     \nedges = routes.select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\").cast(\"long\"), col(\"airport_id\").alias(\"dest_id\").cast(\"long\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row:  (row[\"source_id\"], row[\"dest_id\"], (distance(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])[1]))\n    \ngraph = GraphFrame(vertices, sqlContext.createDataFrame(edges, [\"src\", \"dst\", \"dist\"]))\ngraph.cache()\n\nshort_path_small = graph.shortestPaths(landmarks=[221])\nshort_path_small.select(col(\"airport_id\").cast(\"long\"), explode(\"distances\")).show()\n75/21:\n# 4\nvertices = airports.select(col(\"airport_id\").alias(\"id\").cast(\"long\"), col(\"name\"))\n                                     \nedges = routes.select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\").cast(\"long\"), col(\"airport_id\").alias(\"dest_id\").cast(\"long\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row:  (row[\"source_id\"], row[\"dest_id\"], (distance(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])[1]))\n    \ngraph = GraphFrame(vertices, sqlContext.createDataFrame(edges, [\"src\", \"dst\", \"dist\"]))\ngraph.cache()\n\nshort_path_small = graph.shortestPaths(landmarks=[221])\nshort_path_small.select(col(\"airport_id\").alias(\"id\").cast(\"long\"), explode(\"distances\")).show()\n75/22:\n# 4\nvertices = airports.select(col(\"airport_id\").alias(\"id\").cast(\"long\"), col(\"name\"))\n                                     \nedges = routes.select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\").cast(\"long\"), col(\"airport_id\").alias(\"dest_id\").cast(\"long\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row:  (row[\"source_id\"], row[\"dest_id\"], (distance(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])[1]))\n    \ngraph = GraphFrame(vertices, sqlContext.createDataFrame(edges, [\"src\", \"dst\", \"dist\"]))\ngraph.cache()\n\nshort_path_small = graph.shortestPaths(landmarks=[221])\nshort_path_small.select(col(\"id\").cast(\"long\"), explode(\"distances\")).show()\n75/23:\nfilteredPaths = airportGraph.bfs(fromExpr = 'id = 3877', toExpr = 'id = 221', maxPathLength = 3) \nfilteredPaths.collect()[0]\n75/24:\nfilteredPaths = airportGraph.bfs(fromExpr = 'id = 3877', toExpr = 'id = 221', maxPathLength = 3) \nfilteredPaths.show()\n75/25:\nfilteredPaths = airportGraph.bfs(fromExpr = 'id = 2900', toExpr = 'id = 221', maxPathLength = 3) \nfilteredPaths.collect()[0]\n75/26:\nfilteredPaths = airportGraph.bfs(fromExpr = 'id = 2900', toExpr = 'id = 221', maxPathLength = 3) \nfilteredPaths.show()\n75/27:\nfilteredPaths = graph.bfs(fromExpr = 'id = 2900', toExpr = 'id = 221', maxPathLength = 3) \nfilteredPaths.show()\n75/28:\nfilteredPaths = graph.bfs(fromExpr = 'id = 3877', toExpr = 'id = 221', maxPathLength = 3) \nfilteredPaths.collect()[0]\n75/29:\nfilteredPaths = graph.bfs(fromExpr = 'id = 3877', toExpr = 'id = 221', maxPathLength = 3) \nfilteredPaths.collect()[0].show()\n75/30:\nfilteredPaths = graph.bfs(fromExpr = 'id = 3877', toExpr = 'id = 221', maxPathLength = 3) \nfilteredPaths.collect()[0]\n75/31:\nfrom pyspark import SparkConf\nfrom pyspark.context import SparkContext\nsc = SparkContext.getOrCreate(SparkConf())\n75/32:\n# 6\nvertices = airports.select(col(\"airport_id\"), col(\"name\"))\n\nedges = routes.select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\").cast(\"long\"), col(\"airport_id\").alias(\"dest_id\").cast(\"long\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], (distance(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])))\n\ngraph = GraphFrame(vertices, sqlContext.createDataFrame(edges, [\"src\", \"dst\", \"data\"]))\ngraph.persist()\n\nsc.setCheckpointDir(\"./\")\nresult = airportGraph.connectedComponents()\n75/33:\n# 6\nvertices = airports.select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n\nedges = routes.select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\").cast(\"long\"), col(\"airport_id\").alias(\"dest_id\").cast(\"long\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], (distance(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])))\n\ngraph = GraphFrame(vertices, sqlContext.createDataFrame(edges, [\"src\", \"dst\", \"data\"]))\ngraph.persist()\n\nsc.setCheckpointDir(\"./\")\nresult = airportGraph.connectedComponents()\n75/34:\ninfo = result.groupBy('component').agg({'id': 'count'}).withColumnRenamed('count(id)', 'component_size')\n\ninfo.filter('component_size == 1').join(\n  result, info.component == result.component, 'left'\n).show()\n75/35:\nres = result.select('component').distinct().count()\nprint(res)\n75/36:\n# 2\nvertices = airports.select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n\nedges = routes.select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\"), col(\"stops\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .filter(\"country = 'Ukraine' OR country = 'Italy'\") \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"airline_id\"),col(\"stops\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")).filter(\"country = 'Ukraine' OR country = 'Italy'\") \\\n    .select(col(\"source_id\"), col(\"airport_id\").alias(\"dest_id\"), col(\"airline_id\"), col(\"stops\")) \\\n    .filter(\"stops < 3\") \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], row[\"airline_id\"]))\n\ngraph = GraphFrame(vertices, sqlContext.createDataFrame(edges, [\"src\", \"dst\", \"data\"]))\ngraph.cache()\ngraph.edges.show()\n75/37:\n# 3\nvertices = airports.select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\nedges = routes.select(col(\"source_id\"), col(\"dest_id\")).rdd.map(lambda row:  (row[\"source_id\"], row[\"dest_id\"], 1))\n\ngraph = GraphFrame(vertices, sqlContext.createDataFrame(edges, [\"src\", \"dst\", \"data\"]))\ngraph.cache()\n\ngroup = graph.edges.groupBy(col(\"src\"))\nroutesCount = group.agg(func.sum(\"data\").alias(\"res\"))\n\nroutesCount.sort(col(\"res\").desc()).show()\n75/38:\n# Sorted to see least routes\nroutesCount.sort(col(\"res\").asc()).show()\n75/39:\n# 6\nvertices = airports.select(col(\"airport_id\").alias(\"id\"), col(\"name\"))\n\nedges = routes.select(col(\"airline_id\"), col(\"source_id\"), col(\"dest_id\")) \\\n    .join(airports, col(\"source_id\") == col(\"airport_id\")) \\\n    .select(col(\"airport_id\").alias(\"source_id\"), col(\"dest_id\"), col(\"lat\").alias(\"lat_source\"), col(\"long\").alias(\"long_source\"), col(\"airline_id\")) \\\n    .join(airports, col(\"dest_id\") == col(\"airport_id\")) \\\n    .select(col(\"source_id\").cast(\"long\"), col(\"airport_id\").alias(\"dest_id\").cast(\"long\"), col(\"lat_source\"), col(\"long_source\"), col(\"lat\").alias(\"lat_dest\"), col(\"long\").alias(\"long_dest\"), col(\"airline_id\")) \\\n    .rdd.map(lambda row: (row[\"source_id\"], row[\"dest_id\"], (distance(row[\"lat_source\"], row[\"long_source\"], row[\"lat_dest\"], row[\"long_dest\"]), row[\"airline_id\"])))\n\ngraph = GraphFrame(vertices, sqlContext.createDataFrame(edges, [\"src\", \"dst\", \"data\"]))\ngraph.persist()\n\nsc.setCheckpointDir(\"./\")\nresult = graph.connectedComponents()\n75/40:\ninfo = result.groupBy('component').agg({'id': 'count'}).withColumnRenamed('count(id)', 'component_size')\n\ninfo.filter('component_size == 1').join(\n  result, info.component == result.component, 'left'\n).show()\n75/41:\nres = result.select('component').distinct().count()\nprint(res)\n78/1:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n78/2:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n78/3:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n79/1:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n79/2:\nschema = StructType([\n    StructField(\"ID\", IntegerType(), False),\n    StructField(\"AGE\", IntegerType(), False),\n    StructField(\"SEX\", StringType(), False),\n    StructField(\"CASTE_NAME\", StringType(), False),\n    StructField(\"CATEGORY_CODE\", StringType(), False),\n    StructField(\"CATEGORY_NAME\", StringType(), False),\n    StructField(\"SURGERY_CODE\", StringType(), False),\n    StructField(\"SURGERY\", StringType(), False),\n    StructField(\"DISTRICT_NAME\", StringType(), False),\n    StructField(\"PREAUTH_DATE\", DateType(), False),\n    StructField(\"PREAUTH_AMT\", IntegerType(), False),\n    StructField(\"CLAIM_DATE\", DateType (), False),\n    StructField(\"CLAIM_AMOUNT\", IntegerType(), False),\n    StructField(\"HOSP_NAME\", StringType(), False),\n    StructField(\"HOSP_TYPE\", StringType(), False),\n    StructField(\"HOSP_DISTRICT\", StringType(), False),\n    StructField(\"SURGERY_DATE\", DateType(), False),\n    StructField(\"DISCHARGE_DATE\", DateType(), False),\n    StructField(\"Mortality Y / N\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./hospital_data.csv\")\ncols = dataset.columns\n79/3: dataset.shop(10)\n79/4: dataset.limit(10).toPandas()\n79/5:\ncategoricalColumns = [\n    \"SEX\",\n    \"CASTE_NAME\",\n    \"CATEGORY_CODE\",\n    \"SURGERY_CODE\",\n    \"DISTRICT_NAME\",\n    \"HOSP_NAME\",\n    \"HOSP_TYPE\",\n    \"HOSP_DISTRICT\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n79/6:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"Mortality Y / N\", outputCol=\"label\")\nstages += [label_stringIdx]\n79/7:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"AGE\",\n\"PREAUTH_AMT\",\n\"CLAIM_AMOUNT\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n79/8:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n79/9:\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"features\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(preppedDataDF)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n79/10:\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\nstages += [rf]\npipeline = Pipeline().setStages(stages)\npipeModel = pipeline.fit(dataset)\npredictions = pipeModel.transform(dataset)\n\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"features\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n79/11:\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\nstages += [rf]\npipeline = Pipeline().setStages(stages)\npipeModel = pipeline.fit(dataset)\npredictions = pipeModel.transform(dataset)\n\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"features\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n79/12:\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\nstages += [rf]\npipeline = Pipeline().setStages(stages)\npipeModel = pipeline.fit(dataset)\npredictions = pipeModel.transform(dataset)\n\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"features\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n79/13:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n79/14:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)\nrfModel = pipeModel.stages[2]\nprint(rfModel)  # summary only\n79/15:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\naccuracy = evaluator.evaluate(predictions)\nrfModel = pipeModel.stages[2]\nprint(rfModel)  # summary only\nprint(accuracy)\n80/1:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n80/2:\nschema = StructType([\n    StructField(\"national_inv\", IntegerType(), False),\n    StructField(\"lead_time\", IntegerType(), False),\n    StructField(\"in_transit_qty\", IntegerType(), False),\n    StructField(\"forecast_3_month\", IntegerType(), False),\n    StructField(\"forecast_6_month\", IntegerType(), False),\n    StructField(\"forecast_9_month\", IntegerType(), False),\n    StructField(\"sales_1_month\", IntegerType(), False),\n    StructField(\"sales_3_month\", IntegerType(), False),\n    StructField(\"sales_6_month\", IntegerType(), False),\n    StructField(\"sales_9_month\", IntegerType(), False),\n    StructField(\"min_bank\", IntegerType(), False),\n    StructField(\"potential_issue\", IntegerType(), False),\n    StructField(\"pieces_past_due\", StringType(), False),\n    StructField(\"perf_6_month_avg\", IntegerType(), False),\n    StructField(\"perf_12_month_avg\", IntegerType(), False),\n    StructField(\"local_bo_qty\", IntegerType(), False),\n    StructField(\"deck_risk\", IntegerType(), False),\n    StructField(\"oe_constraint\", StringType(), False),\n    StructField(\"ppap_risk\", StringType(), False),\n    StructField(\"stop_auto_buy\", StringType(), False),\n    StructField(\"rev_stop\", StringType(), False),\n    StructField(\"went_on_backorder\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Training_Dataset_v2.csv\")\ncols = dataset.columns\n80/3: dataset.limit(10).toPandas()\n80/4:\ncategoricalColumns = [\n    \"pieces_past_due\",\n    \"oe_constraint\",\n    \"ppap_risk\",\n    \"stop_auto_buy\",\n    \"rev_stop\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n80/5:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"went_on_backorder\", outputCol=\"label\")\nstages += [label_stringIdx]\n80/6:\nschema = StructType([\n    StructField(\"sku\", IntegerType(), False),\n    StructField(\"national_inv\", IntegerType(), False),\n    StructField(\"lead_time\", IntegerType(), False),\n    StructField(\"in_transit_qty\", IntegerType(), False),\n    StructField(\"forecast_3_month\", IntegerType(), False),\n    StructField(\"forecast_6_month\", IntegerType(), False),\n    StructField(\"forecast_9_month\", IntegerType(), False),\n    StructField(\"sales_1_month\", IntegerType(), False),\n    StructField(\"sales_3_month\", IntegerType(), False),\n    StructField(\"sales_6_month\", IntegerType(), False),\n    StructField(\"sales_9_month\", IntegerType(), False),\n    StructField(\"min_bank\", IntegerType(), False),\n    StructField(\"potential_issue\", StringType(), False),\n    StructField(\"pieces_past_due\", IntegerType(), False),\n    StructField(\"perf_6_month_avg\", IntegerType(), False),\n    StructField(\"perf_12_month_avg\", IntegerType(), False),\n    StructField(\"local_bo_qty\", IntegerType(), False),\n    StructField(\"deck_risk\", StringType(), False),\n    StructField(\"oe_constraint\", StringType(), False),\n    StructField(\"ppap_risk\", StringType(), False),\n    StructField(\"stop_auto_buy\", StringType(), False),\n    StructField(\"rev_stop\", StringType(), False),\n    StructField(\"went_on_backorder\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Training_Dataset_v2.csv\")\ncols = dataset.columns\n80/7: dataset.limit(10).toPandas()\n80/8:\ncategoricalColumns = [\n    \"deck_risk\",\n    \"pieces_past_due\",\n    \"oe_constraint\",\n    \"ppap_risk\",\n    \"stop_auto_buy\",\n    \"rev_stop\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n80/9:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"sku\",\n\"national_inv\",\n\"lead_time\",\n\"in_transit_qty\",\n\"forecast_3_month\",\n\"forecast_6_month\",\n\"forecast_9_month\",\n\"sales_1_month\",\n\"sales_3_month\",\n\"sales_6_month\",\n\"sales_9_month\",\n\"min_bank\",\n\"pieces_past_due\",\n\"perf_6_month_avg\",\n\"perf_12_month_avg\",\n\"local_bo_qty\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n80/10:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n80/11:\n# Fit model to prepped data\nlrModel = LogisticRegression().fit(preppedDataDF)\n\n# ROC for training data\ndisplay(lrModel, preppedDataDF, \"ROC\")\n80/12:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n80/13:\nschema = StructType([\n    StructField(\"sku\", IntegerType(), False),\n    StructField(\"national_inv\", IntegerType(), False),\n    StructField(\"lead_time\", IntegerType(), False),\n    StructField(\"in_transit_qty\", IntegerType(), False),\n    StructField(\"forecast_3_month\", IntegerType(), False),\n    StructField(\"forecast_6_month\", IntegerType(), False),\n    StructField(\"forecast_9_month\", IntegerType(), False),\n    StructField(\"sales_1_month\", IntegerType(), False),\n    StructField(\"sales_3_month\", IntegerType(), False),\n    StructField(\"sales_6_month\", IntegerType(), False),\n    StructField(\"sales_9_month\", IntegerType(), False),\n    StructField(\"min_bank\", IntegerType(), False),\n    StructField(\"potential_issue\", StringType(), False),\n    StructField(\"pieces_past_due\", IntegerType(), False),\n    StructField(\"perf_6_month_avg\", IntegerType(), False),\n    StructField(\"perf_12_month_avg\", IntegerType(), False),\n    StructField(\"local_bo_qty\", IntegerType(), False),\n    StructField(\"deck_risk\", StringType(), False),\n    StructField(\"oe_constraint\", StringType(), False),\n    StructField(\"ppap_risk\", StringType(), False),\n    StructField(\"stop_auto_buy\", StringType(), False),\n    StructField(\"rev_stop\", StringType(), False),\n    StructField(\"went_on_backorder\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Training_Dataset_v2.csv\")\ncols = dataset.columns\n80/14: dataset.limit(10).toPandas()\n80/15:\ncategoricalColumns = [\n    \"deck_risk\",\n    \"pieces_past_due\",\n    \"oe_constraint\",\n    \"ppap_risk\",\n    \"stop_auto_buy\",\n    \"rev_stop\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n80/16:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"went_on_backorder\", outputCol=\"label\")\nstages += [label_stringIdx]\n80/17:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"sku\",\n\"national_inv\",\n\"lead_time\",\n\"in_transit_qty\",\n\"forecast_3_month\",\n\"forecast_6_month\",\n\"forecast_9_month\",\n\"sales_1_month\",\n\"sales_3_month\",\n\"sales_6_month\",\n\"sales_9_month\",\n\"min_bank\",\n\"pieces_past_due\",\n\"perf_6_month_avg\",\n\"perf_12_month_avg\",\n\"local_bo_qty\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n80/18:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n80/19:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\naccuracy = evaluator.evaluate(predictions)\nrfModel = pipeModel.stages[2]\nprint(rfModel)  # summary only\nprint(accuracy)\n80/20:\n# Fit model to prepped data\nlrModel = LogisticRegression().fit(preppedDataDF)\n\n# ROC for training data\ndisplay(lrModel, preppedDataDF, \"ROC\")\n80/21:\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(5,5))\nplt.plot([0, 1], [0, 1], 'r--')\nplt.plot(lrModel.summary.roc.select('FPR').collect(),\n         lrModel.summary.roc.select('TPR').collect())\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()\n80/22:\n### Randomly split data into training and test sets. set seed for reproducibility\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\nprint(trainingData.count())\nprint(testData.count())\n80/23:\nfrom pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(trainingData)\n80/24:\n# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + cols\ndataset = preppedDataDF.select(selectedcols)\ndisplay(dataset.limit(10).toPandas())\n80/25:\n### Randomly split data into training and test sets. set seed for reproducibility\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\nprint(trainingData.count())\nprint(testData.count())\n80/26:\nfrom pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(trainingData)\n80/27: predictions = lrModel.transform(testData)\n80/28:\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"AGE\", \"SEX\")\ndisplay(selected.limit(20).toPandas())\n80/29:\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"lead_time\", \"went_on_backorder\")\ndisplay(selected.limit(20).toPandas())\n80/30:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)\n80/31: evaluator.getMetricName()\n80/32: print(lr.explainParams())\n80/33:\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Create ParamGrid for Cross Validation\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(lr.maxIter, [1, 5, 10])\n             .build())\n80/34:\n# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData)\n# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n80/35:\n# Use test set to measure the accuracy of our model on new data\npredictions = cvModel.transform(testData)\n80/36:\n# cvModel uses the best model found from the Cross Validation\n# Evaluate best model\nevaluator.evaluate(predictions)\n82/1:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n82/2:\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"Source\", StringType(), False),\n    StructField(\"TMC\", FloatType(), False),\n    StructField(\"Severity\", IntegerType(), False),\n    StructField(\"Start_Time\", StringType(), False),\n    StructField(\"End_Time\", StringType(), False),\n    StructField(\"Start_Lat\", FloatType(), False),\n    StructField(\"Start_Lng\", FloatType(), False),\n    StructField(\"End_Lat\", FloatType(), False),\n    StructField(\"End_Lng\", FloatType(), False),\n    StructField(\"Distance(mi)\", FloatType(), False),\n    StructField(\"Description\", StringType(), False),\n    StructField(\"Number\", IntegerType(), False),\n    StructField(\"Street\", StringType(), False),\n    StructField(\"Side\", StringType(), False),\n    StructField(\"City\", StringType(), False),\n    StructField(\"County\", StringType(), False),\n    StructField(\"State\", StringType(), False),\n    StructField(\"Zipcode\", StringType(), False),\n    StructField(\"Country\", StringType(), False),\n    StructField(\"Timezone\", StringType(), False),\n    StructField(\"Airport_Code\", StringType(), False),\n    StructField(\"Weather_Timestamp\", StringType(), False),\n    StructField(\"Temperature(F)\", FloatType(), False),\n    StructField(\"Wind_Chill(F)\", FloatType(), False),\n    StructField(\"Humidity(%)\", FloatType(), False),\n    StructField(\"Pressure(in)\", FloatType(), False),\n    StructField(\"Visibility(mi)\", FloatType(), False),\n    StructField(\"Wind_Direction\", StringType(), False),\n    StructField(\"Wind_Speed(mph)\", FloatType(), False),\n    StructField(\"Precipitation(in)\", FloatType(), False),\n    StructField(\"Weather_Condition\", StringType(), False),\n    StructField(\"Amenity\", StringType(), False),\n    StructField(\"Bump\", StringType(), False),\n    StructField(\"Crossing\", StringType(), False),\n    StructField(\"Give_Way\", StringType(), False),\n    StructField(\"Junction\", StringType(), False),\n    StructField(\"No_Exit\", StringType(), False),\n    StructField(\"Railway\", StringType(), False),\n    StructField(\"Roundabout\", StringType(), False),\n    StructField(\"Station\", StringType(), False),\n    StructField(\"Stop\", StringType(), False),\n    StructField(\"Traffic_Calming\", StringType(), False),\n    StructField(\"Traffic_Signal\", StringType(), False),\n    StructField(\"Turning_Loop\", StringType(), False),\n    StructField(\"Sunrise_Sunset\", StringType(), False),\n    StructField(\"Civil_Twilight\", StringType(), False),\n    StructField(\"Nautical_Twilight\", StringType(), False),\n    StructField(\"Astronomical_Twilight\", StringType(), False),\n\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./US_Accidents_June20.csv\")\ncols = dataset.columns\n82/3: dataset.limit(10).toPandas()\n82/4:\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"Source\", StringType(), False),\n    StructField(\"TMC\", FloatType(), False),\n    StructField(\"Severity\", IntegerType(), False),\n    StructField(\"Start_Time\", StringType(), False),\n    StructField(\"End_Time\", StringType(), False),\n    StructField(\"Start_Lat\", FloatType(), False),\n    StructField(\"Start_Lng\", FloatType(), False),\n    StructField(\"End_Lat\", FloatType(), False),\n    StructField(\"End_Lng\", FloatType(), False),\n    StructField(\"Distance(mi)\", FloatType(), False),\n    StructField(\"Description\", StringType(), False),\n    StructField(\"Number\", IntegerType(), False),\n    StructField(\"Street\", StringType(), False),\n    StructField(\"Side\", StringType(), False),\n    StructField(\"City\", StringType(), False),\n    StructField(\"County\", StringType(), False),\n    StructField(\"State\", StringType(), False),\n    StructField(\"Zipcode\", StringType(), False),\n    StructField(\"Country\", StringType(), False),\n    StructField(\"Timezone\", StringType(), False),\n    StructField(\"Airport_Code\", StringType(), False),\n    StructField(\"Weather_Timestamp\", StringType(), False),\n    StructField(\"Temperature(F)\", FloatType(), False),\n    StructField(\"Wind_Chill(F)\", FloatType(), False),\n    StructField(\"Humidity(%)\", FloatType(), False),\n    StructField(\"Pressure(in)\", FloatType(), False),\n    StructField(\"Visibility(mi)\", FloatType(), False),\n    StructField(\"Wind_Direction\", StringType(), False),\n    StructField(\"Wind_Speed(mph)\", FloatType(), False),\n    StructField(\"Precipitation(in)\", FloatType(), False),\n    StructField(\"Weather_Condition\", StringType(), False),\n    StructField(\"Amenity\", StringType(), False),\n    StructField(\"Bump\", StringType(), False),\n    StructField(\"Crossing\", StringType(), False),\n    StructField(\"Give_Way\", StringType(), False),\n    StructField(\"Junction\", StringType(), False),\n    StructField(\"No_Exit\", StringType(), False),\n    StructField(\"Railway\", StringType(), False),\n    StructField(\"Roundabout\", StringType(), False),\n    StructField(\"Station\", StringType(), False),\n    StructField(\"Stop\", StringType(), False),\n    StructField(\"Traffic_Calming\", StringType(), False),\n    StructField(\"Traffic_Signal\", StringType(), False),\n    StructField(\"Turning_Loop\", StringType(), False),\n    StructField(\"Sunrise_Sunset\", StringType(), False),\n    StructField(\"Civil_Twilight\", StringType(), False),\n    StructField(\"Nautical_Twilight\", StringType(), False),\n    StructField(\"Astronomical_Twilight\", StringType(), False),\n\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./US_Accidents_June20.csv\")\ncols = dataset.columns\n82/5: dataset.limit(10).toPandas()\n82/6:\ncategoricalColumns = [\n    \"Wind_Direction\",\n\"Weather_Condition\",\n\"Amenity\",\n\"Bump\",\n\"Crossing\",\n\"Give_Way\",\n\"Junction\",\n\"No_Exit\",\n\"Railway\",\n\"Roundabout\",\n\"Station\",\n\"Stop\",\n\"Traffic_Calming\",\n\"Traffic_Signal\",\n\"Turning_Loop\",\n\"Sunrise_Sunset\",\n\"Civil_Twilight\",\n\"Nautical_Twilight\",\n\"Astronomical_Twilight\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n82/7:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"Severity\", outputCol=\"label\")\nstages += [label_stringIdx]\n82/8:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"TMC\",\n\"Severity\",\n\"Start_Lat\",\n\"Start_Lng\",\n\"End_Lat\",\n\"End_Lng\",\n\"Distance(mi)\",\n\"Temperature(F)\",\n\"Wind_Chill(F)\",\n\"Humidity(%)\",\n\"Pressure(in)\",\n\"Visibility(mi)\",\n\"Wind_Speed(mph)\",\n\"Precipitation(in)\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n82/9:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n82/10:\ncategoricalColumns = [\n    \"Wind_Direction\",\n\"Weather_Condition\",\n\"Amenity\",\n\"Bump\",\n\"Crossing\",\n\"Give_Way\",\n\"Junction\",\n\"No_Exit\",\n\"Railway\",\n\"Roundabout\",\n\"Station\",\n\"Stop\",\n\"Traffic_Calming\",\n\"Traffic_Signal\",\n\"Sunrise_Sunset\",\n\"Civil_Twilight\",\n\"Nautical_Twilight\",\n\"Astronomical_Twilight\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n82/11:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"Severity\", outputCol=\"label\")\nstages += [label_stringIdx]\n82/12:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"TMC\",\n\"Severity\",\n\"Start_Lat\",\n\"Start_Lng\",\n\"End_Lat\",\n\"End_Lng\",\n\"Distance(mi)\",\n\"Temperature(F)\",\n\"Wind_Chill(F)\",\n\"Humidity(%)\",\n\"Pressure(in)\",\n\"Visibility(mi)\",\n\"Wind_Speed(mph)\",\n\"Precipitation(in)\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n82/13:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n83/1:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n83/2:\nschema = StructType([\n    StructField(\"DAY_OF_MONTH\", StringType(), False),\n    StructField(\"DAY_OF_WEEK\", StringType(), False),\n    StructField(\"OP_UNIQUE_CARRIER\", StringType(), False),\n    StructField(\"OP_CARRIER_AIRLINE_ID\", StringType(), False),\n    StructField(\"OP_CARRIER\", StringType(), False),\n    StructField(\"TAIL_NUM\", StringType(), False),\n    StructField(\"OP_CARRIER_FL_NUM\", IntegerType(), False),\n    StructField(\"ORIGIN_AIRPORT_ID\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"ORIGIN\", StringType(), False),\n    StructField(\"DEST_AIRPORT_ID\", StringType(), False),\n    StructField(\"DEST_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"DEST\", StringType(), False),\n    StructField(\"DEP_TIME\", FloatType(), False),\n    StructField(\"DEP_DEL15\", StringType(), False),\n    StructField(\"DEP_TIME_BLK\", StringType(), False),\n    StructField(\"ARR_TIME\", FloatType(), False),\n    StructField(\"ARR_DEL15\", StringType(), False),\n    StructField(\"CANCELLED\", StringType(), False),\n    StructField(\"DIVERTED\", StringType(), False),\n    StructField(\"DISTANCE\", FloatType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Jan_2019_ontime.csv\")\ncols = dataset.columns\n83/3: dataset.limit(10).toPandas()\n83/4:\ncategoricalColumns = [\n    \"OP_UNIQUE_CARRIER\",\n\"ORIGIN_AIRPORT_ID\",\n\"DEST_AIRPORT_ID\",\n\"ARR_DEL15\",\n\"TAIL_NUM\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n83/5:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"ARR_DEL15\", outputCol=\"label\")\nstages += [label_stringIdx]\n83/6:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"DAY_OF_MONTH\",\n\"DAY_OF_WEEK\",\n\"OP_CARRIER_FL_NUM\",\n\"DEP_TIME\",\n\"ARR_TIME\",\n\"DISTANCE\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n83/7:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n83/8:\nschema = StructType([\n    StructField(\"DAY_OF_MONTH\", IntegerType(), False),\n    StructField(\"DAY_OF_WEEK\", IntegerType(), False),\n    StructField(\"OP_UNIQUE_CARRIER\", StringType(), False),\n    StructField(\"OP_CARRIER_AIRLINE_ID\", StringType(), False),\n    StructField(\"OP_CARRIER\", StringType(), False),\n    StructField(\"TAIL_NUM\", StringType(), False),\n    StructField(\"OP_CARRIER_FL_NUM\", IntegerType(), False),\n    StructField(\"ORIGIN_AIRPORT_ID\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"ORIGIN\", StringType(), False),\n    StructField(\"DEST_AIRPORT_ID\", StringType(), False),\n    StructField(\"DEST_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"DEST\", StringType(), False),\n    StructField(\"DEP_TIME\", FloatType(), False),\n    StructField(\"DEP_DEL15\", StringType(), False),\n    StructField(\"DEP_TIME_BLK\", StringType(), False),\n    StructField(\"ARR_TIME\", FloatType(), False),\n    StructField(\"ARR_DEL15\", StringType(), False),\n    StructField(\"CANCELLED\", StringType(), False),\n    StructField(\"DIVERTED\", StringType(), False),\n    StructField(\"DISTANCE\", FloatType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Jan_2019_ontime.csv\")\ncols = dataset.columns\n83/9: dataset.limit(10).toPandas()\n83/10:\ncategoricalColumns = [\n    \"OP_UNIQUE_CARRIER\",\n\"ORIGIN_AIRPORT_ID\",\n\"DEST_AIRPORT_ID\",\n\"ARR_DEL15\",\n\"TAIL_NUM\",\n\"ORIGIN\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n83/11:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"ARR_DEL15\", outputCol=\"label\")\nstages += [label_stringIdx]\n83/12:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"DAY_OF_MONTH\",\n\"DAY_OF_WEEK\",\n\"OP_CARRIER_FL_NUM\",\n\"DEP_TIME\",\n\"ARR_TIME\",\n\"DISTANCE\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n83/13:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n83/14:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\naccuracy = evaluator.evaluate(predictions)\nrfModel = pipeModel.stages[2]\nprint(rfModel)  # summary only\nprint(accuracy)\n83/15:\n# Fit model to prepped data\nlrModel = LogisticRegression().fit(preppedDataDF)\n\n# ROC for training data\ndisplay(lrModel, preppedDataDF, \"ROC\")\n83/16:\nschema = StructType([\n    StructField(\"DAY_OF_MONTH\", IntegerType(), False),\n    StructField(\"DAY_OF_WEEK\", IntegerType(), False),\n    StructField(\"OP_UNIQUE_CARRIER\", StringType(), False),\n    StructField(\"OP_CARRIER_AIRLINE_ID\", StringType(), False),\n    StructField(\"OP_CARRIER\", StringType(), False),\n    StructField(\"TAIL_NUM\", StringType(), False),\n    StructField(\"OP_CARRIER_FL_NUM\", IntegerType(), False),\n    StructField(\"ORIGIN_AIRPORT_ID\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"ORIGIN\", StringType(), False),\n    StructField(\"DEST_AIRPORT_ID\", StringType(), False),\n    StructField(\"DEST_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"DEST\", StringType(), False),\n    StructField(\"DEP_TIME\", FloatType(), False),\n    StructField(\"DEP_DEL15\", StringType(), False),\n    StructField(\"DEP_TIME_BLK\", StringType(), False),\n    StructField(\"ARR_TIME\", FloatType(), False),\n    StructField(\"ARR_DEL15\", StringType(), False),\n    StructField(\"CANCELLED\", StringType(), False),\n    StructField(\"DIVERTED\", StringType(), False),\n    StructField(\"DISTANCE\", FloatType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Jan_2019_ontime.csv\")\ncols = dataset.columns\n83/17:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n83/18:\nschema = StructType([\n    StructField(\"DAY_OF_MONTH\", IntegerType(), False),\n    StructField(\"DAY_OF_WEEK\", IntegerType(), False),\n    StructField(\"OP_UNIQUE_CARRIER\", StringType(), False),\n    StructField(\"OP_CARRIER_AIRLINE_ID\", StringType(), False),\n    StructField(\"OP_CARRIER\", StringType(), False),\n    StructField(\"TAIL_NUM\", StringType(), False),\n    StructField(\"OP_CARRIER_FL_NUM\", IntegerType(), False),\n    StructField(\"ORIGIN_AIRPORT_ID\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"ORIGIN\", StringType(), False),\n    StructField(\"DEST_AIRPORT_ID\", StringType(), False),\n    StructField(\"DEST_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"DEST\", StringType(), False),\n    StructField(\"DEP_TIME\", FloatType(), False),\n    StructField(\"DEP_DEL15\", StringType(), False),\n    StructField(\"DEP_TIME_BLK\", StringType(), False),\n    StructField(\"ARR_TIME\", FloatType(), False),\n    StructField(\"ARR_DEL15\", StringType(), False),\n    StructField(\"CANCELLED\", StringType(), False),\n    StructField(\"DIVERTED\", StringType(), False),\n    StructField(\"DISTANCE\", FloatType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Jan_2019_ontime.csv\")\ncols = dataset.columns\n83/19: dataset.limit(10).toPandas()\n83/20:\ncategoricalColumns = [\n    \"OP_UNIQUE_CARRIER\",\n\"ORIGIN_AIRPORT_ID\",\n\"DEST_AIRPORT_ID\",\n\"ARR_DEL15\",\n\"TAIL_NUM\",\n\"ORIGIN\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n83/21:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"ARR_DEL15\", outputCol=\"label\")\nstages += [label_stringIdx]\n83/22:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"DAY_OF_MONTH\",\n\"DAY_OF_WEEK\",\n\"OP_CARRIER_FL_NUM\",\n\"DEP_TIME\",\n\"ARR_TIME\",\n\"DISTANCE\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n83/23:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n83/24:\n# Fit model to prepped data\nlrModel = LogisticRegression().fit(preppedDataDF)\n\n# ROC for training data\ndisplay(lrModel, preppedDataDF, \"ROC\")\n83/25:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\naccuracy = evaluator.evaluate(predictions)\nrfModel = pipeModel.stages[2]\nprint(rfModel)  # summary only\nprint(accuracy)\n83/26:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\naccuracy = evaluator.evaluate(preppedDataDF)\nrfModel = pipeModel.stages[2]\nprint(rfModel)  # summary only\nprint(accuracy)\n83/27:\ncategoricalColumns = [\n    \"OP_UNIQUE_CARRIER\",\n    \"ORIGIN_AIRPORT_ID\",\n    \"DEST_AIRPORT_ID\",\n    \"ARR_DEL15\",\n    \"TAIL_NUM\",\n    \"ORIGIN\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\", handleInvalud=\"skip\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n83/28:\ncategoricalColumns = [\n    \"OP_UNIQUE_CARRIER\",\n    \"ORIGIN_AIRPORT_ID\",\n    \"DEST_AIRPORT_ID\",\n    \"ARR_DEL15\",\n    \"TAIL_NUM\",\n    \"ORIGIN\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    stringIndexer.setHandleInvalid(\"skip\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n83/29:\nschema = StructType([\n    StructField(\"DAY_OF_MONTH\", IntegerType(), False),\n    StructField(\"DAY_OF_WEEK\", IntegerType(), False),\n    StructField(\"OP_UNIQUE_CARRIER\", StringType(), False),\n    StructField(\"OP_CARRIER_AIRLINE_ID\", StringType(), False),\n    StructField(\"OP_CARRIER\", StringType(), False),\n    StructField(\"TAIL_NUM\", StringType(), False),\n    StructField(\"OP_CARRIER_FL_NUM\", IntegerType(), False),\n    StructField(\"ORIGIN_AIRPORT_ID\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"ORIGIN\", StringType(), False),\n    StructField(\"DEST_AIRPORT_ID\", StringType(), False),\n    StructField(\"DEST_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"DEST\", StringType(), False),\n    StructField(\"DEP_TIME\", FloatType(), False),\n    StructField(\"DEP_DEL15\", StringType(), False),\n    StructField(\"DEP_TIME_BLK\", StringType(), False),\n    StructField(\"ARR_TIME\", FloatType(), False),\n    StructField(\"ARR_DEL15\", StringType(), False),\n    StructField(\"CANCELLED\", StringType(), False),\n    StructField(\"DIVERTED\", StringType(), False),\n    StructField(\"DISTANCE\", FloatType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Jan_2019_ontime.csv\")\ncols = dataset.columns\n83/30: dataset.limit(10).toPandas()\n83/31:\ncategoricalColumns = [\n    \"OP_UNIQUE_CARRIER\",\n    \"ORIGIN_AIRPORT_ID\",\n    \"DEST_AIRPORT_ID\",\n    \"ARR_DEL15\",\n    \"TAIL_NUM\",\n    \"ORIGIN\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    stringIndexer.setHandleInvalid(\"skip\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n83/32:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"ARR_DEL15\", outputCol=\"label\")\nstages += [label_stringIdx]\n83/33:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"DAY_OF_MONTH\",\n\"DAY_OF_WEEK\",\n\"OP_CARRIER_FL_NUM\",\n\"DEP_TIME\",\n\"ARR_TIME\",\n\"DISTANCE\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n83/34:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n83/35:\n# Fit model to prepped data\nlrModel = LogisticRegression().fit(preppedDataDF)\n\n# ROC for training data\ndisplay(lrModel, preppedDataDF, \"ROC\")\n83/36:\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(5,5))\nplt.plot([0, 1], [0, 1], 'r--')\nplt.plot(lrModel.summary.roc.select('FPR').collect(),\n         lrModel.summary.roc.select('TPR').collect())\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()\n83/37:\nschema = StructType([\n    StructField(\"DAY_OF_MONTH\", IntegerType(), False),\n    StructField(\"DAY_OF_WEEK\", IntegerType(), False),\n    StructField(\"OP_UNIQUE_CARRIER\", StringType(), False),\n    StructField(\"OP_CARRIER_AIRLINE_ID\", StringType(), False),\n    StructField(\"OP_CARRIER\", StringType(), False),\n    StructField(\"TAIL_NUM\", StringType(), False),\n    StructField(\"OP_CARRIER_FL_NUM\", IntegerType(), False),\n    StructField(\"ORIGIN_AIRPORT_ID\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"ORIGIN\", StringType(), False),\n    StructField(\"DEST_AIRPORT_ID\", StringType(), False),\n    StructField(\"DEST_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"DEST\", StringType(), False),\n    StructField(\"DEP_TIME\", FloatType(), False),\n    StructField(\"DEP_DEL15\", StringType(), False),\n    StructField(\"DEP_TIME_BLK\", StringType(), False),\n    StructField(\"ARR_TIME\", FloatType(), False),\n    StructField(\"ARR_DEL15\", StringType(), False),\n    StructField(\"CANCELLED\", StringType(), False),\n    StructField(\"DIVERTED\", StringType(), False),\n    StructField(\"DISTANCE\", FloatType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Jan_2019_ontime.csv\")\ncols = dataset.columns\n83/38: dataset.limit(10).toPandas()\n83/39:\ncategoricalColumns = [\n    \"OP_UNIQUE_CARRIER\",\n    \"ORIGIN_AIRPORT_ID\",\n    \"DEST_AIRPORT_ID\",\n    \"TAIL_NUM\",\n    \"ORIGIN\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    stringIndexer.setHandleInvalid(\"skip\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n83/40:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"ARR_DEL15\", outputCol=\"label\")\nstages += [label_stringIdx]\n83/41:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"DAY_OF_MONTH\",\n    \"DAY_OF_WEEK\",\n    \"OP_CARRIER_FL_NUM\",\n    \"DEP_TIME\",\n    \"ARR_TIME\",\n    \"DISTANCE\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n83/42:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n83/43:\n# Fit model to prepped data\nlrModel = LogisticRegression().fit(preppedDataDF)\n\n# ROC for training data\ndisplay(lrModel, preppedDataDF, \"ROC\")\n83/44:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n83/45:\nschema = StructType([\n    StructField(\"DAY_OF_MONTH\", IntegerType(), False),\n    StructField(\"DAY_OF_WEEK\", IntegerType(), False),\n    StructField(\"OP_UNIQUE_CARRIER\", StringType(), False),\n    StructField(\"OP_CARRIER_AIRLINE_ID\", StringType(), False),\n    StructField(\"OP_CARRIER\", StringType(), False),\n    StructField(\"TAIL_NUM\", StringType(), False),\n    StructField(\"OP_CARRIER_FL_NUM\", IntegerType(), False),\n    StructField(\"ORIGIN_AIRPORT_ID\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"ORIGIN\", StringType(), False),\n    StructField(\"DEST_AIRPORT_ID\", StringType(), False),\n    StructField(\"DEST_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"DEST\", StringType(), False),\n    StructField(\"DEP_TIME\", FloatType(), False),\n    StructField(\"DEP_DEL15\", StringType(), False),\n    StructField(\"DEP_TIME_BLK\", StringType(), False),\n    StructField(\"ARR_TIME\", FloatType(), False),\n    StructField(\"ARR_DEL15\", StringType(), False),\n    StructField(\"CANCELLED\", StringType(), False),\n    StructField(\"DIVERTED\", StringType(), False),\n    StructField(\"DISTANCE\", FloatType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Jan_2019_ontime.csv\")\ncols = dataset.columns\n83/46: dataset.limit(10).toPandas()\n83/47:\ncategoricalColumns = [\n    \"OP_UNIQUE_CARRIER\",\n    \"ORIGIN_AIRPORT_ID\",\n    \"DEST_AIRPORT_ID\",\n    \"TAIL_NUM\",\n    \"ORIGIN\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    stringIndexer.setHandleInvalid(\"skip\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n83/48:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"ARR_DEL15\", outputCol=\"label\")\nstages += [label_stringIdx]\n83/49:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"DAY_OF_MONTH\",\n    \"DAY_OF_WEEK\",\n    \"OP_CARRIER_FL_NUM\",\n    \"DEP_TIME\",\n    \"ARR_TIME\",\n    \"DISTANCE\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n83/50:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n83/51:\n# Fit model to prepped data\nlrModel = LogisticRegression().fit(preppedDataDF)\n\n# ROC for training data\ndisplay(lrModel, preppedDataDF, \"ROC\")\n83/52:\nschema = StructType([\n    StructField(\"DAY_OF_MONTH\", IntegerType(), False),\n    StructField(\"DAY_OF_WEEK\", IntegerType(), False),\n    StructField(\"OP_UNIQUE_CARRIER\", StringType(), False),\n    StructField(\"OP_CARRIER_AIRLINE_ID\", StringType(), False),\n    StructField(\"OP_CARRIER\", StringType(), False),\n    StructField(\"TAIL_NUM\", StringType(), False),\n    StructField(\"OP_CARRIER_FL_NUM\", IntegerType(), False),\n    StructField(\"ORIGIN_AIRPORT_ID\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"ORIGIN\", StringType(), False),\n    StructField(\"DEST_AIRPORT_ID\", StringType(), False),\n    StructField(\"DEST_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"DEST\", StringType(), False),\n    StructField(\"DEP_TIME\", FloatType(), False),\n    StructField(\"DEP_DEL15\", StringType(), False),\n    StructField(\"DEP_TIME_BLK\", StringType(), False),\n    StructField(\"ARR_TIME\", FloatType(), False),\n    StructField(\"ARR_DEL15\", StringType(), False),\n    StructField(\"CANCELLED\", StringType(), False),\n    StructField(\"DIVERTED\", StringType(), False),\n    StructField(\"DISTANCE\", FloatType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Jan_2019_ontime.csv\")\ncols = dataset.columns\n83/53: dataset.limit(10).toPandas()\n83/54:\ncategoricalColumns = [\n    \"OP_UNIQUE_CARRIER\",\n    \"ORIGIN_AIRPORT_ID\",\n    \"DEST_AIRPORT_ID\",\n    \"TAIL_NUM\",\n    \"ORIGIN\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    stringIndexer.setHandleInvalid(\"skip\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n83/55:\ncategoricalColumns = [\n    \"OP_UNIQUE_CARRIER\",\n    \"ORIGIN_AIRPORT_ID\",\n    \"DEST_AIRPORT_ID\",\n    \"TAIL_NUM\",\n    \"ORIGIN\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    stringIndexer.setHandleInvalid(\"skip\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n83/56:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"ARR_DEL15\", outputCol=\"label\")\nlabel_stringIdx.setHandleInvalid(\"skip\")\nstages += [label_stringIdx]\n83/57:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"DAY_OF_MONTH\",\n    \"DAY_OF_WEEK\",\n    \"OP_CARRIER_FL_NUM\",\n    \"DEP_TIME\",\n    \"ARR_TIME\",\n    \"DISTANCE\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n83/58:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n83/59:\n# Fit model to prepped data\nlrModel = LogisticRegression().fit(preppedDataDF)\n\n# ROC for training data\ndisplay(lrModel, preppedDataDF, \"ROC\")\n83/60:\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(5,5))\nplt.plot([0, 1], [0, 1], 'r--')\nplt.plot(lrModel.summary.roc.select('FPR').collect(),\n         lrModel.summary.roc.select('TPR').collect())\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()\n83/61:\nschema = StructType([\n    StructField(\"DAY_OF_MONTH\", IntegerType(), False),\n    StructField(\"DAY_OF_WEEK\", IntegerType(), False),\n    StructField(\"OP_UNIQUE_CARRIER\", StringType(), False),\n    StructField(\"OP_CARRIER_AIRLINE_ID\", StringType(), False),\n    StructField(\"OP_CARRIER\", StringType(), False),\n    StructField(\"TAIL_NUM\", StringType(), False),\n    StructField(\"OP_CARRIER_FL_NUM\", IntegerType(), False),\n    StructField(\"ORIGIN_AIRPORT_ID\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"ORIGIN\", StringType(), False),\n    StructField(\"DEST_AIRPORT_ID\", StringType(), False),\n    StructField(\"DEST_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"DEST\", StringType(), False),\n    StructField(\"DEP_TIME\", FloatType(), False),\n    StructField(\"DEP_DEL15\", StringType(), False),\n    StructField(\"DEP_TIME_BLK\", StringType(), False),\n    StructField(\"ARR_TIME\", FloatType(), False),\n    StructField(\"ARR_DEL15\", StringType(), False),\n    StructField(\"CANCELLED\", StringType(), False),\n    StructField(\"DIVERTED\", StringType(), False),\n    StructField(\"DISTANCE\", FloatType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Jan_2019_ontime.csv\")\ncols = dataset.columns\n83/62: dataset.limit(10).toPandas()\n83/63:\ncategoricalColumns = [\n    \"OP_UNIQUE_CARRIER\",\n    \"ORIGIN_AIRPORT_ID\",\n    \"DEST_AIRPORT_ID\",\n    \"TAIL_NUM\",\n    \"ORIGIN\",\n\"DEP_TIME_BLK\",\n\"DEP_DEL15\",\n\"CANCELLED\",\n\"DIVERTED\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    stringIndexer.setHandleInvalid(\"skip\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n83/64:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"ARR_DEL15\", outputCol=\"label\")\nlabel_stringIdx.setHandleInvalid(\"skip\")\nstages += [label_stringIdx]\n83/65:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"DAY_OF_MONTH\",\n    \"DAY_OF_WEEK\",\n    \"OP_CARRIER_FL_NUM\",\n    \"DEP_TIME\",\n    \"ARR_TIME\",\n    \"DISTANCE\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n83/66:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n83/67:\n# Fit model to prepped data\nlrModel = LogisticRegression().fit(preppedDataDF)\n\n# ROC for training data\ndisplay(lrModel, preppedDataDF, \"ROC\")\n83/68:\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(5,5))\nplt.plot([0, 1], [0, 1], 'r--')\nplt.plot(lrModel.summary.roc.select('FPR').collect(),\n         lrModel.summary.roc.select('TPR').collect())\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()\n83/69:\nschema = StructType([\n    StructField(\"DAY_OF_MONTH\", IntegerType(), False),\n    StructField(\"DAY_OF_WEEK\", IntegerType(), False),\n    StructField(\"OP_UNIQUE_CARRIER\", StringType(), False),\n    StructField(\"OP_CARRIER_AIRLINE_ID\", StringType(), False),\n    StructField(\"OP_CARRIER\", StringType(), False),\n    StructField(\"TAIL_NUM\", StringType(), False),\n    StructField(\"OP_CARRIER_FL_NUM\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_ID\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"ORIGIN\", StringType(), False),\n    StructField(\"DEST_AIRPORT_ID\", StringType(), False),\n    StructField(\"DEST_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"DEST\", StringType(), False),\n    StructField(\"DEP_TIME\", FloatType(), False),\n    StructField(\"DEP_DEL15\", StringType(), False),\n    StructField(\"DEP_TIME_BLK\", StringType(), False),\n    StructField(\"ARR_TIME\", FloatType(), False),\n    StructField(\"ARR_DEL15\", StringType(), False),\n    StructField(\"CANCELLED\", StringType(), False),\n    StructField(\"DIVERTED\", StringType(), False),\n    StructField(\"DISTANCE\", FloatType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Jan_2019_ontime.csv\")\ncols = dataset.columns\n83/70:\nschema = StructType([\n    StructField(\"DAY_OF_MONTH\", IntegerType(), False),\n    StructField(\"DAY_OF_WEEK\", IntegerType(), False),\n    StructField(\"OP_UNIQUE_CARRIER\", StringType(), False),\n    StructField(\"OP_CARRIER_AIRLINE_ID\", StringType(), False),\n    StructField(\"OP_CARRIER\", StringType(), False),\n    StructField(\"TAIL_NUM\", StringType(), False),\n    StructField(\"OP_CARRIER_FL_NUM\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_ID\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"ORIGIN\", StringType(), False),\n    StructField(\"DEST_AIRPORT_ID\", StringType(), False),\n    StructField(\"DEST_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"DEST\", StringType(), False),\n    StructField(\"DEP_TIME\", FloatType(), False),\n    StructField(\"DEP_DEL15\", StringType(), False),\n    StructField(\"DEP_TIME_BLK\", StringType(), False),\n    StructField(\"ARR_TIME\", FloatType(), False),\n    StructField(\"ARR_DEL15\", StringType(), False),\n    StructField(\"CANCELLED\", StringType(), False),\n    StructField(\"DIVERTED\", StringType(), False),\n    StructField(\"DISTANCE\", FloatType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Jan_2019_ontime.csv\")\ncols = dataset.columns\n83/71: dataset.limit(10).toPandas()\n83/72:\ncategoricalColumns = [\n    \"ORIGIN_AIRPORT_ID\",\n    \"DEST_AIRPORT_ID\",\n    \"ORIGIN\",\n    \"DEP_TIME_BLK\",\n    \"DEP_DEL15\",\n    \"CANCELLED\",\n    \"DIVERTED\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    stringIndexer.setHandleInvalid(\"skip\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n83/73:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"ARR_DEL15\", outputCol=\"label\")\nlabel_stringIdx.setHandleInvalid(\"skip\")\nstages += [label_stringIdx]\n83/74:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"DAY_OF_MONTH\",\n    \"DAY_OF_WEEK\",\n    \"DEP_TIME\",\n    \"ARR_TIME\",\n    \"DISTANCE\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n83/75:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n83/76:\n# Fit model to prepped data\nlrModel = LogisticRegression().fit(preppedDataDF)\n\n# ROC for training data\ndisplay(lrModel, preppedDataDF, \"ROC\")\n83/77:\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(5,5))\nplt.plot([0, 1], [0, 1], 'r--')\nplt.plot(lrModel.summary.roc.select('FPR').collect(),\n         lrModel.summary.roc.select('TPR').collect())\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()\n83/78:\n# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + cols\ndataset = preppedDataDF.select(selectedcols)\ndisplay(dataset.limit(10).toPandas())\n83/79:\n### Randomly split data into training and test sets. set seed for reproducibility\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\nprint(trainingData.count())\nprint(testData.count())\n83/80:\nfrom pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(trainingData)\n83/81: predictions = lrModel.transform(testData)\n83/82:\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"lead_time\", \"went_on_backorder\")\ndisplay(selected.limit(20).toPandas())\n83/83:\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"DAY_OF_WEEK\", \"DISTANCE\")\ndisplay(selected.limit(20).toPandas())\n83/84:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)\n83/85: evaluator.getMetricName()\n83/86: print(lr.explainParams())\n83/87:\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Create ParamGrid for Cross Validation\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(lr.maxIter, [1, 5, 10])\n             .build())\n83/88:\n# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData)\n# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n85/1:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n85/2:\nschema = StructType([\n    StructField(\"DAY_OF_MONTH\", IntegerType(), False),\n    StructField(\"DAY_OF_WEEK\", IntegerType(), False),\n    StructField(\"OP_UNIQUE_CARRIER\", StringType(), False),\n    StructField(\"OP_CARRIER_AIRLINE_ID\", StringType(), False),\n    StructField(\"OP_CARRIER\", StringType(), False),\n    StructField(\"TAIL_NUM\", StringType(), False),\n    StructField(\"OP_CARRIER_FL_NUM\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_ID\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"ORIGIN\", StringType(), False),\n    StructField(\"DEST_AIRPORT_ID\", StringType(), False),\n    StructField(\"DEST_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"DEST\", StringType(), False),\n    StructField(\"DEP_TIME\", FloatType(), False),\n    StructField(\"DEP_DEL15\", StringType(), False),\n    StructField(\"DEP_TIME_BLK\", StringType(), False),\n    StructField(\"ARR_TIME\", FloatType(), False),\n    StructField(\"ARR_DEL15\", StringType(), False),\n    StructField(\"CANCELLED\", StringType(), False),\n    StructField(\"DIVERTED\", StringType(), False),\n    StructField(\"DISTANCE\", FloatType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Jan_2019_ontime.csv\")\ncols = dataset.columns\n85/3: dataset.limit(10).toPandas()\n85/4:\ncategoricalColumns = [\n    \"ORIGIN_AIRPORT_ID\",\n    \"DEST_AIRPORT_ID\",\n    \"ORIGIN\",\n    \"DEP_TIME_BLK\",\n    \"DEP_DEL15\",\n    \"CANCELLED\",\n    \"DIVERTED\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    stringIndexer.setHandleInvalid(\"skip\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n85/5:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"ARR_DEL15\", outputCol=\"label\")\nlabel_stringIdx.setHandleInvalid(\"skip\")\nstages += [label_stringIdx]\n85/6:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"DAY_OF_MONTH\",\n    \"DAY_OF_WEEK\",\n    \"DEP_TIME\",\n    \"ARR_TIME\",\n    \"DISTANCE\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n85/7:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n85/8:\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3])\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n                               labels=labelIndexer.labels)\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=stages)\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n85/9:\nimport org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3])\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n                               labels=labelIndexer.labels)\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=stages)\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n85/10:\nfrom pyspark.ml.classification import RandomForestClassifier\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3])\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n                               labels=labelIndexer.labels)\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=stages)\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n85/11:\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3])\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n                               labels=labelIndexer.labels)\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=stages)\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n85/12:\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3])\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n                               labels=labelIndexer.labels)\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=stages)\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n85/13:\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3])\n\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", numTrees=10)\n\n\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n                               labels=labelIndexer.labels)\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=stages)\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n85/14:\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3])\n\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(dataset)\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", numTrees=10)\n\n\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n                               labels=labelIndexer.labels)\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=stages)\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n85/15:\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3])\n\nlabelIndexer = StringIndexer(inputCol=\"ARR_DEL15\", outputCol=\"indexedLabel\").fit(dataset)\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", numTrees=10)\n\n\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n                               labels=labelIndexer.labels)\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=stages)\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n85/16:\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3])\n\nlabelIndexer = StringIndexer(inputCol=\"ARR_DEL15\", outputCol=\"indexedLabel\").fit(dataset)\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", numTrees=10)\n\n\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n                               labels=labelIndexer.labels)\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=stages)\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\n#predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n85/17:\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3])\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=stages)\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\n#predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = BinaryClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n85/18:\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3])\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=stages)\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\n#predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\n85/19:\npredictions.show(10)\nevaluator = BinaryClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n85/20:\nprint(predictions.limit(10).toPandas())\nevaluator = BinaryClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n85/21:\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3])\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=stages + [rf])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\n#predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\n85/22:\n#print(predictions.limit(10).toPandas())\nevaluator = BinaryClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n85/23:\n#print(predictions.limit(10).toPandas())\nevaluator = RandomForestClassifier(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n85/24:\n#print(predictions.limit(10).toPandas())\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n85/25:\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n#print(predictions.limit(10).toPandas())\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n85/26:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n85/27:\nschema = StructType([\n    StructField(\"DAY_OF_MONTH\", IntegerType(), False),\n    StructField(\"DAY_OF_WEEK\", IntegerType(), False),\n    StructField(\"OP_UNIQUE_CARRIER\", StringType(), False),\n    StructField(\"OP_CARRIER_AIRLINE_ID\", StringType(), False),\n    StructField(\"OP_CARRIER\", StringType(), False),\n    StructField(\"TAIL_NUM\", StringType(), False),\n    StructField(\"OP_CARRIER_FL_NUM\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_ID\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"ORIGIN\", StringType(), False),\n    StructField(\"DEST_AIRPORT_ID\", StringType(), False),\n    StructField(\"DEST_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"DEST\", StringType(), False),\n    StructField(\"DEP_TIME\", FloatType(), False),\n    StructField(\"DEP_DEL15\", StringType(), False),\n    StructField(\"DEP_TIME_BLK\", StringType(), False),\n    StructField(\"ARR_TIME\", FloatType(), False),\n    StructField(\"ARR_DEL15\", StringType(), False),\n    StructField(\"CANCELLED\", StringType(), False),\n    StructField(\"DIVERTED\", StringType(), False),\n    StructField(\"DISTANCE\", FloatType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Jan_2019_ontime.csv\")\ncols = dataset.columns\n85/28:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n85/29:\nschema = StructType([\n    StructField(\"DAY_OF_MONTH\", IntegerType(), False),\n    StructField(\"DAY_OF_WEEK\", IntegerType(), False),\n    StructField(\"OP_UNIQUE_CARRIER\", StringType(), False),\n    StructField(\"OP_CARRIER_AIRLINE_ID\", StringType(), False),\n    StructField(\"OP_CARRIER\", StringType(), False),\n    StructField(\"TAIL_NUM\", StringType(), False),\n    StructField(\"OP_CARRIER_FL_NUM\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_ID\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"ORIGIN\", StringType(), False),\n    StructField(\"DEST_AIRPORT_ID\", StringType(), False),\n    StructField(\"DEST_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"DEST\", StringType(), False),\n    StructField(\"DEP_TIME\", FloatType(), False),\n    StructField(\"DEP_DEL15\", StringType(), False),\n    StructField(\"DEP_TIME_BLK\", StringType(), False),\n    StructField(\"ARR_TIME\", FloatType(), False),\n    StructField(\"ARR_DEL15\", StringType(), False),\n    StructField(\"CANCELLED\", StringType(), False),\n    StructField(\"DIVERTED\", StringType(), False),\n    StructField(\"DISTANCE\", FloatType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Jan_2019_ontime.csv\")\ncols = dataset.columns\n85/30: dataset.limit(10).toPandas()\n85/31:\ncategoricalColumns = [\n    \"ORIGIN_AIRPORT_ID\",\n    \"DEST_AIRPORT_ID\",\n    \"ORIGIN\",\n    \"DEP_TIME_BLK\",\n    \"DEP_DEL15\",\n    \"CANCELLED\",\n    \"DIVERTED\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    stringIndexer.setHandleInvalid(\"skip\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n85/32:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"ARR_DEL15\", outputCol=\"label\")\nlabel_stringIdx.setHandleInvalid(\"skip\")\nstages += [label_stringIdx]\n85/33:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"DAY_OF_MONTH\",\n    \"DAY_OF_WEEK\",\n    \"DEP_TIME\",\n    \"ARR_TIME\",\n    \"DISTANCE\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n85/34:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n85/35:\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3])\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=stages + [rf])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\n#predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\n85/36:\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n#print(predictions.limit(10).toPandas())\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n85/37:\n# Fit model to prepped data\nlrModel = LogisticRegression().fit(preppedDataDF)\n\n# ROC for training data\ndisplay(lrModel, preppedDataDF, \"ROC\")\n85/38:\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(5,5))\nplt.plot([0, 1], [0, 1], 'r--')\nplt.plot(lrModel.summary.roc.select('FPR').collect(),\n         lrModel.summary.roc.select('TPR').collect())\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()\n85/39:\n# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + cols\ndataset = preppedDataDF.select(selectedcols)\ndisplay(dataset.limit(10).toPandas())\n85/40:\n### Randomly split data into training and test sets. set seed for reproducibility\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\nprint(trainingData.count())\nprint(testData.count())\n85/41:\nfrom pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(trainingData)\n85/42: predictions = lrModel.transform(testData)\n85/43:\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"DAY_OF_WEEK\", \"DISTANCE\")\ndisplay(selected.limit(20).toPandas())\n85/44:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)\n85/45: evaluator.getMetricName()\n85/46: print(lr.explainParams())\n85/47:\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Create ParamGrid for Cross Validation\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(lr.maxIter, [1, 5, 10])\n             .build())\n85/48:\n# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData)\n# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n85/49:\n# Use test set to measure the accuracy of our model on new data\npredictions = cvModel.transform(testData)\n85/50:\n# cvModel uses the best model found from the Cross Validation\n# Evaluate best model\nevaluator.evaluate(predictions)\n85/51: print('Model Intercept: ', cvModel.bestModel.intercept)\n85/52:\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\nweights = cvModel.bestModel.coefficients\nweights = [(float(w),) for w in weights]  # convert numpy type to float, and to tuple\nweightsDF = sqlContext.createDataFrame(weights, [\"Feature Weight\"])\ndisplay(weightsDF.limit(20).toPandas())\n85/53:\n# View best model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"DAY_OF_WEEK\", \"DISTANCE\")\ndisplay(selected.limit(20).toPandas())\n85/54:\n\nfrom pyspark.ml.classification import DecisionTreeClassifier\n\n# Create initial Decision Tree Model\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n\n# Train model with Training Data\ndtModel = dt.fit(trainingData)\n85/55:\nprint(\"numNodes = \", dtModel.numNodes)\nprint(\"depth = \", dtModel.depth)\n85/56: display(dtModel)\n85/57: predictions = dtModel.transform(testData)\n85/58: predictions.printSchema()\n85/59:\n# View model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"DAY_OF_WEEK\", \"DISTANCE\")\ndisplay(selected.limit(20).toPandas())\n85/60:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n# Evaluate model\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(predictions)\n85/61: dt.getImpurity()\n85/62:\n# Create ParamGrid for Cross Validation\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nparamGrid = (ParamGridBuilder()\n             .addGrid(dt.maxDepth, [1, 2, 6, 10])\n             .addGrid(dt.maxBins, [20, 40, 80])\n             .build())\n85/63:\n# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData)\n# Takes ~5 minutes\n86/1:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n86/2:\nschema = StructType([\n    StructField(\"DAY_OF_MONTH\", IntegerType(), False),\n    StructField(\"DAY_OF_WEEK\", IntegerType(), False),\n    StructField(\"OP_UNIQUE_CARRIER\", StringType(), False),\n    StructField(\"OP_CARRIER_AIRLINE_ID\", StringType(), False),\n    StructField(\"OP_CARRIER\", StringType(), False),\n    StructField(\"TAIL_NUM\", StringType(), False),\n    StructField(\"OP_CARRIER_FL_NUM\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_ID\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"ORIGIN\", StringType(), False),\n    StructField(\"DEST_AIRPORT_ID\", StringType(), False),\n    StructField(\"DEST_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"DEST\", StringType(), False),\n    StructField(\"DEP_TIME\", FloatType(), False),\n    StructField(\"DEP_DEL15\", StringType(), False),\n    StructField(\"DEP_TIME_BLK\", StringType(), False),\n    StructField(\"ARR_TIME\", FloatType(), False),\n    StructField(\"ARR_DEL15\", StringType(), False),\n    StructField(\"CANCELLED\", StringType(), False),\n    StructField(\"DIVERTED\", StringType(), False),\n    StructField(\"DISTANCE\", FloatType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Jan_2019_ontime.csv\")\ncols = dataset.columns\n86/3: dataset.limit(10).toPandas()\n86/4:\ncategoricalColumns = [\n    \"ORIGIN_AIRPORT_ID\",\n    \"DEST_AIRPORT_ID\",\n    \"ORIGIN\",\n    \"DEP_TIME_BLK\",\n    \"DEP_DEL15\",\n    \"CANCELLED\",\n    \"DIVERTED\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    stringIndexer.setHandleInvalid(\"skip\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n86/5:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"ARR_DEL15\", outputCol=\"label\")\nlabel_stringIdx.setHandleInvalid(\"skip\")\nstages += [label_stringIdx]\n86/6:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"DAY_OF_MONTH\",\n    \"DAY_OF_WEEK\",\n    \"DEP_TIME\",\n    \"ARR_TIME\",\n    \"DISTANCE\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n86/7:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n86/8:\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3])\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=stages + [rf])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\n#predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\n86/9:\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n#print(predictions.limit(10).toPandas())\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n86/10:\nfrom pyspark.ml.classification import GBTClassifier\ngbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Chain indexers and GBT in a Pipeline\npipeline = Pipeline(stages=stages + [gbt])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n86/11:\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\ngbtModel = model.stages[2]\nprint(gbtModel)  # summary only\n86/12:\n# Fit model to prepped data\nlrModel = LogisticRegression().fit(preppedDataDF)\n\n# ROC for training data\ndisplay(lrModel, preppedDataDF, \"ROC\")\n86/13:\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(5,5))\nplt.plot([0, 1], [0, 1], 'r--')\nplt.plot(lrModel.summary.roc.select('FPR').collect(),\n         lrModel.summary.roc.select('TPR').collect())\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()\n86/14:\n# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + cols\ndataset = preppedDataDF.select(selectedcols)\ndisplay(dataset.limit(10).toPandas())\n86/15:\n### Randomly split data into training and test sets. set seed for reproducibility\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\nprint(trainingData.count())\nprint(testData.count())\n86/16:\nfrom pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(trainingData)\n86/17: predictions = lrModel.transform(testData)\n86/18:\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"DAY_OF_WEEK\", \"DISTANCE\")\ndisplay(selected.limit(20).toPandas())\n86/19:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)\n86/20: evaluator.getMetricName()\n86/21: print(lr.explainParams())\n86/22:\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Create ParamGrid for Cross Validation\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(lr.maxIter, [1, 5, 10])\n             .build())\n86/23:\n# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData)\n# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n86/24:\n# Use test set to measure the accuracy of our model on new data\npredictions = cvModel.transform(testData)\n86/25:\n# cvModel uses the best model found from the Cross Validation\n# Evaluate best model\nevaluator.evaluate(predictions)\n86/26: print('Model Intercept: ', cvModel.bestModel.intercept)\n86/27:\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\nweights = cvModel.bestModel.coefficients\nweights = [(float(w),) for w in weights]  # convert numpy type to float, and to tuple\nweightsDF = sqlContext.createDataFrame(weights, [\"Feature Weight\"])\ndisplay(weightsDF.limit(20).toPandas())\n86/28:\n# View best model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"DAY_OF_WEEK\", \"DISTANCE\")\ndisplay(selected.limit(20).toPandas())\n86/29:\n\nfrom pyspark.ml.classification import DecisionTreeClassifier\n\n# Create initial Decision Tree Model\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n\n# Train model with Training Data\ndtModel = dt.fit(trainingData)\n86/30:\nprint(\"numNodes = \", dtModel.numNodes)\nprint(\"depth = \", dtModel.depth)\n86/31: display(dtModel)\n86/32: predictions = dtModel.transform(testData)\n86/33: predictions.printSchema()\n86/34:\n# View model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"DAY_OF_WEEK\", \"DISTANCE\")\ndisplay(selected.limit(20).toPandas())\n86/35:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n# Evaluate model\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(predictions)\n86/36: dt.getImpurity()\n86/37:\n# Create ParamGrid for Cross Validation\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nparamGrid = (ParamGridBuilder()\n             .addGrid(dt.maxDepth, [1, 2, 6, 10])\n             .addGrid(dt.maxBins, [20, 40, 80])\n             .build())\n86/38:\n# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData)\n# Takes ~5 minutes\n86/39:\nprint(\"numNodes = \", cvModel.bestModel.numNodes)\nprint(\"depth = \", cvModel.bestModel.depth)\n86/40: predictions = cvModel.transform(testData)\n86/41: evaluator.evaluate(predictions)\n86/42:\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"DAY_OF_WEEK\", \"DISTANCE\")\ndisplay(selected.limit(20).toPandas())\n89/1:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n89/2:\nschema = StructType([\n    StructField(\"DAY_OF_MONTH\", IntegerType(), False),\n    StructField(\"DAY_OF_WEEK\", IntegerType(), False),\n    StructField(\"OP_UNIQUE_CARRIER\", StringType(), False),\n    StructField(\"OP_CARRIER_AIRLINE_ID\", StringType(), False),\n    StructField(\"OP_CARRIER\", StringType(), False),\n    StructField(\"TAIL_NUM\", StringType(), False),\n    StructField(\"OP_CARRIER_FL_NUM\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_ID\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"ORIGIN\", StringType(), False),\n    StructField(\"DEST_AIRPORT_ID\", StringType(), False),\n    StructField(\"DEST_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"DEST\", StringType(), False),\n    StructField(\"DEP_TIME\", FloatType(), False),\n    StructField(\"DEP_DEL15\", StringType(), False),\n    StructField(\"DEP_TIME_BLK\", StringType(), False),\n    StructField(\"ARR_TIME\", FloatType(), False),\n    StructField(\"ARR_DEL15\", StringType(), False),\n    StructField(\"CANCELLED\", StringType(), False),\n    StructField(\"DIVERTED\", StringType(), False),\n    StructField(\"DISTANCE\", FloatType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Jan_2019_ontime.csv\")\ncols = dataset.columns\n89/3: dataset.limit(10).toPandas()\n89/4:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n89/5:\nschema = StructType([\n    StructField(\"DAY_OF_MONTH\", IntegerType(), False),\n    StructField(\"DAY_OF_WEEK\", IntegerType(), False),\n    StructField(\"OP_UNIQUE_CARRIER\", StringType(), False),\n    StructField(\"OP_CARRIER_AIRLINE_ID\", StringType(), False),\n    StructField(\"OP_CARRIER\", StringType(), False),\n    StructField(\"TAIL_NUM\", StringType(), False),\n    StructField(\"OP_CARRIER_FL_NUM\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_ID\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"ORIGIN\", StringType(), False),\n    StructField(\"DEST_AIRPORT_ID\", StringType(), False),\n    StructField(\"DEST_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"DEST\", StringType(), False),\n    StructField(\"DEP_TIME\", FloatType(), False),\n    StructField(\"DEP_DEL15\", StringType(), False),\n    StructField(\"DEP_TIME_BLK\", StringType(), False),\n    StructField(\"ARR_TIME\", FloatType(), False),\n    StructField(\"ARR_DEL15\", StringType(), False),\n    StructField(\"CANCELLED\", StringType(), False),\n    StructField(\"DIVERTED\", StringType(), False),\n    StructField(\"DISTANCE\", FloatType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Jan_2019_ontime.csv\")\ncols = dataset.columns\n89/6: dataset.limit(40).toPandas()\n91/1:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.sql import SQLContext\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Lab5\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n91/2:\n# preprocessing\n\nimport string\n# Remove punctuations from the sentence\ndef remove_punctuation(sentence):\n    punctuations = list(string.punctuation)\n    extra_punctuations = ['.', '``', '...', '\\'s', '--', '-', 'n\\'t', '_', '–']\n    punctuations += extra_punctuations\n    filtered = [w for w in sentence.lower() if w not in punctuations]\n    return (\"\".join(filtered)).split()\n\n\nimport pymorphy2\nmorph = pymorphy2.MorphAnalyzer(lang = 'uk')\n\ndef normalize_uk_words(sentence):\n    words = sentence.split()\n    normalized_words = list(map(lambda word: morph.parse(word)[0].normal_form, words))\n    return ' '.join(normalized_words)\n\nprint(remove_punctuation(normalize_uk_words('задіювалися, автоматизовані - пошукові роботи')))\n92/1:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.sql import SQLContext\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Lab5\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n92/2:\n# preprocessing\n\nimport string\n# Remove punctuations from the sentence\ndef remove_punctuation(sentence):\n    punctuations = list(string.punctuation)\n    extra_punctuations = ['.', '``', '...', '\\'s', '--', '-', 'n\\'t', '_', '–']\n    punctuations += extra_punctuations\n    filtered = [w for w in sentence.lower() if w not in punctuations]\n    return (\"\".join(filtered)).split()\n\n\nimport pymorphy2\nmorph = pymorphy2.MorphAnalyzer(lang = 'uk')\n\ndef normalize_uk_words(sentence):\n    words = sentence.split()\n    normalized_words = list(map(lambda word: morph.parse(word)[0].normal_form, words))\n    return ' '.join(normalized_words)\n\nprint(remove_punctuation(normalize_uk_words('задіювалися, автоматизовані - пошукові роботи')))\n92/3:\n# preprocessing\n\nimport string\n# Remove punctuations from the sentence\ndef remove_punctuation(sentence):\n    punctuations = list(string.punctuation)\n    extra_punctuations = ['.', '``', '...', '\\'s', '--', '-', 'n\\'t', '_', '–']\n    punctuations += extra_punctuations\n    filtered = [w for w in sentence.lower() if w not in punctuations]\n    return (\"\".join(filtered)).split()\n\n\nimport pymorphy2\nmorph = pymorphy2.MorphAnalyzer(lang = 'uk')\n\ndef normalize_uk_words(sentence):\n    words = sentence.split()\n    normalized_words = list(map(lambda word: morph.parse(word)[0].normal_form, words))\n    return ' '.join(normalized_words)\n\nprint(remove_punctuation(normalize_uk_words('задіювалися, автоматизовані - пошукові роботи')))\n92/4:\n# preprocessing\n\nimport string\n# Remove punctuations from the sentence\ndef remove_punctuation(sentence):\n    punctuations = list(string.punctuation)\n    extra_punctuations = ['.', '``', '...', '\\'s', '--', '-', 'n\\'t', '_', '–']\n    punctuations += extra_punctuations\n    filtered = [w for w in sentence.lower() if w not in punctuations]\n    return (\"\".join(filtered)).split()\n\n\nimport pymorphy2\nmorph = pymorphy2.MorphAnalyzer(lang = 'ru')\n\ndef normalize_uk_words(sentence):\n    words = sentence.split()\n    normalized_words = list(map(lambda word: morph.parse(word)[0].normal_form, words))\n    return ' '.join(normalized_words)\n\nprint(remove_punctuation(normalize_uk_words('задіювалися, автоматизовані - пошукові роботи')))\n92/5:\n# preprocessing\n\nimport string\n# Remove punctuations from the sentence\ndef remove_punctuation(sentence):\n    punctuations = list(string.punctuation)\n    extra_punctuations = ['.', '``', '...', '\\'s', '--', '-', 'n\\'t', '_', '–']\n    punctuations += extra_punctuations\n    filtered = [w for w in sentence.lower() if w not in punctuations]\n    return (\"\".join(filtered)).split()\n\n\nimport pymorphy2\nmorph = pymorphy2.MorphAnalyzer(lang = 'ru')\n\ndef normalize_uk_words(sentence):\n    words = sentence.split()\n    normalized_words = list(map(lambda word: morph.parse(word)[0].normal_form, words))\n    return ' '.join(normalized_words)\n\nprint(remove_punctuation(normalize_uk_words('задіювалися, автоматизовані - пошукові роботи')))\n92/6:\n# preprocessing\n\nimport string\n# Remove punctuations from the sentence\ndef remove_punctuation(sentence):\n    punctuations = list(string.punctuation)\n    extra_punctuations = ['.', '``', '...', '\\'s', '--', '-', 'n\\'t', '_', '–']\n    punctuations += extra_punctuations\n    filtered = [w for w in sentence.lower() if w not in punctuations]\n    return (\"\".join(filtered)).split()\n\n\nimport pymorphy2\nmorph = pymorphy2.MorphAnalyzer(lang = 'uk')\n\ndef normalize_uk_words(sentence):\n    words = sentence.split()\n    normalized_words = list(map(lambda word: morph.parse(word)[0].normal_form, words))\n    return ' '.join(normalized_words)\n\nprint(remove_punctuation(normalize_uk_words('задіювалися, автоматизовані - пошукові роботи')))\n92/7:\n# preprocessing\n\nimport string\n# Remove punctuations from the sentence\ndef remove_punctuation(sentence):\n    punctuations = list(string.punctuation)\n    extra_punctuations = ['.', '``', '...', '\\'s', '--', '-', 'n\\'t', '_', '–']\n    punctuations += extra_punctuations\n    filtered = [w for w in sentence.lower() if w not in punctuations]\n    return (\"\".join(filtered)).split()\n\n\nimport pymorphy2\nmorph = pymorphy2.MorphAnalyzer(lang = 'uk')\n\ndef normalize_uk_words(sentence):\n    words = sentence.split()\n    normalized_words = list(map(lambda word: morph.parse(word)[0].normal_form, words))\n    return ' '.join(normalized_words)\n\nprint(remove_punctuation(normalize_uk_words('задіювалися, автоматизовані - пошукові роботи')))\n92/8:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data):\n    hashing_tf = HashingTF(inputCol = 'words', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/9:\nconf = SparkConf()\nsc = SparkContext(conf=conf)\nsqlContext = SQLContext(sc)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = tweetsDF.select(\"BODY\")\nnormalizedData = data.map(lambda s: remove_punctuation(normalize_uk_words(s.message)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/10:\nfrom pyspark import SparkConf, SparkContext\nconf = SparkConf()\nsc = SparkContext(conf=conf)\nsqlContext = SQLContext(sc)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = tweetsDF.select(\"BODY\")\nnormalizedData = data.map(lambda s: remove_punctuation(normalize_uk_words(s.message)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/11:\nfrom pyspark import SparkConf, SparkContext\n\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = tweetsDF.select(\"BODY\")\nnormalizedData = data.map(lambda s: remove_punctuation(normalize_uk_words(s.message)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/12:\nfrom pyspark import SparkConf, SparkContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = tweetsDF.select(\"BODY\")\nnormalizedData = data.map(lambda s: remove_punctuation(normalize_uk_words(s.message)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/13:\nfrom pyspark import SparkConf, SparkContext\nsqlContext = SQLContext(sc)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = tweetsDF.select(\"BODY\")\nnormalizedData = data.map(lambda s: remove_punctuation(normalize_uk_words(s.message)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/14:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = tweetsDF.select(\"BODY\")\nnormalizedData = data.map(lambda s: remove_punctuation(normalize_uk_words(s.message)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/15:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = tweetsDF.select(\"BODY\")\nnormalizedData = data.map(lambda s: remove_punctuation(normalize_uk_words(s.message)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/16:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = tweetsDF.select(\"BODY\")\nnormalizedData = data.map(lambda s: remove_punctuation(normalize_uk_words(s.message)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/17:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\nnormalizedData = data.map(lambda s: remove_punctuation(normalize_uk_words(s.message)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/18:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\nnormalizedData = data.apply(lambda s: remove_punctuation(normalize_uk_words(s.message)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/19:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\nnormalizedData = data.toPandas().apply(lambda s: remove_punctuation(normalize_uk_words(s.message)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/20:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\nnormalizedData = data.rdd.map(lambda s: remove_punctuation(normalize_uk_words(s.message)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/21:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\nnormalizedData = data.toPandas().apply(lambda s: remove_punctuation(normalize_uk_words(s.message)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/22:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\ndata.show()\nnormalizedData = data.toPandas().apply(lambda s: remove_punctuation(normalize_uk_words(s.message)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/23:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\ndata.show()\nnormalizedData = data.toPandas().apply(lambda s: remove_punctuation(normalize_uk_words(s.Body)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/24:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\ndata.show()\nnormalizedData = data.toPandas().apply(lambda s: remove_punctuation(normalize_uk_words(s.Body)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/25:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\ndata.schema\nnormalizedData = data.toPandas().apply(lambda s: remove_punctuation(normalize_uk_words(s.Body)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/26:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\nprint(data.schema)\nnormalizedData = data.toPandas().apply(lambda s: remove_punctuation(normalize_uk_words(s.Body)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/27:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\ndata.printSchema()\nnormalizedData = data.toPandas().apply(lambda s: remove_punctuation(normalize_uk_words(s.Body)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/28:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n#data.printSchema()\nnormalizedData = data.toPandas().apply(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/29:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n#data.printSchema()\nnormalizedData = data.toPandas().apply(lambda s: remove_punctuation(normalize_uk_words(s)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/30:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n#data.printSchema()\nnormalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s)))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/31:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n#data.printSchema()\nnormalizedData = data.toPandas().apply(lambda s: print(s) )\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/32:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n#data.printSchema()\nnormalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[Body])))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/33:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n#data.printSchema()\nnormalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/34:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(normalizedData)\n\nprint(tf_idf_data)\n92/35:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(mappedData)\n\nprint(tf_idf_data)\n92/36:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    data_rdd_df = data_rdd.toDF()\n    hashing_tf = HashingTF(inputCol = 'words', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/37:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(mappedData)\n\nprint(tf_idf_data)\n92/38:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    data_rdd_df = data_rdd.toDF()\n    hashing_tf = HashingTF(inputCol = '_1', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/39:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    data_rdd_df = data_rdd.toDF()\n    hashing_tf = HashingTF(inputCol = '_1', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/40:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(mappedData)\n\nprint(tf_idf_data)\n92/41:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    data_rdd_df = data_rdd.toDF()\n    data_rdd.toPandast().printSchema()\n    hashing_tf = HashingTF(inputCol = 'words', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/42:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    data_rdd_df = data_rdd.toDF()\n    data_rdd.toPandast().printSchema()\n    hashing_tf = HashingTF(inputCol = 'words', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/43:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(mappedData)\n\nprint(tf_idf_data)\n92/44:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    data_rdd_df = data_rdd.toDF()\n    data_rdd.toPandas().printSchema()\n    hashing_tf = HashingTF(inputCol = 'words', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/45:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    data_rdd_df = data_rdd.toDF()\n    data_rdd.toPandas().printSchema()\n    hashing_tf = HashingTF(inputCol = 'words', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/46:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(mappedData)\n\nprint(tf_idf_data)\n92/47:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    data_rdd.df.toPandas().printSchema()\n    data_rdd_df = data_rdd.toDF()\n    hashing_tf = HashingTF(inputCol = 'words', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/48:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    data_rdd.df.toPandas().printSchema()\n    data_rdd_df = data_rdd.toDF()\n    hashing_tf = HashingTF(inputCol = 'words', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/49:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(mappedData)\n\nprint(tf_idf_data)\n92/50:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    data_rdd.df().toPandas().printSchema()\n    data_rdd_df = data_rdd.toDF()\n    hashing_tf = HashingTF(inputCol = 'words', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/51:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(mappedData)\n\nprint(tf_idf_data)\n92/52:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    pritn(data_rdd.take(10))\n    data_rdd_df = data_rdd.toDF()\n    hashing_tf = HashingTF(inputCol = 'words', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/53:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(mappedData)\n\nprint(tf_idf_data)\n92/54:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    print(data_rdd.take(10))\n    data_rdd_df = data_rdd.toDF()\n    hashing_tf = HashingTF(inputCol = 'words', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/55:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(mappedData)\n\nprint(tf_idf_data)\n92/56:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    print(data_rdd.take(10))\n    data_rdd_df = data_rdd.toDF()\n    hashing_tf = HashingTF(inputCol = 'body', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/57:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    print(data_rdd.take(10))\n    data_rdd_df = data_rdd.toDF()\n    hashing_tf = HashingTF(inputCol = 'body', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/58:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(mappedData)\n\nprint(tf_idf_data)\n92/59:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    print(data_rdd.take(10))\n    data_rdd_df = data_rdd.toDF()\n    data_rdd_df.show()\n    hashing_tf = HashingTF(inputCol = 'body', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/60:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(mappedData)\n\nprint(tf_idf_data)\n92/61:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    print(data_rdd.take(10))\n    data_rdd_df = data_rdd.toDF()\n    data_rdd_df.printSchema()\n    data_rdd_df.show(truncate=False)\n    hashing_tf = HashingTF(inputCol = 'body', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/62:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    print(data_rdd.take(10))\n    data_rdd_df = data_rdd.toDF()\n    data_rdd_df.printSchema()\n    data_rdd_df.show(truncate=False)\n    hashing_tf = HashingTF(inputCol = 'body', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/63:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(mappedData)\n\nprint(tf_idf_data)\n92/64:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n#     print(data_rdd.take(10))\n    data_rdd_df = data_rdd.toDF()\n#     data_rdd_df.printSchema()\n#     data_rdd_df.show(truncate=False)\n    hashing_tf = HashingTF(inputCol = 'body', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/65:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(mappedData)\n\nprint(tf_idf_data)\n92/66:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n#     print(data_rdd.take(10))\n    data_rdd_df = data_rdd.toDF()\n    data_rdd_df.printSchema()\n#     data_rdd_df.printSchema()\n#     data_rdd_df.show(truncate=False)\n    hashing_tf = HashingTF(inputCol = 'body', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/67:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(mappedData)\n\nprint(tf_idf_data)\n92/68:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n#     print(data_rdd.take(10))\n    data_rdd_df = data_rdd.toDF()\n    data_rdd_df.printSchema()\n#     data_rdd_df.printSchema()\n#     data_rdd_df.show(truncate=False)\n    hashing_tf = HashingTF(inputCol = '_1', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/69:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(mappedData)\n\nprint(tf_idf_data)\n92/70:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n#     print(data_rdd.take(10))\n    data_rdd.printSchema()\n    data_rdd_df = data_rdd.toDF()\n    data_rdd_df.printSchema()\n#     data_rdd_df.printSchema()\n#     data_rdd_df.show(truncate=False)\n    hashing_tf = HashingTF(inputCol = '_1', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/71:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(mappedData)\n\nprint(tf_idf_data)\n92/72:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n#     print(data_rdd.take(10))\n    #data_rdd.printSchema()\n    data_rdd_df = data_rdd.toDF()\n    data_rdd_df.printSchema()\n    \n#     data_rdd_df.printSchema()\n     data_rdd_df.show(truncate=False)\n    hashing_tf = HashingTF(inputCol = '_1', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/73:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n#     print(data_rdd.take(10))\n    #data_rdd.printSchema()\n    data_rdd_df = data_rdd.toDF()\n    data_rdd_df.printSchema()\n    \n#     data_rdd_df.printSchema()\n    data_rdd_df.show(truncate=False)\n    hashing_tf = HashingTF(inputCol = '_1', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/74:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(mappedData)\n\nprint(tf_idf_data)\n92/75:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n#     print(data_rdd.take(10))\n    #data_rdd.printSchema()\n    data_rdd_df = data_rdd.toDF()\n    print(data_rdd_df)\n    \n#     data_rdd_df.printSchema()\n    data_rdd_df.show(truncate=False)\n    hashing_tf = HashingTF(inputCol = '_1', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/76:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(mappedData)\n\nprint(tf_idf_data)\n92/77:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n#     print(data_rdd.take(10))\n    data_rdd.printSchema()\n    data_rdd_df = data_rdd.toDF()\n #   print(data_rdd_df)\n    \n#     data_rdd_df.printSchema()\n    data_rdd_df.show(truncate=False)\n    hashing_tf = HashingTF(inputCol = '_1', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/78:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(mappedData)\n\nprint(tf_idf_data)\n92/79:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    print(data_rdd.take(10))\n    data_rdd.printSchema()\n    data_rdd_df = data_rdd.toDF()\n #   print(data_rdd_df)\n    \n#     data_rdd_df.printSchema()\n    data_rdd_df.show(truncate=False)\n    hashing_tf = HashingTF(inputCol = '_1', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/80:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(mappedData)\n\nprint(tf_idf_data)\n92/81:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\ndata_df = sqlContext.createDataFrame(mappedData, ['BODY'])\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(data_df)\n\nprint(tf_idf_data)\n92/82:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    #print(data_rdd.take(10))\n    #data_rdd.printSchema()\n    data_rdd_df = data_rdd.toDF()\n #   print(data_rdd_df)\n    \n#     data_rdd_df.printSchema()\n    data_rdd_df.show(truncate=False)\n    hashing_tf = HashingTF(inputCol = '_1', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/83:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\ndata_df = sqlContext.createDataFrame(mappedData, ['BODY'])\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(data_df)\n\nprint(tf_idf_data)\n92/84:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    #print(data_rdd.take(10))\n    #data_rdd.printSchema()\n    data_rdd_df = data_rdd.toDF()\n #   print(data_rdd_df)\n    \n#     data_rdd_df.printSchema()\n    data_rdd_df.show(truncate=False)\n    hashing_tf = HashingTF(inputCol = 'BODY', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/85:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\ndata_df = sqlContext.createDataFrame(mappedData, ['BODY'])\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(data_df)\n\nprint(tf_idf_data)\n92/86:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\n# filteredData = data.rdd.filter(lambda s: s.BODY)\n# mappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n# data_df = sqlContext.createDataFrame(mappedData, ['BODY'])\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/87:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    #print(data_rdd.take(10))\n    #data_rdd.printSchema()\n    #data_rdd_df = data_rdd.toDF()\n #   print(data_rdd_df)\n    \n#     data_rdd_df.printSchema()\n    hashing_tf = HashingTF(inputCol = 'BODY', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd_df)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/88:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\n\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\ndata_df = sqlContext.createDataFrame(mappedData, ['BODY'])\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(data_df)\n\nprint(tf_idf_data)\n92/89:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    #print(data_rdd.take(10))\n    #data_rdd.printSchema()\n    #data_rdd_df = data_rdd.toDF()\n #   print(data_rdd_df)\n    \n#     data_rdd_df.printSchema()\n    hashing_tf = HashingTF(inputCol = 'BODY', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/90:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\n\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\ndata_df = sqlContext.createDataFrame(mappedData, ['BODY'])\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(data_df)\n\nprint(tf_idf_data)\n92/91:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nprint(filteredData.take(10))\n\n# mappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n# data_df = sqlContext.createDataFrame(mappedData, ['BODY'])\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/92:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndataset.show()\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nprint(filteredData.take(10))\n\n# mappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n# data_df = sqlContext.createDataFrame(mappedData, ['BODY'])\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/93:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.sql import SQLContext\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Lab5\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n92/94:\n# preprocessing\n\nimport string\n# Remove punctuations from the sentence\ndef remove_punctuation(sentence):\n    punctuations = list(string.punctuation)\n    extra_punctuations = ['.', '``', '...', '\\'s', '--', '-', 'n\\'t', '_', '–']\n    punctuations += extra_punctuations\n    filtered = [w for w in sentence.lower() if w not in punctuations]\n    return (\"\".join(filtered)).split()\n\n\nimport pymorphy2\nmorph = pymorphy2.MorphAnalyzer(lang = 'uk')\n\ndef normalize_uk_words(sentence):\n    words = sentence.split()\n    normalized_words = list(map(lambda word: morph.parse(word)[0].normal_form, words))\n    return ' '.join(normalized_words)\n\nprint(remove_punctuation(normalize_uk_words('задіювалися, автоматизовані - пошукові роботи')))\n92/95:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    #print(data_rdd.take(10))\n    #data_rdd.printSchema()\n    #data_rdd_df = data_rdd.toDF()\n #   print(data_rdd_df)\n    \n#     data_rdd_df.printSchema()\n    hashing_tf = HashingTF(inputCol = 'BODY', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['label', 'words', 'features'])\n92/96:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndataset.show()\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nprint(filteredData.take(10))\n\n# mappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n# data_df = sqlContext.createDataFrame(mappedData, ['BODY'])\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/97:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndataset.show()\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nprint(filteredData.take(10))\n\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\ndata_df = sqlContext.createDataFrame(mappedData, ['BODY'])\n\n#data.printSchema()\n#normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(data_df)\n\nprint(tf_idf_data)\n92/98:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndataset.show()\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nprint(filteredData.take(10))\n\n# mappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\n# data_df = sqlContext.createDataFrame(mappedData, ['BODY'])\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/99:\n",
      "from pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndataset.show()\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\ndata_df = sqlContext.createDataFrame(mappedData, ['BODY'])\ndata_df.printSchema()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/100:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndataset.show()\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\ndata_df = sqlContext.createDataFrame(mappedData, 'BODY')\ndata_df.printSchema()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/101:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndataset.show()\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY))).toDf()\ndata_df.printSchema()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/102:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndataset.show()\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY))).toDF()\ndata_df.printSchema()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/103:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndataset.show()\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s))).toDF()\ndata_df.printSchema()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/104:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndataset.show()\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY))).toDF([\"BODY\"])\ndata_df.printSchema()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/105:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndataset.show()\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY))).toDF([\"BODY\"])\nmappedData.printSchema()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/106:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndataset.show()\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY))).toDF()\nmappedData.printSchema()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/107:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndataset.show()\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY))).toDF()\nmappedData.printSchema()\nmappedData.show()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/108:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndataset.show()\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nprint(filteredData.take(10))\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY))).toDF()\nmappedData.printSchema()\nmappedData.show()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/109:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndataset.show()\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s[1]))).toDF()\nmappedData.printSchema()\nmappedData.show()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/110:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndataset.show()\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s[1])))\\\n    .toDF()\nmappedData.printSchema()\nmappedData.show()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/111:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndataset.show()\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words('задіювалися, автоматизовані - пошукові роботи')))\\\n    .toDF()\nmappedData.printSchema()\nmappedData.show()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/112:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\ndataset.show()\ndata = dataset.select(\"BODY\")\ndata.printSchema()\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\nprint(mappedData.take(10))\n# mappedData.printSchema()\n# mappedData.show()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/113:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\n\ndata = dataset.select(\"BODY\")\n\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: remove_punctuation(normalize_uk_words(s.BODY)))\nprint(mappedData.take(10))\n# mappedData.printSchema()\n# mappedData.show()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/114:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\n\ndata = dataset.select(\"BODY\")\n\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: Row(body=remove_punctuation(normalize_uk_words(s.BODY)))\nprint(mappedData.take(10))\n# mappedData.printSchema()\n# mappedData.show()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/115:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\n\ndata = dataset.select(\"BODY\")\n\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: Row(body=remove_punctuation(normalize_uk_words(s.BODY))))\nprint(mappedData.take(10))\n# mappedData.printSchema()\n# mappedData.show()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/116:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import Row\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\n\ndata = dataset.select(\"BODY\")\n\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: Row(body=remove_punctuation(normalize_uk_words(s.BODY))))\nprint(mappedData.take(10))\n# mappedData.printSchema()\n# mappedData.show()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/117:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import Row\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\n\ndata = dataset.select(\"BODY\")\n\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: Row(body=remove_punctuation(normalize_uk_words(s.BODY)))).toDF()\n#print(mappedData.take(10))\n mappedData.printSchema()\n# mappedData.show()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/118:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import Row\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\n\ndata = dataset.select(\"BODY\")\n\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: Row(body=remove_punctuation(normalize_uk_words(s.BODY)))).toDF()\n#print(mappedData.take(10))\nmappedData.printSchema()\n# mappedData.show()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\n# tf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/119:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import Row\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\n\ndata = dataset.select(\"BODY\")\n\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: Row(body=remove_punctuation(normalize_uk_words(s.BODY)))).toDF()\n#print(mappedData.take(10))\n#mappedData.printSchema()\n# mappedData.show()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/120:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import Row\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\n\ndata = dataset.select(\"BODY\")\n\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: Row(body=remove_punctuation(normalize_uk_words(s.BODY))))\ndata_df = sqlContext.createDataFrame(mappedData, ['BODY'])\n#print(mappedData.take(10))\n#mappedData.printSchema()\n# mappedData.show()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/121:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    #print(data_rdd.take(10))\n    #data_rdd.printSchema()\n    #data_rdd_df = data_rdd.toDF()\n #   print(data_rdd_df)\n    \n#     data_rdd_df.printSchema()\n    hashing_tf = HashingTF(inputCol = 'BODY', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['BODY', 'tf_features', 'features'])\n92/122:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import Row\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\n\ndata = dataset.select(\"BODY\")\n\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: Row(body=remove_punctuation(normalize_uk_words(s.BODY))))\ndata_df = sqlContext.createDataFrame(mappedData, ['BODY'])\n#print(mappedData.take(10))\n#mappedData.printSchema()\n# mappedData.show()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(data_df)\n\n# print(tf_idf_data)\n92/123: tf_idf_data.write.csv('mycsv.csv')\n92/124:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import Row\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\n\ndata = dataset.select(\"BODY\")\n\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: Row(body=remove_punctuation(normalize_uk_words(s.BODY))))\ndata_df = sqlContext.createDataFrame(mappedData, ['BODY'])\n#print(mappedData.take(10))\n#mappedData.printSchema()\n# mappedData.show()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(data_df)\n\nprint(tf_idf_data)\n92/125:\ntf_idf_data.show()\n#tf_idf_data.write.csv('mycsv.csv')\n96/1:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.sql import SQLContext\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Lab5\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n96/2:\n# preprocessing\n\nimport string\n# Remove punctuations from the sentence\ndef remove_punctuation(sentence):\n    punctuations = list(string.punctuation)\n    extra_punctuations = ['.', '``', '...', '\\'s', '--', '-', 'n\\'t', '_', '–']\n    punctuations += extra_punctuations\n    filtered = [w for w in sentence.lower() if w not in punctuations]\n    return (\"\".join(filtered)).split()\n\n\nimport pymorphy2\nmorph = pymorphy2.MorphAnalyzer(lang = 'uk')\n\ndef normalize_uk_words(sentence):\n    words = sentence.split()\n    normalized_words = list(map(lambda word: morph.parse(word)[0].normal_form, words))\n    return ' '.join(normalized_words)\n\nprint(remove_punctuation(normalize_uk_words('задіювалися, автоматизовані - пошукові роботи')))\n96/3:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    #print(data_rdd.take(10))\n    #data_rdd.printSchema()\n    #data_rdd_df = data_rdd.toDF()\n #   print(data_rdd_df)\n    \n#     data_rdd_df.printSchema()\n    hashing_tf = HashingTF(inputCol = 'BODY', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['BODY', 'tf_features', 'features'])\n96/4:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import Row\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\n\ndata = dataset.select(\"BODY\")\n\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: Row(body=remove_punctuation(normalize_uk_words(s.BODY))))\ndata_df = sqlContext.createDataFrame(mappedData, ['BODY'])\n#print(mappedData.take(10))\n#mappedData.printSchema()\n# mappedData.show()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(data_df)\n\nprint(tf_idf_data)\n96/5:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.sql import SQLContext\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Lab5\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n96/6:\n# preprocessing\n\nimport string\n# Remove punctuations from the sentence\ndef remove_punctuation(sentence):\n    punctuations = list(string.punctuation)\n    extra_punctuations = ['.', '``', '...', '\\'s', '--', '-', 'n\\'t', '_', '–']\n    punctuations += extra_punctuations\n    filtered = [w for w in sentence.lower() if w not in punctuations]\n    return (\"\".join(filtered)).split()\n\n\nimport pymorphy2\nmorph = pymorphy2.MorphAnalyzer(lang = 'uk')\n\ndef normalize_uk_words(sentence):\n    words = sentence.split()\n    normalized_words = list(map(lambda word: morph.parse(word)[0].normal_form, words))\n    return ' '.join(normalized_words)\n\nprint(remove_punctuation(normalize_uk_words('задіювалися, автоматизовані - пошукові роботи')))\n96/7:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    #print(data_rdd.take(10))\n    #data_rdd.printSchema()\n    #data_rdd_df = data_rdd.toDF()\n #   print(data_rdd_df)\n    \n#     data_rdd_df.printSchema()\n    hashing_tf = HashingTF(inputCol = 'BODY', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['BODY', 'tf_features', 'features'])\n96/8:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import Row\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\n\ndata = dataset.select(\"BODY\")\n\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: Row(body=remove_punctuation(normalize_uk_words(s.BODY))))\ndata_df = sqlContext.createDataFrame(mappedData, ['BODY'])\n#print(mappedData.take(10))\n#mappedData.printSchema()\n# mappedData.show()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(data_df)\n\nprint(tf_idf_data)\n96/9:\nprint(tf_idf_data[0])\n#tf_idf_data.write.csv('mycsv.csv')\n96/10:\nprint(tf_idf_data\n#tf_idf_data.write.csv('mycsv.csv')\n96/11:\nprint(tf_idf_data)\n#tf_idf_data.write.csv('mycsv.csv')\n96/12:\nprint(tf_idf_data.limit(2))\n#tf_idf_data.write.csv('mycsv.csv')\n96/13:\nprint(tf_idf_data.limit(2).toPandas())\n#tf_idf_data.write.csv('mycsv.csv')\n96/14:\nprint(tf_idf_data.limit(1).toPandas())\n#tf_idf_data.write.csv('mycsv.csv')\n96/15:\ntf_idf_data.limit(1).toPandas().to_csv(\"results.csv\")\n#tf_idf_data.write.csv('mycsv.csv')\n97/1:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.sql import SQLContext\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Lab5\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n97/2:\n# preprocessing\n\nimport string\n# Remove punctuations from the sentence\ndef remove_punctuation(sentence):\n    punctuations = list(string.punctuation)\n    extra_punctuations = ['.', '``', '...', '\\'s', '--', '-', 'n\\'t', '_', '–']\n    punctuations += extra_punctuations\n    filtered = [w for w in sentence.lower() if w not in punctuations]\n    return (\"\".join(filtered)).split()\n\n\nimport pymorphy2\nmorph = pymorphy2.MorphAnalyzer(lang = 'uk')\n\ndef normalize_uk_words(sentence):\n    words = sentence.split()\n    normalized_words = list(map(lambda word: morph.parse(word)[0].normal_form, words))\n    return ' '.join(normalized_words)\n\nprint(remove_punctuation(normalize_uk_words('задіювалися, автоматизовані - пошукові роботи')))\n97/3:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    #print(data_rdd.take(10))\n    #data_rdd.printSchema()\n    #data_rdd_df = data_rdd.toDF()\n #   print(data_rdd_df)\n    \n#     data_rdd_df.printSchema()\n    hashing_tf = HashingTF(inputCol = 'BODY', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['BODY', 'tf_features', 'features'])\n97/4:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import Row\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\n\ndata = dataset.select(\"BODY\")\n\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: Row(body=remove_punctuation(normalize_uk_words(s.BODY))))\ndata_df = sqlContext.createDataFrame(mappedData, ['BODY'])\n#print(mappedData.take(10))\n#mappedData.printSchema()\n# mappedData.show()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(data_df)\n\nprint(tf_idf_data)\n97/5:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.sql import SQLContext\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Lab5\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n97/6:\n# preprocessing\n\nimport string\n# Remove punctuations from the sentence\ndef remove_punctuation(sentence):\n    punctuations = list(string.punctuation)\n    extra_punctuations = ['.', '``', '...', '\\'s', '--', '-', 'n\\'t', '_', '–']\n    punctuations += extra_punctuations\n    filtered = [w for w in sentence.lower() if w not in punctuations]\n    return (\"\".join(filtered)).split()\n\n\nimport pymorphy2\nmorph = pymorphy2.MorphAnalyzer(lang = 'uk')\n\ndef normalize_uk_words(sentence):\n    words = sentence.split()\n    normalized_words = list(map(lambda word: morph.parse(word)[0].normal_form, words))\n    return ' '.join(normalized_words)\n\nprint(remove_punctuation(normalize_uk_words('задіювалися, автоматизовані - пошукові роботи')))\n97/7:\nfrom pyspark.ml.feature import HashingTF, IDF\n\n# Calculate term frequency–inverse document frequency for reflecting importance of words in Tweet.\n# :param data_rdd: input data rdd\n# :return: transformed dataframe\n\ndef tf_idf(data_rdd):\n    #print(data_rdd.take(10))\n    #data_rdd.printSchema()\n    #data_rdd_df = data_rdd.toDF()\n #   print(data_rdd_df)\n    \n#     data_rdd_df.printSchema()\n    hashing_tf = HashingTF(inputCol = 'BODY', outputCol = 'tf_features')\n    tf_data = hashing_tf.transform(data_rdd)\n\n    idf_data = IDF(inputCol = 'tf_features', outputCol = 'features').fit(tf_data)\n    tf_idf_data = idf_data.transform(tf_data)\n    return tf_idf_data.select(['BODY', 'tf_features', 'features'])\n97/8:\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import Row\nsqlContext = SQLContext(spark)\n\nschema = StructType([\n    StructField(\"ID\", StringType(), False),\n    StructField(\"TITLE\", StringType(), False),\n    StructField(\"BODY\", StringType(), False),\n])\n\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./1.csv\")   # Returns dataframe\n\ndata = dataset.select(\"BODY\")\n\n\n#data = dataset.select('isPositive', 'text')\nfilteredData = data.rdd.filter(lambda s: s.BODY)\nmappedData = filteredData.map(lambda s: Row(body=remove_punctuation(normalize_uk_words(s.BODY))))\ndata_df = sqlContext.createDataFrame(mappedData, ['BODY'])\n#print(mappedData.take(10))\n#mappedData.printSchema()\n# mappedData.show()\n\n# #data.printSchema()\n# #normalizedData = data.toPandas().apply(lambda s: print(s) remove_punctuation(normalize_uk_words(s[\"Body\"])))\ntf_idf_data = tf_idf(data_df)\n\nprint(tf_idf_data)\n97/9:\ntf_idf_data.limit(1).toPandas().to_csv(\"first_row_results.csv\")\ntf_idf_data.toPandas().to_csv(\"all_rows_results.csv\")\n#tf_idf_data.write.csv('mycsv.csv')\n100/1:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nfrom distutils.version import LooseVersion\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType, DateType, IntegerType\n100/2:\nschema = StructType([\n    StructField(\"DAY_OF_MONTH\", IntegerType(), False),\n    StructField(\"DAY_OF_WEEK\", IntegerType(), False),\n    StructField(\"OP_UNIQUE_CARRIER\", StringType(), False),\n    StructField(\"OP_CARRIER_AIRLINE_ID\", StringType(), False),\n    StructField(\"OP_CARRIER\", StringType(), False),\n    StructField(\"TAIL_NUM\", StringType(), False),\n    StructField(\"OP_CARRIER_FL_NUM\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_ID\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"ORIGIN\", StringType(), False),\n    StructField(\"DEST_AIRPORT_ID\", StringType(), False),\n    StructField(\"DEST_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"DEST\", StringType(), False),\n    StructField(\"DEP_TIME\", FloatType(), False),\n    StructField(\"DEP_DEL15\", StringType(), False),\n    StructField(\"DEP_TIME_BLK\", StringType(), False),\n    StructField(\"ARR_TIME\", FloatType(), False),\n    StructField(\"ARR_DEL15\", StringType(), False),\n    StructField(\"CANCELLED\", StringType(), False),\n    StructField(\"DIVERTED\", StringType(), False),\n    StructField(\"DISTANCE\", FloatType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Jan_2019_ontime.csv\")\ncols = dataset.columns\n100/3:\nschema = StructType([\n    StructField(\"DAY_OF_MONTH\", IntegerType(), False),\n    StructField(\"DAY_OF_WEEK\", IntegerType(), False),\n    StructField(\"OP_UNIQUE_CARRIER\", StringType(), False),\n    StructField(\"OP_CARRIER_AIRLINE_ID\", StringType(), False),\n    StructField(\"OP_CARRIER\", StringType(), False),\n    StructField(\"TAIL_NUM\", StringType(), False),\n    StructField(\"OP_CARRIER_FL_NUM\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_ID\", StringType(), False),\n    StructField(\"ORIGIN_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"ORIGIN\", StringType(), False),\n    StructField(\"DEST_AIRPORT_ID\", StringType(), False),\n    StructField(\"DEST_AIRPORT_SEQ_ID\", StringType(), False),\n    StructField(\"DEST\", StringType(), False),\n    StructField(\"DEP_TIME\", FloatType(), False),\n    StructField(\"DEP_DEL15\", StringType(), False),\n    StructField(\"DEP_TIME_BLK\", StringType(), False),\n    StructField(\"ARR_TIME\", FloatType(), False),\n    StructField(\"ARR_DEL15\", StringType(), False),\n    StructField(\"CANCELLED\", StringType(), False),\n    StructField(\"DIVERTED\", StringType(), False),\n    StructField(\"DISTANCE\", FloatType(), False),\n])\n\ndataset = spark.read.format(\"csv\").schema(schema).load(\"./Jan_2019_ontime.csv\")\ncols = dataset.columns\n100/4:\n\n# Pie chart, where the slices will be ordered and plotted counter-clockwise:\nlabels = 'Frogs', 'Hogs', 'Dogs', 'Logs'\nsizes = [15, 30, 45, 10]\nexplode = (0, 0.1, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()\n100/5:\ndataset.groupBy(\"DAY_OF_WEEK\").count().show()\n# Pie chart, where the slices will be ordered and plotted counter-clockwise:\nlabels = 'Frogs', 'Hogs', 'Dogs', 'Logs'\nsizes = [15, 30, 45, 10]\nexplode = (0, 0.1, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()\n100/6:\nweekDayGB = dataset.groupBy(\"DAY_OF_WEEK\").count()\n# Pie chart, where the slices will be ordered and plotted counter-clockwise:\nlabels = weekDayGB[0]\nsizes = [15, 30, 45, 10]\nexplode = (0, 0.1, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()\n100/7:\nweekDayGB = dataset.groupBy(\"DAY_OF_WEEK\").count()\n# Pie chart, where the slices will be ordered and plotted counter-clockwise:\nweekDayGB.toPandas().plot.pie(y=\"DAY_OF_WEEK\")\nlabels = 'Frogs', 'Hogs', 'Dogs', 'Logs'\nsizes = [15, 30, 45, 10]\nexplode = (0, 0.1, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()\n100/8:\nweekDayGB = dataset.groupBy(\"DAY_OF_WEEK\").count()\n# Pie chart, where the slices will be ordered and plotted counter-clockwise:\nweekDayGB.toPandas().plot.pie(y=\"DAY_OF_WEEK\")\n100/9:\nweekDayGB = dataset.groupBy(\"DAY_OF_WEEK\").count()\n# Pie chart, where the slices will be ordered and plotted counter-clockwise:\nweekDayGB.toPandas().plot.pie(y=\"DAY_OF_WEEK\")\n100/10:\nweekDayGB = dataset.groupBy(\"DAY_OF_WEEK\").count()\n# Pie chart, where the slices will be ordered and plotted counter-clockwise:\nweekDayGB.toPandas().plot.pie(y=\"DAY_OF_WEEK\")\n100/11:\nweekDayGB = dataset.groupBy(\"DAY_OF_WEEK\").count()\nweekDayGB.toPandas().plot.pie(y=\"DAY_OF_MONTH\")\n100/12:\nweekDayGB = dataset.groupBy(\"DAY_OF_MONTH\").count()\nweekDayGB.toPandas().plot.pie(y=\"DAY_OF_MONTH\")\n100/13:\nweekDayGB = dataset.groupBy(\"DAY_OF_MONTH\").count()\nweekDayGB.toPandas().plot.pie(y=\"DAY_OF_MONTH\")\n100/14:\nweekDayGB = dataset.groupBy(\"DAY_OF_MONTH\").count()\nweekDayGB.show()\nweekDayGB.toPandas().plot.pie(y=\"DAY_OF_MONTH\")\n100/15:\nweekDayGB = dataset.groupBy(\"DAY_OF_MONTH\").count().orderBy(\"count\").head(10)\nweekDayGB.toPandas().plot.pie(y=\"DAY_OF_MONTH\")\n100/16:\nweekDayGB = dataset.groupBy(\"DAY_OF_MONTH\").count().orderBy(\"count\")\nweekDayGB.toPandas().plot.pie(y=\"DAY_OF_MONTH\")\n100/17:\nweekDayGB = dataset.groupBy(\"DAY_OF_MONTH\").count().orderBy(\"count\")\nweekDayGB.toPandas().plot.pie(y=\"DAY_OF_MONTH\", legent=False)\n100/18:\nweekDayGB = dataset.groupBy(\"DAY_OF_MONTH\").count().orderBy(\"count\")\nweekDayGB.toPandas().plot.pie(y=\"DAY_OF_MONTH\", legend=False)\n100/19:\nweekDayGB = dataset.groupBy(\"DAY_OF_WEEK\").count()\nweekDayGB.toPandas().plot.pie(y=\"DAY_OF_WEEK\", legend=False)\n100/20:\nweekDayGB = dataset.groupBy(\"DAY_OF_WEEK\").sum(\"CANCELLED\")\nweekDayGB.show()\n#weekDayGB.toPandas().plot.pie(y=\"DAY_OF_WEEK\", legend=False)\n100/21:\nweekDayGB = dataset.where('ARR_DEL15 = \"1.00\"').groupBy(\"DAY_OF_WEEK\").count()\nweekDayGB.show()\n#weekDayGB.toPandas().plot.pie(y=\"DAY_OF_WEEK\", legend=False)\n100/22:\nweekDayGB = dataset.groupBy(\"DAY_OF_WEEK\").count()\nweekDayGB.show()\nweekDayGB.toPandas().plot.pie(y=\"DAY_OF_WEEK\", legend=False)\n100/23:\nweekDayGB = dataset.where('ARR_DEL15 = \"1.00\"').groupBy(\"DAY_OF_WEEK\").count()\nweekDayGB.show()\nweekDayGB.toPandas().plot.pie(y=\"DAY_OF_WEEK\", legend=False)\n100/24:\nweekDayGB = dataset.where('ARR_DEL15 = \"1.00\"').groupBy(\"DAY_OF_WEEK\").count()\nweekDayGB.show()\nweekDayGB.toPandas().plot.pie(y=\"DAY_OF_WEEK\", legend=False)\n100/25:\nweekDayGB = dataset.where('ARR_DEL15 = \"1.00\"').orderBy(\"DISTANCE\")\nweekDayGB.show()\n100/26:\nweekDayGB = dataset.where('ARR_DEL15 = \"1.00\"').orderBy(\"DISTANCE\")\nweekDayGB.toPandas().show()\n100/27:\nweekDayGB = dataset.where('ARR_DEL15 = \"1.00\"').orderBy(\"DISTANCE\")\nweekDayGB.toPandas()\n100/28:\nweekDayGB = dataset.where('ARR_DEL15 = \"1.00\"').groupBy(\"DISTANCE\").count().orderBy(\"DISTANCE\")\nweekDayGB.toPandas()\n100/29:\nweekDayGB = dataset.where('ARR_DEL15 = \"1.00\"').groupBy(\"DISTANCE\").count().orderBy(\"DISTANCE\")\npandData = weekDayGB.toPandas()\nprint(pandData)\npandData.hist(\"DISTANCE\")\n100/30:\nweekDayGB = dataset.where('ARR_DEL15 = \"1.00\"').groupBy(\"DISTANCE\").count().orderBy(\"DISTANCE\")\npandData = weekDayGB.toPandas()\nprint(pandData)\npandData.hist(\"DISTANCE\")\n100/31:\ndat = dataset.toPandas().groupBy(['DISTANCE', 'ARR_DEL15']).agg({'cnt': 'count'})\ndat.groupBy(level=0).apply(lambda x: 100*x/float(x.count()))\n100/32:\ndat = dataset.toPandas().groupby(['DISTANCE', 'ARR_DEL15']).agg({'cnt': 'count'})\ndat.groupby(level=0).apply(lambda x: 100*x/float(x.count()))\n100/33:\ndat = dataset.toPandas().groupby(['DISTANCE', 'ARR_DEL15']).count()\ndat.groupby(level=0).apply(lambda x: 100*x/float(x.count()))\n100/34:\ndistAndDelGB = dataset.toPandas().groupby(['DISTANCE', 'ARR_DEL15']).count()\ndistGB = dataset.toPandas().groupby(['DISTANCE']).count()\ndistAndDelGB.div(distGB, level='DISTANCE') * 100\n100/35:\ndistAndDelGB = dataset.toPandas().groupby(['DISTANCE', 'ARR_DEL15']).agg({'ARR_TIME', 'count'})\ndistGB = dataset.toPandas().groupby(['DISTANCE']).agg({'ARR_TIME', 'count'})\ndistAndDelGB.div(distGB, level='DISTANCE') * 100\n100/36:\ndistAndDelGB = dataset.toPandas().groupby(['DISTANCE', 'ARR_DEL15']).agg({'DAY_OF_MONTH', 'count'})\ndistGB = dataset.toPandas().groupby(['DISTANCE']).agg({'DAY_OF_MONTH', 'count'})\ndistAndDelGB.div(distGB, level='DISTANCE') * 100\n100/37:\ndistAndDelGB = dataset.toPandas().groupby(['DISTANCE', 'ARR_DEL15']).count()\ndistGB = dataset.toPandas().groupby(['DISTANCE']).count()\ndistAndDelGB.div(distGB, level='DISTANCE') * 100\n100/38:\ndistAndDelGB = dataset.toPandas().groupby(['DISTANCE', 'ARR_DEL15']).count()\ndistGB = dataset.toPandas().groupby(['DISTANCE']).count()\ndistAndDelGB.div(distGB, level='count') * 100\n100/39:\nweekDayGB = dataset.where('ARR_DEL15 = \"1.00\"').groupBy(\"ORIGIN\").count().orderBy(\"count\")\npandData = weekDayGB.toPandas()\nprint(pandData)\npandData.hist(\"ORIGIN\")\n100/40:\nweekDayGB = dataset.where('ARR_DEL15 = \"1.00\"').groupBy(\"ORIGIN\").count().orderBy(\"count\")\npandData = weekDayGB.toPandas()\nprint(pandData)\npandData.hist(\"count\")\n100/41:\nweekDayGB = dataset.groupBy(\"ORIGIN\", \"ARR_DEL15\").count().orderBy(\"count\")\npandData = weekDayGB.toPandas()\nprint(pandData)\npandData.hist(\"count\")\n100/42:\nweekDayGB = dataset.where('ARR_DEL15 = \"1.00\"').groupBy(\"ORIGIN\").count().orderBy(\"count\")\npandData1 = weekDayGB.toPandas()\nprint(pandData1)\nweekDayGB = dataset.where('ARR_DEL15 = \"0.00\"').groupBy(\"ORIGIN\").count().orderBy(\"count\")\npandData2 = weekDayGB.toPandas()\nprint(pandData2)\n\npandData1(\"count_without_del\") = pandData2(\"count\")\npandData.plot.hist(alpha=0.5)\n100/43:\nweekDayGB = dataset.where('ARR_DEL15 = \"1.00\"').groupBy(\"ORIGIN\").count().orderBy(\"count\")\npandData1 = weekDayGB.toPandas()\nprint(pandData1)\nweekDayGB = dataset.where('ARR_DEL15 = \"0.00\"').groupBy(\"ORIGIN\").count().orderBy(\"count\")\npandData2 = weekDayGB.toPandas()\nprint(pandData2)\n\npandData1[\"count_without_del\"] = pandData2[\"count\"]\npandData.plot.hist(alpha=0.5)\n100/44:\nweekDayGB = dataset.where('ARR_DEL15 = \"1.00\"').groupBy(\"ORIGIN\").count().orderBy(\"count\")\npandData1 = weekDayGB.toPandas()\nprint(pandData1)\nweekDayGB = dataset.where('ARR_DEL15 = \"0.00\"').groupBy(\"ORIGIN\").count().orderBy(\"count\")\npandData2 = weekDayGB.toPandas()\nprint(pandData2)\n\npandData1[\"count_without_del\"] = pandData2[\"count\"]\npandData1.plot.hist(alpha=0.5)\n100/45:\nweekDayGB = dataset.where('ARR_DEL15 = \"1.00\"').groupBy(\"ORIGIN\").count().orderBy(\"count\")\npandData1 = weekDayGB.toPandas()\nprint(pandData1)\nweekDayGB = dataset.where('ARR_DEL15 = \"0.00\"').groupBy(\"ORIGIN\").count().orderBy(\"count\")\npandData2 = weekDayGB.toPandas()\nprint(pandData2)\n\npandData1[\"count_without_del\"] = pandData2[\"count\"]\npandData1.plot.hist(by=[\"count_without_del\", \"count\"],alpha=0.5)\n100/46:\nweekDayGB = dataset.where('ARR_DEL15 = \"1.00\"').groupBy(\"ORIGIN\").count().orderBy(\"count\")\npandData1 = weekDayGB.toPandas()\n100/47:\nweekDayGB = dataset.where('ARR_DEL15 = \"0.00\"').groupBy(\"ORIGIN\").count().orderBy(\"count\")\npandData2 = weekDayGB.toPandas()\n100/48:\nweekDayGB = dataset.where('ARR_DEL15 = \"1.00\"').groupBy(\"ORIGIN\").count().orderBy(\"count\")\nweekDayGB.toPandas()\n100/49:\nweekDayGB = dataset.where('ARR_DEL15 = \"0.00\"').groupBy(\"ORIGIN\").count().orderBy(\"count\")\nweekDayGB.toPandas()\n100/50:\nweekDayGB = dataset.where('ARR_DEL15 = \"1.00\"').groupBy(\"DEST\").count().orderBy(\"count\")\nweekDayGB.toPandas()\n100/51:\nweekDayGB = dataset.where('ARR_DEL15 = \"0.00\"').groupBy(\"DEST\").count().orderBy(\"count\")\nweekDayGB.toPandas()\n100/52:\nmonthDayGB = dataset.groupBy(\"DAY_OF_MONTH\").count().orderBy(\"count\")\nmonthDayGB.show()\nmonthDayGB.toPandas().plot.pie(y=\"DAY_OF_MONTH\", legend=False)\n100/53:\ncategoricalColumns = [\n    \"ORIGIN_AIRPORT_ID\",\n    \"DEST_AIRPORT_ID\",\n    \"ORIGIN\",\n    \"DEP_TIME_BLK\",\n    \"DEP_DEL15\",\n    \"CANCELLED\",\n    \"DIVERTED\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    stringIndexer.setHandleInvalid(\"skip\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n        from pyspark.ml.feature import OneHotEncoderEstimator\n        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    else:\n        from pyspark.ml.feature import OneHotEncoder\n        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n100/54:\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"ARR_DEL15\", outputCol=\"label\")\nlabel_stringIdx.setHandleInvalid(\"skip\")\nstages += [label_stringIdx]\n100/55:\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"DAY_OF_MONTH\",\n    \"DAY_OF_WEEK\",\n    \"DEP_TIME\",\n    \"ARR_TIME\",\n    \"DISTANCE\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nassembler.setParams(handleInvalid=\"skip\")\nstages += [assembler]\n100/56:\nfrom pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n100/57:\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n#print(predictions.limit(10).toPandas())\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n100/58:\n    from pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3])\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=stages + [rf])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\n#predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\n100/59:\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n#print(predictions.limit(10).toPandas())\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n101/1:\n# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + cols\ndataset = preppedDataDF.select(selectedcols)\ndisplay(dataset.limit(10).toPandas())\n102/1:\nimport cv2\n\ncv2.imread(\"add.png\")\n102/2:\nimport cv2\n\ncv2.imread(\"add1.png\")\n102/3:\nimport cv2\n\ncv2.imread(\"add1.png\")\n102/4:\nimport cv2\n\ncv2.imread(\"add.png\")\n102/5:\nimport cv2\nimport tf\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n102/6:\nimport cv2\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n102/7:\nimg_np = cv2.imread(\"add.png\")\nimg_tf = tf.convert_to_tensor(img_np)\n\nimg_tf = tf.constant(...)\nimg_np = img_tf.numpy()\n102/8:\nimg_np = cv2.imread(\"add.png\")\nimg_tf = tf.convert_to_tensor(img_np)\n\nplt.imshow(img_tf.numpy())\n102/9:\nimg_np = cv2.imread(\"add.png\")\nimg_tf = tf.convert_to_tensor(img_np)\n\nplt.imshow(img_tf.constant().numpy())\n102/10:\nimg_np = cv2.imread(\"add.png\")\nimg_tf = tf.convert_to_tensor(img_np)\n\nplt.imshow(img_tf.numpy())\n102/11:\nimg_np = cv2.imread(\"add.png\")\nimg_tf = tf.convert_to_tensor(img_np)\n\nplt.imshow(img_tf.numpy())\n102/12: hsv_img = tf.image.rgb_to_hsv(img_tf)\n102/13:\nhsv_img = tf.image.rgb_to_hsv(img_tf)\nplt.imshow(hsv_img.numpy())\n102/14:\nimg_np = cv2.imread(\"add.png\")\nimg_tf = tf.convert_to_tensor(img_np)\n\nplt.imshow(img_tf.numpy())\n102/15:\nhsv_img = tf.image.rgb_to_hsv(img_tf)\nplt.imshow(hsv_img.numpy())\n102/16:\nhsv_img = tf.image.rgb_to_hsv(img_tf.double())\nplt.imshow(hsv_img.numpy())\n102/17:\nhsv_img = tf.image.rgb_to_hsv(img_tf.type(torch.DoubleTensor))\nplt.imshow(hsv_img.numpy())\n102/18:\nimg_tf = tf.cast(img_tf, dtype=tf.float32)\nhsv_img = tf.image.rgb_to_hsv(img_tf)\nplt.imshow(hsv_img.numpy())\n102/19:\nimg_tf = tf.cast(img_tf, dtype=tf.float32)\nhsv_img = tf.image.rgb_to_hsv(img_tf)\n#plt.imshow(hsv_img.numpy())\n102/20:\nimg_tf = tf.cast(img_tf, dtype=tf.float32)\nhsv_img = tf.image.rgb_to_hsv(img_tf)\nplt.imshow(hsv_img.numpy())\n102/21:\nhsv_img = tfa.image.gaussian_filter2d(hsv_img)\nplt.imshow(hsv_img.numpy())\n102/22:\nhsv_img = tf.image.gaussian_filter2d(hsv_img)\nplt.imshow(hsv_img.numpy())\n102/23:\nhsv_img = tf.tfa.image.gaussian_filter2d(hsv_img)\nplt.imshow(hsv_img.numpy())\n102/24:\nimport cv2\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n102/25:\nhsv_img = tfa.image.gaussian_filter2d(hsv_img)\nplt.imshow(hsv_img.numpy())\n102/26:\nhsv_img = tfa.image.median_filter2d(hsv_img)\nplt.imshow(hsv_img.numpy())\n102/27:\nimg_np = cv2.imread(\"bg3.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\n\nplt.imshow(img_tf.numpy())\n102/28:\nimg_np = cv2.imread(\"bg3.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\ntmg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf.numpy())\n102/29:\nimg_np = cv2.imread(\"bg3.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\ntmg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n102/30:\nimg_np = cv2.imread(\"bg3.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\ntmg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n102/31:\nimg_np = cv2.imread(\"bg3.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\ntmg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\nimg_tf = tf.cast(img_tf, dtype=tf.float32) \nimg_tf = tf.image.rgb_to_hsv(img_tf)\n\nplt.imshow(img_tf)\n102/32:\nimg_np = cv2.imread(\"bg3.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\ntmg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\nimg_tf = tf.cast(img_tf, dtype=tf.float32) / 255 \nimg_tf = tf.image.rgb_to_hsv(img_tf)\n\nplt.imshow(img_tf)\n102/33:\nimg_np = cv2.imread(\"bg3.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\ntmg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\nimg_tf = tf.dtypes.cast(img_tf, dtype=tf.float32) / 255 \nimg_tf = tf.image.rgb_to_hsv(img_tf)\n\nplt.imshow(img_tf)\n102/34:\nimport cv2\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n102/35:\nimg_np = cv2.imread(\"bg3.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\ntmg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\nimg_tf = tf.cast(img_tf, dtype=tf.float32) / 255 \nimg_tf = tf.image.rgb_to_hsv(img_tf)\n\nshow_img(img_tf)\n102/36:\ndef show_img(tf_img):\n    tf.cast(tf.image.hsv_to_rgv(tf_img) * 255, tf.int32)\n    plt.imshow(tf_img)\n102/37:\nimg_np = cv2.imread(\"bg3.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\ntmg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\nimg_tf = tf.cast(img_tf, dtype=tf.float32) / 255 \nimg_tf = tf.image.rgb_to_hsv(img_tf)\n\nshow_img(img_tf)\n102/38:\ndef show_img(tf_img):\n    tf.cast(tf.image.hsv_to_rgb(tf_img) * 255, tf.int32)\n    plt.imshow(tf_img)\n102/39:\nimg_np = cv2.imread(\"bg3.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\ntmg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\nimg_tf = tf.cast(img_tf, dtype=tf.float32) / 255 \nimg_tf = tf.image.rgb_to_hsv(img_tf)\n\nshow_img(img_tf)\n102/40:\nimg_np = cv2.imread(\"bg3.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\ntmg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\nimg_hsv = tf.cast(img_hsv, dtype=tf.float32) / 255 \nimg_hsv = tf.image.rgb_to_hsv(img_hsv)\n\nshow_img(img_hsv)\n102/41:\nimg_np = cv2.imread(\"bg3.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\ntmg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\nimg_hsv = tf.cast(tmg_tf, dtype=tf.float32) / 255 \nimg_hsv = tf.image.rgb_to_hsv(img_hsv)\n\nshow_img(img_hsv)\n102/42:\nimg_np = cv2.imread(\"bg3.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\ntmg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\nimg_hsv = tf.cast(tmg_tf, dtype=tf.float32) / 255 \nimg_hsv = tf.image.rgb_to_hsv(img_hsv)\n\n\ntf.cast(tf.image.hsv_to_rgb(tf_img), tf.int32)\nplt.imshow(img_tf)\nshow_img(img_hsv)\n102/43:\nimg_np = cv2.imread(\"bg3.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\ntmg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\nimg_hsv = tf.cast(tmg_tf, dtype=tf.float32) / 255 \nimg_hsv = tf.image.rgb_to_hsv(img_hsv)\n\n\nplt.imshow(img_tf)\n#show_img(img_hsv)\n102/44:\nimg_np = cv2.imread(\"bg3.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\ntmg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,:1]], axis=-1)\nimg_hsv = tf.cast(tmg_tf, dtype=tf.float32) / 255 \nimg_hsv = tf.image.rgb_to_hsv(img_hsv)\n\n\nplt.imshow(img_tf)\n#show_img(img_hsv)\n102/45:\nimg_np = cv2.imread(\"bg3.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\n#tmg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,:1]], axis=-1)\nimg_hsv = tf.cast(tmg_tf, dtype=tf.float32) / 255 \nimg_hsv = tf.image.rgb_to_hsv(img_hsv)\n\n\nplt.imshow(img_tf)\n#show_img(img_hsv)\n102/46:\nimg_np = cv2.imread(\"bg3.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]])\nimg_hsv = tf.cast(tmg_tf, dtype=tf.float32) / 255 \nimg_hsv = tf.image.rgb_to_hsv(img_hsv)\n\n\nplt.imshow(img_tf)\n#show_img(img_hsv)\n102/47:\nimg_np = cv2.imread(\"bg3.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=0)\nimg_hsv = tf.cast(tmg_tf, dtype=tf.float32) / 255 \nimg_hsv = tf.image.rgb_to_hsv(img_hsv)\n\n\nplt.imshow(img_tf)\n#show_img(img_hsv)\n102/48:\nimg_np = cv2.imread(\"bg3.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\nimg_hsv = tf.cast(tmg_tf, dtype=tf.float32) / 255 \nimg_hsv = tf.image.rgb_to_hsv(img_hsv)\n\n\nplt.imshow(img_tf)\n#show_img(img_hsv)\n102/49:\n# HSV + normalization\n\ndef normalize(color):\n    max_color = tf.reduce_max(color)\n    min_color = tf.reduce_min(color)\n    return (color - min_color) / (max_color - min_color)\n\n\nimg_tf_hsv = tf.cast(tmg_tf, dtype=tf.float32) / 255 \nimg_tf_hsv = tf.image.rgb_to_hsv(img_hsv)\nimg_tf_hsv = tf.concat([img_tf_hsv[:,:,:-1], normalize(img_hsv[:,:,-1:])], axis = -1)\n\nimg_tf = tf.cast(img_tf, dtype=tf.float32) \nimg_tf_rgb = tf.image.hsv_to_rgb(img_tf_hsv * 255)\nimg_tf_rgb = tf.cast(img_tf_rgb, tf.int32)\n\nplt.imshow(img_tf_rgb)\n102/50:\nimg_np = cv2.imread(\"bg3.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n102/51:\n# HSV + normalization\n\ndef normalize(color):\n    max_color = tf.reduce_max(color)\n    min_color = tf.reduce_min(color)\n    return (color - min_color) / (max_color - min_color)\n\n\nimg_tf_hsv = tf.cast(tmg_tf, dtype=tf.float32) / 255 \nimg_tf_hsv = tf.image.rgb_to_hsv(img_hsv)\nimg_tf_hsv = tf.concat([img_tf_hsv[:,:,:-1], normalize(img_hsv[:,:,-1:])], axis = -1)\n\nimg_tf = tf.cast(img_tf, dtype=tf.float32) \nimg_tf_rgb = tf.image.hsv_to_rgb(img_tf_hsv * 255)\nimg_tf_rgb = tf.cast(img_tf_rgb, tf.int32)\n\nplt.imshow(img_tf_rgb)\n102/52:\n# HSV + normalization\n\ndef normalize(color):\n    max_color = tf.reduce_max(color)\n    min_color = tf.reduce_min(color)\n    return (color - min_color) / (max_color - min_color)\n\n\nimg_tf_hsv = tf.cast(tmg_tf, dtype=tf.float32) / 255 \nimg_tf_hsv = tf.image.rgb_to_hsv(img_hsv)\nimg_tf_hsv = tf.concat([img_tf_hsv[:,:,:-1], normalize(img_hsv[:,:,-1:])], axis = -1)\n\nimg_tf = tf.cast(img_tf, dtype=tf.float32) \nimg_tf_rgb = tf.image.hsv_to_rgb(img_tf_hsv) * 255\nimg_tf_rgb = tf.cast(img_tf_rgb, tf.int32)\n\nplt.imshow(img_tf_rgb)\n102/53:\nimg_np = cv2.imread(\"dark_img.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n102/54:\n# HSV + normalization\n\ndef normalize(color):\n    max_color = tf.reduce_max(color)\n    min_color = tf.reduce_min(color)\n    return (color - min_color) / (max_color - min_color)\n\n\nimg_tf_hsv = tf.cast(tmg_tf, dtype=tf.float32) / 255 \nimg_tf_hsv = tf.image.rgb_to_hsv(img_hsv)\nimg_tf_hsv = tf.concat([img_tf_hsv[:,:,:-1], normalize(img_hsv[:,:,-1:])], axis = -1)\n\nimg_tf = tf.cast(img_tf, dtype=tf.float32) \nimg_tf_rgb = tf.image.hsv_to_rgb(img_tf_hsv) * 255\nimg_tf_rgb = tf.cast(img_tf_rgb, tf.int32)\n\nplt.imshow(img_tf_rgb)\n102/55:\n# HSV + normalization\n\ndef normalize(color):\n    max_color = tf.reduce_max(color)\n    min_color = tf.reduce_min(color)\n    return (color - min_color) / (max_color - min_color)\n\n\nimg_tf_hsv = tf.cast(tmg_tf, dtype=tf.float32) / 255 \nimg_tf_hsv = tf.image.rgb_to_hsv(img_hsv)\nimg_tf_hsv = tf.concat([img_tf_hsv[:,:,:-1], normalize(img_hsv[:,:,-1:])], axis = -1)\n\nimg_tf = tf.cast(img_tf, dtype=tf.float32) \nimg_tf_rgb = tf.image.hsv_to_rgb(img_tf_hsv) * 255\nimg_tf_rgb = tf.cast(img_tf_rgb, tf.int32)\n\nplt.imshow(img_tf_rgb)\n102/56:\nimg_np = cv2.imread(\"dark_img.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n102/57:\n# HSV + normalization\n\ndef normalize(color):\n    max_color = tf.reduce_max(color)\n    min_color = tf.reduce_min(color)\n    return (color - min_color) / (max_color - min_color)\n\n\nimg_tf_hsv = tf.cast(tmg_tf, dtype=tf.float32) / 255 \nimg_tf_hsv = tf.image.rgb_to_hsv(img_hsv)\nimg_tf_hsv = tf.concat([img_tf_hsv[:,:,:-1], normalize(img_hsv[:,:,-1:])], axis = -1)\n\nimg_tf = tf.cast(img_tf, dtype=tf.float32) \nimg_tf_rgb = tf.image.hsv_to_rgb(img_tf_hsv) * 255\nimg_tf_rgb = tf.cast(img_tf_rgb, tf.int32)\n\nplt.imshow(img_tf_rgb)\n102/58:\n# HSV + normalization\n\ndef normalize(color):\n    max_color = tf.reduce_max(color)\n    min_color = tf.reduce_min(color)\n    return (color - min_color) / (max_color - min_color)\n\n\nimg_tf_hsv = tf.cast(tmg_tf, dtype=tf.float32) / 255 \nimg_tf_hsv = tf.image.rgb_to_hsv(img_hsv)\nimg_tf_hsv = tf.concat([img_tf_hsv[:,:,:-1], normalize(img_hsv[:,:,-1:])], axis = -1)\n\nimg_tf_rgb = tf.cast(img_tf, dtype=tf.float32) \nimg_tf_rgb = tf.image.hsv_to_rgb(img_tf_hsv) * 255\nimg_tf_rgb = tf.cast(img_tf_rgb, tf.int32)\n\nplt.imshow(img_tf_rgb)\n102/59:\nimport cv2\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n102/60:\nimg_np = cv2.imread(\"dark_img.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n102/61:\n# HSV + normalization\n\ndef normalize(color):\n    max_color = tf.reduce_max(color)\n    min_color = tf.reduce_min(color)\n    return (color - min_color) / (max_color - min_color)\n\n\nimg_tf_hsv = tf.cast(tmg_tf, dtype=tf.float32) / 255 \nimg_tf_hsv = tf.image.rgb_to_hsv(img_hsv)\nimg_tf_hsv = tf.concat([img_tf_hsv[:,:,:-1], normalize(img_hsv[:,:,-1:])], axis = -1)\n\nimg_tf_rgb = tf.cast(img_tf, dtype=tf.float32) \nimg_tf_rgb = tf.image.hsv_to_rgb(img_tf_hsv) * 255\nimg_tf_rgb = tf.cast(img_tf_rgb, tf.int32)\n\nplt.imshow(img_tf_rgb)\n102/62:\n# HSV + normalization\n\ndef normalize(color):\n    max_color = tf.reduce_max(color)\n    min_color = tf.reduce_min(color)\n    return (color - min_color) / (max_color - min_color)\n\n\nimg_tf_hsv = tf.cast(tmg_tf, dtype=tf.float32) / 255 \nimg_tf_hsv = tf.image.rgb_to_hsv(img_hsv)\nimg_tf_hsv = tf.concat([img_tf_hsv[:,:,:-1], normalize(img_hsv[:,:,-1:])], axis = -1)\n\nimg_tf_rgb = tf.cast(img_tf_hsv, dtype=tf.float32) \nimg_tf_rgb = tf.image.hsv_to_rgb(img_tf_hsv) * 255\nimg_tf_rgb = tf.cast(img_tf_rgb, tf.int32)\n\nplt.imshow(img_tf_rgb)\n102/63:\n# HSV + normalization\n\ndef normalize(color):\n    max_color = tf.reduce_max(color)\n    min_color = tf.reduce_min(color)\n    return (color - min_color) / (max_color - min_color)\n\n\nimg_tf_hsv = tf.cast(tmg_tf, dtype=tf.float32) / 255 \nimg_tf_hsv = tf.image.rgb_to_hsv(img_tf_hsv)\nimg_tf_hsv = tf.concat([img_tf_hsv[:,:,:-1], normalize(img_hsv[:,:,-1:])], axis = -1)\n\nimg_tf_rgb = tf.cast(img_tf_hsv, dtype=tf.float32) \nimg_tf_rgb = tf.image.hsv_to_rgb(img_tf_hsv) * 255\nimg_tf_rgb = tf.cast(img_tf_rgb, tf.int32)\n\nplt.imshow(img_tf_rgb)\n102/64:\n# HSV + normalization\n\ndef normalize(color):\n    max_color = tf.reduce_max(color)\n    min_color = tf.reduce_min(color)\n    return (color - min_color) / (max_color - min_color)\n\n\nimg_tf_hsv = tf.cast(tmg_tf, dtype=tf.float32) / 255 \nimg_tf_hsv = tf.image.rgb_to_hsv(img_tf_hsv)\nimg_tf_hsv = tf.concat([img_tf_hsv[:,:,:-1], normalize(img_tf_hsv[:,:,-1:])], axis = -1)\n\nimg_tf_rgb = tf.cast(img_tf_hsv, dtype=tf.float32) \nimg_tf_rgb = tf.image.hsv_to_rgb(img_tf_rgb) * 255\nimg_tf_rgb = tf.cast(img_tf_rgb, tf.int32)\n\nplt.imshow(img_tf_rgb)\n102/65:\nimport cv2\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n102/66:\nimg_np = cv2.imread(\"dark_img.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n102/67:\n# HSV + normalization\n\ndef normalize(color):\n    max_color = tf.reduce_max(color)\n    min_color = tf.reduce_min(color)\n    return (color - min_color) / (max_color - min_color)\n\n\nimg_tf_hsv = tf.cast(tmg_tf, dtype=tf.float32) / 255 \nimg_tf_hsv = tf.image.rgb_to_hsv(img_tf_hsv)\nimg_tf_hsv = tf.concat([img_tf_hsv[:,:,:-1], normalize(img_tf_hsv[:,:,-1:])], axis = -1)\n\nimg_tf_rgb = tf.cast(img_tf_hsv, dtype=tf.float32) \nimg_tf_rgb = tf.image.hsv_to_rgb(img_tf_rgb) * 255\nimg_tf_rgb = tf.cast(img_tf_rgb, tf.int32)\n\nplt.imshow(img_tf_rgb)\n103/1:\nimport cv2\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n103/2:\nimg_np = cv2.imread(\"dark_img.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n103/3:\n# HSV + normalization\n\ndef normalize(color):\n    max_color = tf.reduce_max(color)\n    min_color = tf.reduce_min(color)\n    return (color - min_color) / (max_color - min_color)\n\n\nimg_tf_hsv = tf.cast(tmg_tf, dtype=tf.float32) / 255 \nimg_tf_hsv = tf.image.rgb_to_hsv(img_tf_hsv)\nimg_tf_hsv = tf.concat([img_tf_hsv[:,:,:-1], normalize(img_tf_hsv[:,:,-1:])], axis = -1)\n\nimg_tf_rgb = tf.cast(img_tf_hsv, dtype=tf.float32) \nimg_tf_rgb = tf.image.hsv_to_rgb(img_tf_rgb) * 255\nimg_tf_rgb = tf.cast(img_tf_rgb, tf.int32)\n\nplt.imshow(img_tf_rgb)\n103/4:\n# HSV + normalization\n\ndef normalize(color):\n    max_color = tf.reduce_max(color)\n    min_color = tf.reduce_min(color)\n    return (color - min_color) / (max_color - min_color)\n\n\nimg_tf_hsv = tf.cast(img_tf, dtype=tf.float32) / 255 \nimg_tf_hsv = tf.image.rgb_to_hsv(img_tf_hsv)\nimg_tf_hsv = tf.concat([img_tf_hsv[:,:,:-1], normalize(img_tf_hsv[:,:,-1:])], axis = -1)\n\nimg_tf_rgb = tf.cast(img_tf_hsv, dtype=tf.float32) \nimg_tf_rgb = tf.image.hsv_to_rgb(img_tf_rgb) * 255\nimg_tf_rgb = tf.cast(img_tf_rgb, tf.int32)\n\nplt.imshow(img_tf_rgb)\n103/5:\nimg_np = cv2.imread(\"dark_img.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n103/6:\n# HSV + normalization\n\ndef normalize(color):\n    max_color = tf.reduce_max(color)\n    min_color = tf.reduce_min(color)\n    return (color - min_color) / (max_color - min_color)\n\n\nimg_tf_hsv = tf.cast(img_tf, dtype=tf.float32) / 255 \nimg_tf_hsv = tf.image.rgb_to_hsv(img_tf_hsv)\nimg_tf_hsv = tf.concat([img_tf_hsv[:,:,:-1], normalize(img_tf_hsv[:,:,-1:])], axis = -1)\n\nimg_tf_rgb = tf.cast(img_tf_hsv, dtype=tf.float32) \nimg_tf_rgb = tf.image.hsv_to_rgb(img_tf_rgb) * 255\nimg_tf_rgb = tf.cast(img_tf_rgb, tf.int32)\n\nplt.imshow(img_tf_rgb)\n103/7:\nimport cv2\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n103/8:\nimg_np = cv2.imread(\"dark_img.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n103/9:\n# HSV + normalization\n\ndef normalize(color):\n    max_color = tf.reduce_max(color)\n    min_color = tf.reduce_min(color)\n    return (color - min_color) / (max_color - min_color)\n\n\nimg_tf_hsv = tf.cast(img_tf, dtype=tf.float32) / 255 \nimg_tf_hsv = tf.image.rgb_to_hsv(img_tf_hsv)\nimg_tf_hsv = tf.concat([img_tf_hsv[:,:,:-1], normalize(img_tf_hsv[:,:,-1:])], axis = -1)\n\nimg_tf_rgb = tf.cast(img_tf_hsv, dtype=tf.float32) \nimg_tf_rgb = tf.image.hsv_to_rgb(img_tf_rgb) * 255\nimg_tf_rgb = tf.cast(img_tf_rgb, tf.int32)\n\nplt.imshow(img_tf_rgb)\n103/10:\ndef convert(img, filter)\n    img = tf.cast(img, tf.float32)\n    # Add additional dimensions\n    filter = tf.reshape(filter, [filter.shape[0], filter.shape[1], 1, 1])\n    img = tf.reshape(img, [1, img.shape[0], img.shape[1], 1])\n    # Applying convolution\n    conv_res = tf.nn.convolution(img, filter, padding='SAME')\n    # Remove additional dimenstions\n    return tf.reshape(conv_res, [conv_res.shape[1], conv_res.shape[2], conv_res.shape[3]])\n\ngauss = tf.convert_to_tensor(\n    [[0, 0.01, 0.02, 9.01, 0.],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0.02, 0.1, 5.16, 0.1, 0.02],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0., 0.01, 0.02, 9.01, 0.]])\nimg_gauss = tf.concat( [convert (img_tf[:,:,0:1], gauss), convert (img_tf[:,:,1:2], gauss), convert(img_tf[:,:,2:3], gauss), axis=-1)\nimg_gauss = tf.cast(img_gauss, tf.int32)\nplt.imshow(img_gauss)\nhsv_img = tfa.image.gaussian_filter2d(hsv_img)\nplt.imshow(hsv_img.numpy())\n103/11:\ndef convert(img, filter)\n    img = tf.cast(img, tf.float32)\n    # Add additional dimensions\n    filter = tf.reshape(filter, [filter.shape[0], filter.shape[1], 1, 1])\n    img = tf.reshape(img, [1, img.shape[0], img.shape[1], 1])\n    # Applying convolution\n    conv_res = tf.nn.convolution(img, filter, padding='SAME')\n    # Remove additional dimenstions\n    return tf.reshape(conv_res, [conv_res.shape[1], conv_res.shape[2], conv_res.shape[3]])\n\ngauss = tf.convert_to_tensor(\n    [[0, 0.01, 0.02, 9.01, 0.],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0.02, 0.1, 5.16, 0.1, 0.02],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0., 0.01, 0.02, 9.01, 0.]])\nimg_gauss = tf.concat( [convert (img_tf[:,:,0:1], gauss), convert (img_tf[:,:,1:2], gauss), convert(img_tf[:,:,2:3], gauss)], axis=-1)\nimg_gauss = tf.cast(img_gauss, tf.int32)\nplt.imshow(img_gauss)\nhsv_img = tfa.image.gaussian_filter2d(hsv_img)\nplt.imshow(hsv_img.numpy())\n103/12:\ndef convert(img, filter):\n    img = tf.cast(img, tf.float32)\n    # Add additional dimensions\n    filter = tf.reshape(filter, [filter.shape[0], filter.shape[1], 1, 1])\n    img = tf.reshape(img, [1, img.shape[0], img.shape[1], 1])\n    # Applying convolution\n    conv_res = tf.nn.convolution(img, filter, padding='SAME')\n    # Remove additional dimenstions\n    return tf.reshape(conv_res, [conv_res.shape[1], conv_res.shape[2], conv_res.shape[3]])\n\ngauss = tf.convert_to_tensor(\n    [[0, 0.01, 0.02, 9.01, 0.],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0.02, 0.1, 5.16, 0.1, 0.02],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0., 0.01, 0.02, 9.01, 0.]])\nimg_gauss = tf.concat( [convert (img_tf[:,:,0:1], gauss), convert (img_tf[:,:,1:2], gauss), convert(img_tf[:,:,2:3], gauss)], axis=-1)\nimg_gauss = tf.cast(img_gauss, tf.int32)\nplt.imshow(img_gauss)\nhsv_img = tfa.image.gaussian_filter2d(hsv_img)\nplt.imshow(hsv_img.numpy())\n103/13:\ndef convert(img, filter):\n    img = tf.cast(img, tf.float32)\n    # Add additional dimensions\n    filter = tf.reshape(filter, [filter.shape[0], filter.shape[1], 1, 1])\n    img = tf.reshape(img, [1, img.shape[0], img.shape[1], 1])\n    # Applying convolution\n    conv_res = tf.nn.convolution(img, filter, padding='SAME')\n    # Remove additional dimenstions\n    return tf.reshape(conv_res, [conv_res.shape[1], conv_res.shape[2], conv_res.shape[3]])\n\ngauss = tf.convert_to_tensor(\n    [[0, 0.01, 0.02, 9.01, 0.],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0.02, 0.1, 5.16, 0.1, 0.02],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0., 0.01, 0.02, 9.01, 0.]])\nimg_gauss = tf.concat( [convert (img_tf[:,:,0:1], gauss), convert (img_tf[:,:,1:2], gauss), convert(img_tf[:,:,2:3], gauss)], axis=-1)\nimg_gauss = tf.cast(img_gauss, tf.int32)\nplt.imshow(img_gauss)\nplt.imshow(hsv_img.numpy())\n103/14:\ndef convert(img, filter):\n    img = tf.cast(img, tf.float32)\n    # Add additional dimensions\n    filter = tf.reshape(filter, [filter.shape[0], filter.shape[1], 1, 1])\n    img = tf.reshape(img, [1, img.shape[0], img.shape[1], 1])\n    # Applying convolution\n    conv_res = tf.nn.convolution(img, filter, padding='SAME')\n    # Remove additional dimenstions\n    return tf.reshape(conv_res, [conv_res.shape[1], conv_res.shape[2], conv_res.shape[3]])\n\ngauss = tf.convert_to_tensor(\n    [[0, 0.01, 0.02, 0.01, 0.],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0.02, 0.1, 0.16, 0.1, 0.02],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0., 0.01, 0.02, 0.01, 0.]])\nimg_gauss = tf.concat( [convert (img_tf[:,:,0:1], gauss), convert (img_tf[:,:,1:2], gauss), convert(img_tf[:,:,2:3], gauss)], axis=-1)\nimg_gauss = tf.cast(img_gauss, tf.int32)\nplt.imshow(img_gauss)\nplt.imshow(hsv_img.numpy())\n103/15:\ndef convert(img, filter):\n    img = tf.cast(img, tf.float32)\n    # Add additional dimensions\n    filter = tf.reshape(filter, [filter.shape[0], filter.shape[1], 1, 1])\n    img = tf.reshape(img, [1, img.shape[0], img.shape[1], 1])\n    # Applying convolution\n    conv_res = tf.nn.convolution(img, filter, padding='SAME')\n    # Remove additional dimenstions\n    return tf.reshape(conv_res, [conv_res.shape[1], conv_res.shape[2], conv_res.shape[3]])\n\ngauss = tf.convert_to_tensor(\n    [[0, 0.01, 0.02, 0.01, 0.],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0.02, 0.1, 0.16, 0.1, 0.02],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0., 0.01, 0.02, 0.01, 0.]])\nimg_gauss = tf.concat( [convert (img_tf[:,:,0:1], gauss), convert (img_tf[:,:,1:2], gauss), convert(img_tf[:,:,2:3], gauss)], axis=-1)\nimg_gauss = tf.cast(img_gauss, tf.int32)\nplt.imshow(img_gauss)\n103/16:\n# HSV + normalization\n\ndef normalize(color):\n    max_color = tf.reduce_max(color)\n    min_color = tf.reduce_min(color)\n    return (color - min_color) / (max_color - min_color)\n\n\nimg_tf_hsv = tf.cast(img_tf, dtype=tf.float32) / 255 \nimg_tf_hsv = tf.image.rgb_to_hsv(img_tf_hsv)\nimg_tf_hsv = tf.concat([img_tf_hsv[:,:,:-1], normalize(img_tf_hsv[:,:,-1:])], axis = -1)\n\nimg_tf_rgb = tf.cast(img_tf_hsv, dtype=tf.float32) \nimg_tf_rgb = tf.image.hsv_to_rgb(img_tf_rgb) * 255\nimg_tf_rgb = tf.cast(img_tf_rgb, tf.int32)\n\nplt.imshow(img_tf_rgb)\n103/17:\nimg_np = cv2.imread(\"bg3.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n103/18:\nimg_np = cv2.imread(\"space.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n103/19:\ndef convert(img, filter):\n    img = tf.cast(img, tf.float32)\n    # Add additional dimensions\n    filter = tf.reshape(filter, [filter.shape[0], filter.shape[1], 1, 1])\n    img = tf.reshape(img, [1, img.shape[0], img.shape[1], 1])\n    # Applying convolution\n    conv_res = tf.nn.convolution(img, filter, padding='SAME')\n    # Remove additional dimenstions\n    return tf.reshape(conv_res, [conv_res.shape[1], conv_res.shape[2], conv_res.shape[3]])\n\ngauss = tf.convert_to_tensor(\n    [[0, 0.01, 0.02, 0.01, 0.],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0.02, 0.1, 0.16, 0.1, 0.02],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0., 0.01, 0.02, 0.01, 0.]])\nimg_gauss = tf.concat( [convert (img_tf[:,:,0:1], gauss), convert (img_tf[:,:,1:2], gauss), convert(img_tf[:,:,2:3], gauss)], axis=-1)\nimg_gauss = tf.cast(img_gauss, tf.int32)\nplt.imshow(img_gauss)\n103/20:\nboxFilter = tf.fill([3, 3], 1/9)\nimg_box_filter = tf.concat( [convert (img_tf[:,:,0:1], boxFilter), convert (img_tf[:,:,1:2], boxFilter), convert(img_tf[:,:,2:3], boxFilter)], axis=-1)\nimg_box_filter = tf.cast(img_box_filter, tf.int32)\nplt.imshow(img_box_filter)\n103/21:\nimg_np = cv2.imread(\"planets.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n103/22:\ndef convert(img, filter):\n    img = tf.cast(img, tf.float32)\n    # Add additional dimensions\n    filter = tf.reshape(filter, [filter.shape[0], filter.shape[1], 1, 1])\n    img = tf.reshape(img, [1, img.shape[0], img.shape[1], 1])\n    # Applying convolution\n    conv_res = tf.nn.convolution(img, filter, padding='SAME')\n    # Remove additional dimenstions\n    return tf.reshape(conv_res, [conv_res.shape[1], conv_res.shape[2], conv_res.shape[3]])\n\ngaussFilter = tf.convert_to_tensor(\n    [[0, 0.01, 0.02, 0.01, 0.],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0.02, 0.1, 0.16, 0.1, 0.02],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0., 0.01, 0.02, 0.01, 0.]])\nimg_gauss = tf.concat( [convert (img_tf[:,:,0:1], gaussFilter), convert (img_tf[:,:,1:2], gaussFilter), convert(img_tf[:,:,2:3], gaussFilter)], axis=-1)\nimg_gauss = tf.cast(img_gauss, tf.int32)\nplt.imshow(img_gauss)\n103/23:\nimg_np = cv2.imread(\"planets.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n103/24:\ndef convert(img, filter):\n    img = tf.cast(img, tf.float32)\n    # Add additional dimensions\n    filter = tf.reshape(filter, [filter.shape[0], filter.shape[1], 1, 1])\n    img = tf.reshape(img, [1, img.shape[0], img.shape[1], 1])\n    # Applying convolution\n    conv_res = tf.nn.convolution(img, filter, padding='SAME')\n    # Remove additional dimenstions\n    return tf.reshape(conv_res, [conv_res.shape[1], conv_res.shape[2], conv_res.shape[3]])\n\ngaussFilter = tf.convert_to_tensor(\n    [[0, 0.01, 0.02, 0.01, 0.],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0.02, 0.1, 0.16, 0.1, 0.02],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0., 0.01, 0.02, 0.01, 0.]])\nimg_gauss = tf.concat( [convert (img_tf[:,:,0:1], gaussFilter), convert (img_tf[:,:,1:2], gaussFilter), convert(img_tf[:,:,2:3], gaussFilter)], axis=-1)\nimg_gauss = tf.cast(img_gauss, tf.int32)\nplt.imshow(img_gauss)\n103/25:\nimg_np = cv2.imread(\"img.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n103/26:\nimg_np = cv2.imread(\"img.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n103/27:\ndef convert(img, filter):\n    img = tf.cast(img, tf.float32)\n    # Add additional dimensions\n    filter = tf.reshape(filter, [filter.shape[0], filter.shape[1], 1, 1])\n    img = tf.reshape(img, [1, img.shape[0], img.shape[1], 1])\n    # Applying convolution\n    conv_res = tf.nn.convolution(img, filter, padding='SAME')\n    # Remove additional dimenstions\n    return tf.reshape(conv_res, [conv_res.shape[1], conv_res.shape[2], conv_res.shape[3]])\n\ngaussFilter = tf.convert_to_tensor(\n    [[0, 0.01, 0.02, 0.01, 0.],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0.02, 0.1, 0.16, 0.1, 0.02],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0., 0.01, 0.02, 0.01, 0.]])\nimg_gauss = tf.concat( [convert (img_tf[:,:,0:1], gaussFilter), convert (img_tf[:,:,1:2], gaussFilter), convert(img_tf[:,:,2:3], gaussFilter)], axis=-1)\nimg_gauss = tf.cast(img_gauss, tf.int32)\nplt.imshow(img_gauss)\n103/28:\nimg_np = cv2.imread(\"add.ong\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n103/29:\nimg_np = cv2.imread(\"add.png\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n103/30:\ndef convert(img, filter):\n    img = tf.cast(img, tf.float32)\n    # Add additional dimensions\n    filter = tf.reshape(filter, [filter.shape[0], filter.shape[1], 1, 1])\n    img = tf.reshape(img, [1, img.shape[0], img.shape[1], 1])\n    # Applying convolution\n    conv_res = tf.nn.convolution(img, filter, padding='SAME')\n    # Remove additional dimenstions\n    return tf.reshape(conv_res, [conv_res.shape[1], conv_res.shape[2], conv_res.shape[3]])\n\ngaussFilter = tf.convert_to_tensor(\n    [[0, 0.01, 0.02, 0.01, 0.],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0.02, 0.1, 0.16, 0.1, 0.02],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0., 0.01, 0.02, 0.01, 0.]])\nimg_gauss = tf.concat( [convert (img_tf[:,:,0:1], gaussFilter), convert (img_tf[:,:,1:2], gaussFilter), convert(img_tf[:,:,2:3], gaussFilter)], axis=-1)\nimg_gauss = tf.cast(img_gauss, tf.int32)\nplt.imshow(img_gauss)\n103/31:\nboxFilter = tf.fill([3, 3], 1/9)\nimg_box_filter = tf.concat( [convert (img_tf[:,:,0:1], boxFilter), convert (img_tf[:,:,1:2], boxFilter), convert(img_tf[:,:,2:3], boxFilter)], axis=-1)\nimg_box_filter = tf.cast(img_box_filter, tf.int32)\nplt.imshow(img_box_filter)\n103/32:\nimg_np = cv2.imread(\"planets.png\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n103/33:\nimg_np = cv2.imread(\"planets.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n103/34:\nimg_np = cv2.imread(\"planets.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n103/35:\ndef convert(img, filter):\n    img = tf.cast(img, tf.float32)\n    # Add additional dimensions\n    filter = tf.reshape(filter, [filter.shape[0], filter.shape[1], 1, 1])\n    img = tf.reshape(img, [1, img.shape[0], img.shape[1], 1])\n    # Applying convolution\n    conv_res = tf.nn.convolution(img, filter, padding='SAME')\n    # Remove additional dimenstions\n    return tf.reshape(conv_res, [conv_res.shape[1], conv_res.shape[2], conv_res.shape[3]])\n\ngaussFilter = tf.convert_to_tensor(\n    [[0, 0.01, 0.02, 0.01, 0.],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0.02, 0.1, 0.16, 0.1, 0.02],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0., 0.01, 0.02, 0.01, 0.]])\nimg_gauss = tf.concat( [convert (img_tf[:,:,0:1], gaussFilter), convert (img_tf[:,:,1:2], gaussFilter), convert(img_tf[:,:,2:3], gaussFilter)], axis=-1)\nimg_gauss = tf.cast(img_gauss, tf.int32)\nplt.imshow(img_gauss)\n103/36:\nboxFilter = tf.fill([3, 3], 1/9)\nimg_box_filter = tf.concat( [convert (img_tf[:,:,0:1], boxFilter), convert (img_tf[:,:,1:2], boxFilter), convert(img_tf[:,:,2:3], boxFilter)], axis=-1)\nimg_box_filter = tf.cast(img_box_filter, tf.int32)\nplt.imshow(img_box_filter)\n103/37:\nimg_np = cv2.imread(\"img.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n103/38:\nimg_np = cv2.imread(\"img.jpg\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n103/39:\ndef convert(img, filter):\n    img = tf.cast(img, tf.float32)\n    # Add additional dimensions\n    filter = tf.reshape(filter, [filter.shape[0], filter.shape[1], 1, 1])\n    img = tf.reshape(img, [1, img.shape[0], img.shape[1], 1])\n    # Applying convolution\n    conv_res = tf.nn.convolution(img, filter, padding='SAME')\n    # Remove additional dimenstions\n    return tf.reshape(conv_res, [conv_res.shape[1], conv_res.shape[2], conv_res.shape[3]])\n\ngaussFilter = tf.convert_to_tensor(\n    [[0, 0.01, 0.02, 0.01, 0.],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0.02, 0.1, 0.16, 0.1, 0.02],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0., 0.01, 0.02, 0.01, 0.]])\nimg_gauss = tf.concat( [convert (img_tf[:,:,0:1], gaussFilter), convert (img_tf[:,:,1:2], gaussFilter), convert(img_tf[:,:,2:3], gaussFilter)], axis=-1)\nimg_gauss = tf.cast(img_gauss, tf.int32)\nplt.imshow(img_gauss)\n103/40:\nboxFilter = tf.fill([3, 3], 1/9)\nimg_box_filter = tf.concat( [convert (img_tf[:,:,0:1], boxFilter), convert (img_tf[:,:,1:2], boxFilter), convert(img_tf[:,:,2:3], boxFilter)], axis=-1)\nimg_box_filter = tf.cast(img_box_filter, tf.int32)\nplt.imshow(img_box_filter)\n103/41:\nimg_np = cv2.imread(\"gem.png\")\nimg_tf = tf.convert_to_tensor(img_np)\nimg_tf = tf.concat([img_tf[:,:,-1:], img_tf[:,:,1:2], img_tf[:,:,0:1]], axis=-1)\n\nplt.imshow(img_tf)\n103/42:\ndef convert(img, filter):\n    img = tf.cast(img, tf.float32)\n    # Add additional dimensions\n    filter = tf.reshape(filter, [filter.shape[0], filter.shape[1], 1, 1])\n    img = tf.reshape(img, [1, img.shape[0], img.shape[1], 1])\n    # Applying convolution\n    conv_res = tf.nn.convolution(img, filter, padding='SAME')\n    # Remove additional dimenstions\n    return tf.reshape(conv_res, [conv_res.shape[1], conv_res.shape[2], conv_res.shape[3]])\n\ngaussFilter = tf.convert_to_tensor(\n    [[0, 0.01, 0.02, 0.01, 0.],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0.02, 0.1, 0.16, 0.1, 0.02],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0., 0.01, 0.02, 0.01, 0.]])\nimg_gauss = tf.concat( [convert(img_tf[:,:,0:1], gaussFilter), convert(img_tf[:,:,1:2], gaussFilter), convert(img_tf[:,:,2:3], gaussFilter)], axis=-1)\nimg_gauss = tf.cast(img_gauss, tf.int32)\nplt.imshow(img_gauss)\n103/43:\nboxFilter = tf.fill([3, 3], 1/9)\nimg_box_filter = tf.concat( [convert(img_tf[:,:,0:1], boxFilter), convert(img_tf[:,:,1:2], boxFilter), convert(img_tf[:,:,2:3], boxFilter)], axis=-1)\nimg_box_filter = tf.cast(img_box_filter, tf.int32)\nplt.imshow(img_box_filter)\n103/44:\nboxFilter = tf.fill([3, 3], 1/9)\nimg_box_filter = tf.concat([convert(img_tf[:,:,0:1], boxFilter), convert(img_tf[:,:,1:2], boxFilter), convert(img_tf[:,:,2:3], boxFilter)], axis=-1)\nimg_box_filter = tf.cast(img_box_filter, tf.int32)\nplt.imshow(img_box_filter)\nplt.imshow(img_tf)\n103/45:\nboxFilter = tf.fill([3, 3], 1/9)\nimg_box_filter = tf.concat([convert(img_tf[:,:,0:1], boxFilter), convert(img_tf[:,:,1:2], boxFilter), convert(img_tf[:,:,2:3], boxFilter)], axis=-1)\nimg_box_filter = tf.cast(img_box_filter, tf.int32)\nplt.imshow(img_box_filter)\n103/46: plt.imshow(img_tf)\n103/47: plt.imshow(img_tf)\n103/48:\nsharpFilter = tf.convert_to_tensor(\n    [[0., -1., 0.],\n    [-1., 5., -1.],\n    [0., -1., 0.]])\nimg_gauss = tf.concat([convert(img_tf[:,:,0:1], sharpFilter),\n                       convert(img_tf[:,:,1:2], sharpFilter),\n                       convert(img_tf[:,:,2:3], sharpFilter)], axis=-1)\nimg_gauss = tf.cast(img_gauss, tf.int32)\nplt.imshow(img_gauss)\n103/49:\nsharpFilter = tf.convert_to_tensor(\n    [[0., -1., 0.],\n    [-1., 5., -1.],\n    [0., -1., 0.]])\nimg_sharp = tf.concat([convert(img_tf[:,:,0:1], sharpFilter),\n                       convert(img_tf[:,:,1:2], sharpFilter),\n                       convert(img_tf[:,:,2:3], sharpFilter)], axis=-1)\nimg_sharp = tf.cast(img_sharp, tf.int32)\nplt.imshow(img_gauss)\n103/50:\nsharpFilter = tf.convert_to_tensor(\n    [[0., -1., 0.],\n    [-1., 5., -1.],\n    [0., -1., 0.]])\nimg_sharp = tf.concat([convert(img_tf[:,:,0:1], sharpFilter),\n                       convert(img_tf[:,:,1:2], sharpFilter),\n                       convert(img_tf[:,:,2:3], sharpFilter)], axis=-1)\nimg_sharp = tf.cast(img_sharp, tf.int32)\nplt.imshow(img_sharp)\n103/51:\ndef convert(img, filter):\n    img = tf.cast(img, tf.float32)\n    # Add additional dimensions\n    filter = tf.reshape(filter, [filter.shape[0], filter.shape[1], 1, 1])\n    img = tf.reshape(img, [1, img.shape[0], img.shape[1], 1])\n    # Applying convolution\n    conv_res = tf.nn.convolution(img, filter, padding='SAME')\n    # Remove additional dimenstions\n    return tf.reshape(conv_res, [conv_res.shape[1], conv_res.shape[2], conv_res.shape[3]])\n\ngaussFilter = tf.convert_to_tensor(\n    [[0, 0.01, 0.02, 0.01, 0.],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0.02, 0.1, 0.16, 0.1, 0.02],\n    [0.01, 0.06, 0.1, 0.06, 0.01],\n    [0., 0.01, 0.02, 0.01, 0.]])\nimg_gauss = tf.concat([convert(img_tf[:,:,0:1], gaussFilter), convert(img_tf[:,:,1:2], gaussFilter), convert(img_tf[:,:,2:3], gaussFilter)], axis=-1)\nimg_gauss = tf.cast(img_gauss, tf.int32)\nplt.imshow(img_gauss)\n103/52:\nsharpFilter = tf.convert_to_tensor(\n    [[0., -1., 0.],\n    [-1., 5., -1.],\n    [0., -1., 0.]])\nimg_sharp = tf.concat([convert(img_gauss[:,:,0:1], sharpFilter),\n                       convert(img_gauss[:,:,1:2], sharpFilter),\n                       convert(img_gauss[:,:,2:3], sharpFilter)], axis=-1)\nimg_sharp = tf.cast(img_sharp, tf.int32)\nplt.imshow(img_sharp)\n103/53:\nsharpFilter = tf.convert_to_tensor(\n    [[0., -1., 0.],\n    [-1., 5., -1.],\n    [0., -1., 0.]])\nimg_sharp = tf.concat([convert(img_gauss[:,:,0:1], sharpFilter),\n                       convert(img_gauss[:,:,1:2], sharpFilter),\n                       convert(img_gauss[:,:,2:3], sharpFilter)], axis=-1)\nimg_sharp = tf.cast(img_sharp, tf.int32)\nplt.imshow(img_sharp)\n103/54:\ng_x = tf.convert_to_tensor(\n    [[1., 0., -1.],\n    [2., 0., -2.],\n    [1., 0., -1.]])\n\ng_y = tf.convert_to_tensor(\n    [[1., 2., 1.],\n    [0., 0., 0.],\n    [-1., -2., -1.]])\n\nimg_g_x = tf.concat([convert(img_gauss[:,:,0:1], g_x),\n                       convert(img_gauss[:,:,1:2], g_x),\n                       convert(img_gauss[:,:,2:3], g_x)], axis=-1)\nimg_g_y = tf.concat([convert(img_gauss[:,:,0:1], g_y),\n                       convert(img_gauss[:,:,1:2], g_y),\n                       convert(img_gauss[:,:,2:3], g_y)], axis=-1)\nimg_sobel = tf.math.sqrt(tf.math.pow(img_g_x, 2)\n                        + tf.math.pow(img_g_y, 2))\nimg_sobel = tf.cast(img_sobel, tf.int32)\nplt.imshow(img_sharp)\n103/55:\ng_x = tf.convert_to_tensor(\n    [[1., 0., -1.],\n    [2., 0., -2.],\n    [1., 0., -1.]])\n\ng_y = tf.convert_to_tensor(\n    [[1., 2., 1.],\n    [0., 0., 0.],\n    [-1., -2., -1.]])\n\nimg_g_x = tf.concat([convert(img_tf[:,:,0:1], g_x),\n                       convert(img_tf[:,:,1:2], g_x),\n                       convert(img_tf[:,:,2:3], g_x)], axis=-1)\nimg_g_y = tf.concat([convert(img_tf[:,:,0:1], g_y),\n                       convert(img_tf[:,:,1:2], g_y),\n                       convert(img_tf[:,:,2:3], g_y)], axis=-1)\nimg_sobel = tf.math.sqrt(tf.math.pow(img_g_x, 2)\n                        + tf.math.pow(img_g_y, 2))\nimg_sobel = tf.cast(img_sobel, tf.int32)\nplt.imshow(img_sharp)\n103/56:\ng_x = tf.convert_to_tensor(\n    [[1., 0., -1.],\n    [2., 0., -2.],\n    [1., 0., -1.]])\n\ng_y = tf.convert_to_tensor(\n    [[1., 2., 1.],\n    [0., 0., 0.],\n    [-1., -2., -1.]])\n\nimg_g_x = tf.concat([convert(img_tf[:,:,0:1], g_x),\n                       convert(img_tf[:,:,1:2], g_x),\n                       convert(img_tf[:,:,2:3], g_x)], axis=-1)\nimg_g_y = tf.concat([convert(img_tf[:,:,0:1], g_y),\n                       convert(img_tf[:,:,1:2], g_y),\n                       convert(img_tf[:,:,2:3], g_y)], axis=-1)\nimg_sobel = tf.math.sqrt(tf.math.pow(img_g_x, 2)\n                        + tf.math.pow(img_g_y, 2))\nimg_sobel = tf.cast(img_sobel, tf.int32)\nplt.imshow(img_sobel)\n106/1:\ntrain_gen = data_gen(train_x, train_y, aug='Train')\ntest_gen = data_gen(test_x, test_y, aug='Test')\n\nx, y = next(train_gen)\n# print(y[0])\nplt.imshow(x[0])\n106/2:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.regularizers import *\n\nfrom imgaug import augmenters as iaa\nimport glob\nimport random\n106/3:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.regularizers import *\n\nfrom imgaug import augmenters as iaa\nimport glob\nimport random\n106/4:\ndef unpickle(file):\n  with open(file, 'rb') as fo:\n    dict = pickle.load(fo, encoding='bytes')\n  return dict\n\n\ndef getTrainImages():\n  # images = np.empty(shape=(0, 32, 32, 3), dtype=np.int8)\n  # labels = list()\n\n  # for i in range(1, 6):\n  #   data = unpickle('data_batch_' + str(i))\n  #   images = np.concatenate((images, data[b'data'].reshape((10000, 32, 32, 3), order='F')), axis=0)\n  #   labels = labels + data[b'labels']\n\n  data = unpickle('train')\n  images = data[b'data'].reshape((50000, 32, 32, 3), order='F')\n  labels = data[b'fine_labels']\n\n  return images, labels\n\n\ndef getTestImages():\n  data = unpickle('test')\n  images = data[b'data'].reshape((10000, 32, 32, 3), order='F')\n  labels = data[b'fine_labels']\n  \n  return images, labels\n106/5:\nlabel_names = unpickle('meta')[b'fine_label_names']\nprint(label_names)\n106/6:\ntrain_x, train_y = getTrainImages()\n\nprint(train_x.shape)\nprint(len(train_y))\n\nprint(label_names[train_y[0]])\nplt.imshow(train_x[0])\n106/7:\ntest_x, test_y = getTestImages()\n\nprint(test_x.shape)\nprint(len(test_y))\n\nprint(label_names[test_y[0]])\nplt.imshow(test_x[0])\n106/8:\ntrain_x, train_y = getTrainImages()\n\nprint(train_x.shape)\nprint(len(train_y))\n\nprint(label_names[train_y[124]])\nplt.imshow(train_x[124])\n106/9:\ntest_x, test_y = getTestImages()\n\nprint(test_x.shape)\nprint(len(test_y))\n\nprint(label_names[test_y[124]])\nplt.imshow(test_x[124])\n106/10:\n# Аугментация\nseq_train = iaa.Sequential([\n    # iaa.Rot90(1),\n    iaa.Sometimes(0.5, iaa.Affine(rotate=(-90, 0))), # TODO: Поменять значения\n    iaa.Sometimes(0.5, iaa.Affine(scale=(1.1, 0.9)))\n])\n\n# seq_test = iaa.Sequential([\n#     iaa.Rot90(1)\n# ])\n106/11:\ndef onehot(i, size):\n  res = [0] * size\n  res[i] = 1\n  return res\n\n\ndef data_gen(data, labels, batch_size=128, aug='None'): # TODO: Палево\n    \n  x = list()\n  y = list()\n  \n  while True:\n  \n    while len(x) < batch_size:\n      i = random.randint(0, data.shape[0] - 1)\n      img = data[i]\n      label = onehot(labels[i], len(label_names))\n\n      x.append(img)\n      y.append(label)\n\n    x = np.stack(x, axis=0)\n    y = np.stack(y, axis=0)\n    \n    if aug == 'Train':\n      x = seq_train(images=x)\n    # elif aug == 'Test':\n    #   x = seq_test(images=x)\n    \n    yield x, y\n106/12:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.regularizers import *\nfrom tensorflow.python.client import device_lib\n\nfrom imgaug import augmenters as iaa\nimport glob\nimport random\n106/13:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.regularizers import *\nfrom tensorflow.python.client import device_lib\n\nfrom imgaug import augmenters as iaa\nimport glob\nimport random\n106/14:\ndef get_available_devices():\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos]\n\nprint(get_available_devices())\n106/15:\ndef get_available_devices():\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos]\n\nprint(get_available_devices())\n106/16: tf.config.experimental.list_physical_devices('GPU')\n106/17: tf.config.experimental.list_physical_devices('GPU')\n106/18:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.regularizers import *\nfrom tensorflow.python.client import device_lib\n\nfrom imgaug import augmenters as iaa\nimport glob\nimport random\n106/19:\ndef get_available_devices():\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos]\n\nprint(get_available_devices())\n146/1:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.regularizers import *\nfrom tensorflow.python.client import device_lib\n\nfrom imgaug import augmenters as iaa\nimport glob\nimport random\n146/2:\ndef get_available_devices():\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos]\n\nprint(get_available_devices())\n147/1:\nfrom tensorflow.python.client import device_lib \nprint(device_lib.list_local_devices())\n147/2:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.regularizers import *\nfrom tensorflow.python.client import device_lib\n\nfrom imgaug import augmenters as iaa\nimport glob\nimport random\n147/3:\ndef get_available_devices():\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos]\n\nprint(get_available_devices())\n147/4: tf.config.experimental.list_physical_devices('GPU')\n147/5:\nfrom google.colab import files\nuploaded = files.upload()\n147/6:\ndef unpickle(file):\n  with open(file, 'rb') as fo:\n    dict = pickle.load(fo, encoding='bytes')\n  return dict\n\n\ndef getTrainImages():\n  # images = np.empty(shape=(0, 32, 32, 3), dtype=np.int8)\n  # labels = list()\n\n  # for i in range(1, 6):\n  #   data = unpickle('data_batch_' + str(i))\n  #   images = np.concatenate((images, data[b'data'].reshape((10000, 32, 32, 3), order='F')), axis=0)\n  #   labels = labels + data[b'labels']\n\n  data = unpickle('train')\n  images = data[b'data'].reshape((50000, 32, 32, 3), order='F')\n  labels = data[b'fine_labels']\n\n  return images, labels\n\n\ndef getTestImages():\n  data = unpickle('test')\n  images = data[b'data'].reshape((10000, 32, 32, 3), order='F')\n  labels = data[b'fine_labels']\n  \n  return images, labels\n147/7:\nlabel_names = unpickle('meta')[b'fine_label_names']\nprint(label_names)\n147/8:\ntrain_x, train_y = getTrainImages()\n\nprint(train_x.shape)\nprint(len(train_y))\n\nprint(label_names[train_y[124]])\nplt.imshow(train_x[124])\n147/9:\ntest_x, test_y = getTestImages()\n\nprint(test_x.shape)\nprint(len(test_y))\n\nprint(label_names[test_y[124]])\nplt.imshow(test_x[124])\n147/10:\n# Аугментация\nseq_train = iaa.Sequential([\n    # iaa.Rot90(1),\n    iaa.Sometimes(0.5, iaa.Affine(rotate=(-90, 0))), # TODO: Поменять значения\n    iaa.Sometimes(0.5, iaa.Affine(scale=(1.1, 0.9)))\n])\n\n# seq_test = iaa.Sequential([\n#     iaa.Rot90(1)\n# ])\n147/11:\ndef onehot(i, size):\n  res = [0] * size\n  res[i] = 1\n  return res\n\n\ndef data_gen(data, labels, batch_size=128, aug='None'): # TODO: Палево\n    \n  x = list()\n  y = list()\n  \n  while True:\n  \n    while len(x) < batch_size:\n      i = random.randint(0, data.shape[0] - 1)\n      img = data[i]\n      label = onehot(labels[i], len(label_names))\n\n      x.append(img)\n      y.append(label)\n\n    x = np.stack(x, axis=0)\n    y = np.stack(y, axis=0)\n    \n    if aug == 'Train':\n      x = seq_train(images=x)\n    # elif aug == 'Test':\n    #   x = seq_test(images=x)\n    \n    yield x, y\n147/12:\ntrain_gen = data_gen(train_x, train_y, aug='Train')\ntest_gen = data_gen(test_x, test_y, aug='Test')\n\nx, y = next(train_gen)\n# print(y[0])\nplt.imshow(x[0])\n147/13:\ndef resnet_block_1(x, filters):\n  inp = x\n  f1, f2 = filters\n\n  x = Conv2D(f1, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)\n  x = BatchNormalization()(x)\n  x = Activation('relu')(x)\n\n  x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.001))(x)\n  x = BatchNormalization()(x)\n  x = Activation('relu')(x)\n\n  x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)\n  x = BatchNormalization()(x)\n  \n  x = Add()([x, inp])\n  x = Activation('relu')(x)\n\n  return x\n\n\ndef resnet_block_2(x, s, filters):\n  inp = x\n  f1, f2 = filters\n\n  x = Conv2D(f1, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.001))(x)\n  x = BatchNormalization()(x)\n  x = Activation('relu')(x)\n\n  x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.001))(x)\n  x = BatchNormalization()(x)\n  x = Activation('relu')(x)\n\n  x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)\n  x = BatchNormalization()(x)\n\n  inp = Conv2D(f2, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.001))(inp)\n  inp = BatchNormalization()(inp)\n\n  x = Add()([x, inp])\n  x = Activation('relu')(x)\n\n  return x\n147/14:\ndef resnet(image_shape):\n\n  inp = Input(shape=(image_shape[0], image_shape[1], image_shape[2]))\n  x = ZeroPadding2D(padding=(3, 3))(inp)\n\n  x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2))(x)\n  x = BatchNormalization()(x)\n  x = Activation('relu')(x)\n  x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n  x = resnet_block_2(x, s=1, filters=(32, 128))\n  x = resnet_block_1(x, filters=(32, 128))\n  x = resnet_block_1(x, filters=(32, 128))\n\n  x = resnet_block_2(x, s=2, filters=(64, 256))\n  x = resnet_block_1(x, filters=(64, 256))\n  x = resnet_block_1(x, filters=(64, 256))\n\n  x = resnet_block_2(x, s=2, filters=(128, 512))\n  x = resnet_block_1(x, filters=(128, 512))\n  x = resnet_block_1(x, filters=(128, 512))\n  x = resnet_block_1(x, filters=(128, 512))\n\n  x = resnet_block_2(x, s=2, filters=(256, 1024))\n  x = resnet_block_1(x, filters=(256, 1024))\n  x = resnet_block_1(x, filters=(256, 1024))\n\n  x = AveragePooling2D((2, 2), padding='same')(x)\n\n  x = Flatten()(x)\n  x = Dense(len(label_names), activation='softmax', kernel_initializer='he_normal')(x)\n\n  model = Model(inputs=inp, outputs=x)\n\n  return model\n147/15:\nmodel = resnet(train_x[0].shape)\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n    metrics=['accuracy'])\n147/16:\nwith tf.device('/device:GPU:0'):\n  model.fit(\n    train_gen,\n    steps_per_epoch=50,\n    epochs=40,\n    validation_data=test_gen,\n    validation_steps=10\n  )\n148/1:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.regularizers import *\nfrom tensorflow.python.client import device_lib\n\nfrom imgaug import augmenters as iaa\nimport glob\nimport random\n148/2:\ndef get_available_devices():\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos]\n\nprint(get_available_devices())\n148/3:\ndef unpickle(file):\n  with open(file, 'rb') as fo:\n    dict = pickle.load(fo, encoding='bytes')\n  return dict\n\n\ndef getTrainImages():\n  # images = np.empty(shape=(0, 32, 32, 3), dtype=np.int8)\n  # labels = list()\n\n  # for i in range(1, 6):\n  #   data = unpickle('data_batch_' + str(i))\n  #   images = np.concatenate((images, data[b'data'].reshape((10000, 32, 32, 3), order='F')), axis=0)\n  #   labels = labels + data[b'labels']\n\n  data = unpickle('train')\n  images = data[b'data'].reshape((50000, 32, 32, 3), order='F')\n  labels = data[b'fine_labels']\n\n  return images, labels\n\n\ndef getTestImages():\n  data = unpickle('test')\n  images = data[b'data'].reshape((10000, 32, 32, 3), order='F')\n  labels = data[b'fine_labels']\n  \n  return images, labels\n148/4:\nlabel_names = unpickle('meta')[b'fine_label_names']\nprint(label_names)\n148/5:\ntrain_x, train_y = getTrainImages()\n\nprint(train_x.shape)\nprint(len(train_y))\n\nprint(label_names[train_y[124]])\nplt.imshow(train_x[124])\n148/6:\ntest_x, test_y = getTestImages()\n\nprint(test_x.shape)\nprint(len(test_y))\n\nprint(label_names[test_y[124]])\nplt.imshow(test_x[124])\n148/7:\n# Аугментация\nseq_train = iaa.Sequential([\n    # iaa.Rot90(1),\n    iaa.Sometimes(0.5, iaa.Affine(rotate=(-90, 0))), # TODO: Поменять значения\n    iaa.Sometimes(0.5, iaa.Affine(scale=(1.1, 0.9)))\n])\n\n# seq_test = iaa.Sequential([\n#     iaa.Rot90(1)\n# ])\n148/8:\ndef onehot(i, size):\n  res = [0] * size\n  res[i] = 1\n  return res\n\n\ndef data_gen(data, labels, batch_size=128, aug='None'): # TODO: Палево\n    \n  x = list()\n  y = list()\n  \n  while True:\n  \n    while len(x) < batch_size:\n      i = random.randint(0, data.shape[0] - 1)\n      img = data[i]\n      label = onehot(labels[i], len(label_names))\n\n      x.append(img)\n      y.append(label)\n\n    x = np.stack(x, axis=0)\n    y = np.stack(y, axis=0)\n    \n    if aug == 'Train':\n      x = seq_train(images=x)\n    # elif aug == 'Test':\n    #   x = seq_test(images=x)\n    \n    yield x, y\n148/9:\ntrain_gen = data_gen(train_x, train_y, aug='Train')\ntest_gen = data_gen(test_x, test_y, aug='Test')\n\nx, y = next(train_gen)\n# print(y[0])\nplt.imshow(x[0])\n148/10:\ndef vgg_block(layer_in, n_filters, n_conv):\n    # add convolutional layers\n    for _ in range(n_conv):\n        layer_in = Conv2D(n_filters, (3,3), padding='same', activation='relu')(layer_in)\n    # add max pooling layer\n    layer_in = MaxPooling2D((2,2), strides=(2,2))(layer_in)\n    return layer_in\n148/11:\ndef resnet(image_shape):\n    # define model input\n    inp = Input(shape=(image_shape[0], image_shape[1], image_shape[2]))\n    # add vgg module\n    x = vgg_block(inp, 64, 2)\n    x = vgg_block(x, 128, 2)\n    x = vgg_block(x, 256, 4)\n    \n    x = AveragePooling2D((2, 2), padding='same')(x)\n\n    x = Flatten()(x)\n    x = Dense(len(label_names), activation='softmax', kernel_initializer='he_normal')(x)    \n    \n    model = Model(inputs=inp, outputs=x)\n    return model\n#   inp = Input(shape=(image_shape[0], image_shape[1], image_shape[2]))\n#   x = ZeroPadding2D(padding=(3, 3))(inp)\n\n#   x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2))(x)\n#   x = BatchNormalization()(x)\n#   x = Activation('relu')(x)\n#   x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n#   x = resnet_block_2(x, s=1, filters=(32, 128))\n#   x = resnet_block_1(x, filters=(32, 128))\n#   x = resnet_block_1(x, filters=(32, 128))\n\n#   x = resnet_block_2(x, s=2, filters=(64, 256))\n#   x = resnet_block_1(x, filters=(64, 256))\n#   x = resnet_block_1(x, filters=(64, 256))\n\n#   x = resnet_block_2(x, s=2, filters=(128, 512))\n#   x = resnet_block_1(x, filters=(128, 512))\n#   x = resnet_block_1(x, filters=(128, 512))\n#   x = resnet_block_1(x, filters=(128, 512))\n\n#   x = resnet_block_2(x, s=2, filters=(256, 1024))\n#   x = resnet_block_1(x, filters=(256, 1024))\n#   x = resnet_block_1(x, filters=(256, 1024))\n\n#   x = AveragePooling2D((2, 2), padding='same')(x)\n\n#   x = Flatten()(x)\n#   x = Dense(len(label_names), activation='softmax', kernel_initializer='he_normal')(x)\n\n#   model = Model(inputs=inp, outputs=x)\n\n#   return model\n148/12:\nmodel = resnet(train_x[0].shape)\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n    metrics=['accuracy'])\n148/13:\nwith tf.device('/device:GPU:0'):\n  model.fit(\n    train_gen,\n    steps_per_epoch=50,\n    epochs=40,\n    validation_data=test_gen,\n    validation_steps=10\n  )\n147/17:\ndef resnet(image_shape):\n\n  inp = Input(shape=(image_shape[0], image_shape[1], image_shape[2]))\n  x = ZeroPadding2D(padding=(3, 3))(inp)\n\n  x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2))(x)\n  x = BatchNormalization()(x)\n  x = Activation('relu')(x)\n  x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n  x = resnet_block_2(x, s=1, filters=(32, 128))\n  x = resnet_block_1(x, filters=(32, 128))\n  x = resnet_block_1(x, filters=(32, 128))\n\n  x = resnet_block_2(x, s=2, filters=(64, 256))\n  x = resnet_block_1(x, filters=(64, 256))\n  x = resnet_block_1(x, filters=(64, 256))\n  x = resnet_block_1(x, filters=(64, 256))\n  x = resnet_block_1(x, filters=(64, 256))\n\n  x = resnet_block_2(x, s=2, filters=(128, 512))\n  x = resnet_block_1(x, filters=(128, 512))\n  x = resnet_block_1(x, filters=(128, 512))\n  x = resnet_block_1(x, filters=(128, 512))\n  x = resnet_block_1(x, filters=(128, 512))\n\n  x = resnet_block_2(x, s=2, filters=(256, 1024))\n  x = resnet_block_1(x, filters=(256, 1024))\n  x = resnet_block_1(x, filters=(256, 1024))\n\n  x = AveragePooling2D((2, 2), padding='same')(x)\n\n  x = Flatten()(x)\n  x = Dense(len(label_names), activation='softmax', kernel_initializer='he_normal')(x)\n\n  model = Model(inputs=inp, outputs=x)\n\n  return model\n147/18:\nmodel = resnet(train_x[0].shape)\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n    metrics=['accuracy'])\n147/19:\nwith tf.device('/device:GPU:0'):\n  model.fit(\n    train_gen,\n    steps_per_epoch=50,\n    epochs=40,\n    validation_data=test_gen,\n    validation_steps=10\n  )\n148/14:\ndef vggnet(image_shape):\n  inp = Input(shape=image_shape)\n\n  x = inp\n  x = tf.cast(x, tf.float32)\n  x = x / (127.5) - 1.\n\n  x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(x)\n  # x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(x)\n  x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n\n  x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x)\n  # x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x)\n  x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n\n  x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n  # x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n  # x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n  x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n\n  x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n  # x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n  # x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n  x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n\n  # x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n  # x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n  # x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n  # x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n\n  x = Flatten()(x)\n  x = Dense(units = 128, activation ='relu')(x)\n  # x = Dense(units = 4096, activation ='relu')(x)\n  output = Dense(units = len(label_names), activation ='softmax')(x)\n\n  model = Model(inputs=inp, outputs=output)\n  return model\n148/15:\nmodel = vggnet(train_x[0].shape)\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n    metrics=['accuracy'])\n148/16:\nmodel.fit(\n  train_gen,\n  steps_per_epoch=100,\n  epochs=40,\n  validation_data=test_gen,\n  validation_steps=10\n)\n147/20:\ndef resnet(image_shape):\n\n    inp = Input(shape=(image_shape[0], image_shape[1], image_shape[2]))\n    x = ZeroPadding2D(padding=(3, 3))(inp)\n\n    x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2))(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n    x = resnet_block_2(x, s=1, filters=(64, 256))\n    x = resnet_block_1(x, filters=(64, 256))\n    x = resnet_block_1(x, filters=(64, 256))\n\n    x = resnet_block_2(x, s=2, filters=(128, 512))\n    x = resnet_block_1(x, filters=(128, 512))\n    x = resnet_block_1(x, filters=(128, 512))\n    x = resnet_block_1(x, filters=(128, 512))\n\n    x = resnet_block_2(x, s=2, filters=(256, 1024))\n    x = resnet_block_1(x, filters=(256, 1024))\n    x = resnet_block_1(x, filters=(256, 1024))\n    x = resnet_block_1(x, filters=(256, 1024))\n    x = resnet_block_1(x, filters=(256, 1024))\n    x = resnet_block_1(x, filters=(256, 1024))\n\n    x = resnet_block_2(x, s=2, filters=(512, 2048))\n    x = resnet_block_1(x, filters=(512, 2048))\n    x = resnet_block_1(x, filters=(512, 2048))\n\n    x = AveragePooling2D((2, 2), padding='same')(x)\n\n    x = Flatten()(x)\n    x = Dense(len(label_names), activation='softmax', kernel_initializer='he_normal')(x)\n\n    model = Model(inputs=inp, outputs=x)\n\n    return model\n147/21:\nmodel = resnet(train_x[0].shape)\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n    metrics=['accuracy'])\n147/22:\nwith tf.device('/device:GPU:0'):\n  model.fit(\n    train_gen,\n    steps_per_epoch=100,\n    epochs=40,\n    validation_data=test_gen,\n    validation_steps=10\n  )\n147/23:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.regularizers import *\nfrom tensorflow.python.client import device_lib\n\nfrom imgaug import augmenters as iaa\nimport glob\nimport random\n147/24:\ndef unpickle(file):\n  with open(file, 'rb') as fo:\n    dict = pickle.load(fo, encoding='bytes')\n  return dict\n\n\ndef getTrainImages():\n  # images = np.empty(shape=(0, 32, 32, 3), dtype=np.int8)\n  # labels = list()\n\n  # for i in range(1, 6):\n  #   data = unpickle('data_batch_' + str(i))\n  #   images = np.concatenate((images, data[b'data'].reshape((10000, 32, 32, 3), order='F')), axis=0)\n  #   labels = labels + data[b'labels']\n\n  data = unpickle('train')\n  images = data[b'data'].reshape((50000, 32, 32, 3), order='F')\n  labels = data[b'fine_labels']\n\n  return images, labels\n\n\ndef getTestImages():\n  data = unpickle('test')\n  images = data[b'data'].reshape((10000, 32, 32, 3), order='F')\n  labels = data[b'fine_labels']\n  \n  return images, labels\n147/25:\nlabel_names = unpickle('meta')[b'fine_label_names']\nprint(label_names)\n147/26:\ntrain_x, train_y = getTrainImages()\n\nprint(train_x.shape)\nprint(len(train_y))\n\nprint(label_names[train_y[124]])\nplt.imshow(train_x[124])\n147/27:\ntest_x, test_y = getTestImages()\n\nprint(test_x.shape)\nprint(len(test_y))\n\nprint(label_names[test_y[124]])\nplt.imshow(test_x[124])\n147/28:\n# Аугментация\nseq_train = iaa.Sequential([\n    # iaa.Rot90(1),\n    iaa.Sometimes(0.5, iaa.Affine(rotate=(-90, 0))), # TODO: Поменять значения\n    iaa.Sometimes(0.5, iaa.Affine(scale=(1.1, 0.9)))\n])\n\n# seq_test = iaa.Sequential([\n#     iaa.Rot90(1)\n# ])\n147/29:\n# Аугментация\nseq_train = iaa.Sequential([\n    # iaa.Rot90(1),\n    iaa.Sometimes(0.5, iaa.Affine(rotate=(-90, 0))), # TODO: Поменять значения\n    iaa.Sometimes(0.5, iaa.Affine(scale=(1.1, 0.9)))\n])\n\n# seq_test = iaa.Sequential([\n#     iaa.Rot90(1)\n# ])\n147/30:\ndef onehot(i, size):\n  res = [0] * size\n  res[i] = 1\n  return res\n\n\ndef data_gen(data, labels, batch_size=128, aug='None'): # TODO: Палево\n    \n  x = list()\n  y = list()\n  \n  while True:\n  \n    while len(x) < batch_size:\n      i = random.randint(0, data.shape[0] - 1)\n      img = data[i]\n      label = onehot(labels[i], len(label_names))\n\n      x.append(img)\n      y.append(label)\n\n    x = np.stack(x, axis=0)\n    y = np.stack(y, axis=0)\n    \n    if aug == 'Train':\n      x = seq_train(images=x)\n    # elif aug == 'Test':\n    #   x = seq_test(images=x)\n    \n    yield x, y\n147/31:\ndef onehot(i, size):\n    res = [0] * size\n    res[i] = 1\n    return res\n\n\ndef data_gen(data, labels, batch_size=128, aug='None'): # TODO: Палево\n    \n    x = list()\n    y = list()\n    \n    while True:\n    \n        while len(x) < batch_size:\n            i = random.randint(0, data.shape[0] - 1)\n            img = data[i]\n            label = onehot(labels[i], len(label_names))\n\n            x.append(img)\n            y.append(label)\n\n        x = np.stack(x, axis=0)\n        y = np.stack(y, axis=0)\n        \n        if aug == 'Train':\n            x = seq_train(images=x)\n        # elif aug == 'Test':\n        #     x = seq_test(images=x)\n        \n        yield x, y\n147/32:\ntrain_gen = data_gen(train_x, train_y, aug='Train')\ntest_gen = data_gen(test_x, test_y, aug='Test')\n\nx, y = next(train_gen)\n# print(y[0])\nplt.imshow(x[0])\n147/33:\ntrain_gen = data_gen(train_x, train_y, aug='Train')\ntest_gen = data_gen(test_x, test_y, aug='Test')\n\nx, y = next(train_gen)\n# print(y[0])\nplt.imshow(x[1])\n147/34:\ndef identity_block(x, filters):\n    inp = x\n    f1, f2, f3 = filters\n\n    x = Conv2D(f1, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n    x = Activation('relu')(x)\n    \n    x = ZeroPadding2D(padding=(1, 1))(x)\n    x = Conv2D(f2, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.001), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(f3, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n    \n    x = Add()([x, inp])\n    x = Activation('relu')(x)\n\n    return x\n\n\ndef conv_block(x, s, filters):\n    inp = x\n    f1, f2, f3 = filters\n\n    x = Conv2D(f1, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.001), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n    x = Activation('relu')(x)\n\n    \n    x = ZeroPadding2D(padding=(1, 1))(x)\n    x = Conv2D(f2, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.001), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(f3, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n\n    inp = Conv2D(f3, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.001), use_bias=False)(inp)\n    inp = BatchNormalization(epsilon=0.0001)(inp)\n\n    x = Add()([x, inp])\n    x = Activation('relu')(x)\n\n    return x\n147/35:\ndef resnet(image_shape):\n\n    inp = Input(shape=(image_shape[0], image_shape[1], image_shape[2]))\n    x = ZeroPadding2D(padding=(3, 3))(inp)\n\n    x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n    x = Activation('relu')(x)    \n    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n    x = conv_block(x, s=1, filters=(64, 256))\n    x = identity_block(x, filters=(64, 256))\n    x = identity_block(x, filters=(64, 256))\n\n    x = conv_block(x, s=2, filters=(128, 512))\n    x = identity_block(x, filters=(128, 512))\n    x = identity_block(x, filters=(128, 512))\n    x = identity_block(x, filters=(128, 512))\n\n    x = conv_block(x, s=2, filters=(256, 1024))\n    x = identity_block(x, filters=(256, 1024))\n    x = identity_block(x, filters=(256, 1024))\n    x = identity_block(x, filters=(256, 1024))\n    x = identity_block(x, filters=(256, 1024))\n    x = identity_block(x, filters=(256, 1024))\n\n    x = conv_block(x, s=2, filters=(512, 2048))\n    x = identity_block(x, filters=(512, 2048))\n    x = identity_block(x, filters=(512, 2048))\n\n    x = AveragePooling2D((7, 7), padding='same')(x)\n\n    x = Flatten()(x)\n    x = Dense(len(label_names), activation='softmax', kernel_initializer='he_normal')(x)\n\n    model = Model(inputs=inp, outputs=x)\n\n    return model\n147/36:\nmodel = resnet(train_x[0].shape)\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n    metrics=['accuracy'])\n147/37:\ndef resnet(image_shape):\n\n    inp = Input(shape=(image_shape[0], image_shape[1], image_shape[2]))\n    x = ZeroPadding2D(padding=(3, 3))(inp)\n\n    x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n    x = Activation('relu')(x)    \n    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n    x = conv_block(x, s=1, filters=(64, 64, 256))\n    x = identity_block(x, filters=(64, 64, 256))\n    x = identity_block(x, filters=(64, 64, 256))\n\n    x = conv_block(x, s=2, filters=(128, 128, 512))\n    x = identity_block(x, filters=(128, 128, 512))\n    x = identity_block(x, filters=(128, 128, 512))\n    x = identity_block(x, filters=(128, 128, 512))\n\n    x = conv_block(x, s=2, filters=(256, 256, 1024))\n    x = identity_block(x, filters=(256, 256, 1024))\n    x = identity_block(x, filters=(256, 256, 1024))\n    x = identity_block(x, filters=(256, 256, 1024))\n    x = identity_block(x, filters=(256, 256, 1024))\n    x = identity_block(x, filters=(256, 256, 1024))\n\n    x = conv_block(x, s=2, filters=(512, 512, 2048))\n    x = identity_block(x, filters=(512, 512, 2048))\n    x = identity_block(x, filters=(512, 512, 2048))\n\n    x = AveragePooling2D((7, 7), padding='same')(x)\n\n    x = Flatten()(x)\n    x = Dense(len(label_names), activation='softmax', kernel_initializer='he_normal')(x)\n\n    model = Model(inputs=inp, outputs=x)\n\n    return model\n147/38:\nmodel = resnet(train_x[0].shape)\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n    metrics=['accuracy'])\n147/39:\ndef identity_block(x, filters):\n    inp = x\n    f1, f2 = filters\n\n    x = Conv2D(f1, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n    x = Activation('relu')(x)\n    \n    x = ZeroPadding2D(padding=(1, 1))(x)\n    x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.001), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n    \n    x = Add()([x, inp])\n    x = Activation('relu')(x)\n\n    return x\n\n\ndef conv_block(x, s, filters):\n    inp = x\n    f1, f2 = filters\n\n    x = Conv2D(f1, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.001), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n    x = Activation('relu')(x)\n\n    \n    x = ZeroPadding2D(padding=(1, 1))(x)\n    x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.001), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n\n    inp = Conv2D(f2, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.001), use_bias=False)(inp)\n    inp = BatchNormalization(epsilon=0.0001)(inp)\n\n    x = Add()([x, inp])\n    x = Activation('relu')(x)\n\n    return x\n147/40:\ndef resnet(image_shape):\n\n    inp = Input(shape=(image_shape[0], image_shape[1], image_shape[2]))\n    x = ZeroPadding2D(padding=(3, 3))(inp)\n\n    x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n    x = Activation('relu')(x)    \n    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n    x = conv_block(x, s=1, filters=(64, 256))\n    x = identity_block(x, filters=(64, 256))\n    x = identity_block(x, filters=(64, 256))\n\n    x = conv_block(x, s=2, filters=(128, 512))\n    x = identity_block(x, filters=(128, 512))\n    x = identity_block(x, filters=(128, 512))\n    x = identity_block(x, filters=(128, 512))\n\n    x = conv_block(x, s=2, filters=(256, 1024))\n    x = identity_block(x, filters=(256, 1024))\n    x = identity_block(x, filters=(256, 1024))\n    x = identity_block(x, filters=(256, 1024))\n    x = identity_block(x, filters=(256, 1024))\n    x = identity_block(x, filters=(256, 1024))\n\n    x = conv_block(x, s=2, filters=(512, 2048))\n    x = identity_block(x, filters=(512, 2048))\n    x = identity_block(x, filters=(512, 2048))\n\n    x = AveragePooling2D((7, 7), padding='same')(x)\n\n    x = Flatten()(x)\n    x = Dense(len(label_names), activation='softmax', kernel_initializer='he_normal')(x)\n\n    model = Model(inputs=inp, outputs=x)\n\n    return model\n147/41:\nmodel = resnet(train_x[0].shape)\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n    metrics=['accuracy'])\n147/42:\ndef identity_block(x, filters):\n    inp = x\n    f1, f2 = filters\n\n    x = Conv2D(f1, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n    x = Activation('relu')(x)\n    \n    #x = ZeroPadding2D(padding=(1, 1))(x)\n    x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.001), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n    \n    x = Add()([x, inp])\n    x = Activation('relu')(x)\n\n    return x\n\n\ndef conv_block(x, s, filters):\n    inp = x\n    f1, f2 = filters\n\n    x = Conv2D(f1, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.001), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n    x = Activation('relu')(x)\n\n    \n    #x = ZeroPadding2D(padding=(1, 1))(x)\n    x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.001), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n\n    inp = Conv2D(f2, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.001), use_bias=False)(inp)\n    inp = BatchNormalization(epsilon=0.0001)(inp)\n\n    x = Add()([x, inp])\n    x = Activation('relu')(x)\n\n    return x\n147/43:\ndef resnet(image_shape):\n\n    inp = Input(shape=(image_shape[0], image_shape[1], image_shape[2]))\n    x = ZeroPadding2D(padding=(3, 3))(inp)\n\n    x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2), use_bias=False)(x)\n    x = BatchNormalization(epsilon=0.0001)(x)\n    x = Activation('relu')(x)    \n    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n    x = conv_block(x, s=1, filters=(64, 256))\n    x = identity_block(x, filters=(64, 256))\n    x = identity_block(x, filters=(64, 256))\n\n    x = conv_block(x, s=2, filters=(128, 512))\n    x = identity_block(x, filters=(128, 512))\n    x = identity_block(x, filters=(128, 512))\n    x = identity_block(x, filters=(128, 512))\n\n    x = conv_block(x, s=2, filters=(256, 1024))\n    x = identity_block(x, filters=(256, 1024))\n    x = identity_block(x, filters=(256, 1024))\n    x = identity_block(x, filters=(256, 1024))\n    x = identity_block(x, filters=(256, 1024))\n    x = identity_block(x, filters=(256, 1024))\n\n    x = conv_block(x, s=2, filters=(512, 2048))\n    x = identity_block(x, filters=(512, 2048))\n    x = identity_block(x, filters=(512, 2048))\n\n    x = AveragePooling2D((7, 7), padding='same')(x)\n\n    x = Flatten()(x)\n    x = Dense(len(label_names), activation='softmax', kernel_initializer='he_normal')(x)\n\n    model = Model(inputs=inp, outputs=x)\n\n    return model\n147/44:\nmodel = resnet(train_x[0].shape)\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n    metrics=['accuracy'])\n147/45:\nwith tf.device('/device:GPU:0'):\n  model.fit(\n    train_gen,\n    steps_per_epoch=100,\n    epochs=40,\n    validation_data=test_gen,\n    validation_steps=10\n  )\n154/1:\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\n\nimport re\nimport functools\nimport random\n154/2:\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\n\nimport re\nimport functools\nimport random\n154/3: ds = tfds.load('imdb_reviews', split='train')\n154/4:\ntrain_data, validation_data, test_data = tfds.load(\n    name=\"imdb_reviews\", \n    split=('train[:60%]', 'train[60%:]', 'test'),\n    as_supervised=True)\n154/5: print(ds.lenght)\n154/6: print(ds.lenght).options()\n154/7: ds.options()\n154/8: ds.take(1)\n154/9: ds.take(1).as_numpy_iterator()\n154/10: ds.take(1)\n154/11:\ndef split_in_words(data):\n    return [re.sub(r'[.,\"():;!?\\\\]', '', pr[0].numpy().decode('utf-8').replace('<br />', '').replace(\"\\\\'\", \"'\")).split(' ') for pr in list(data)]\n    \n\nreviews_train = split_in_wordssplit_in_word(train_data)\nreviews_test = split_in_words(test_data)\nreviews_train[10:]\n154/12:\ndef split_in_words(data):\n    return [re.sub(r'[.,\"():;!?\\\\]', '', pr[0].numpy().decode('utf-8').replace('<br />', '').replace(\"\\\\'\", \"'\")).split(' ') for pr in list(data)]\n    \n\nreviews_train = split_in_words(train_data)\nreviews_test = split_in_words(test_data)\nreviews_train[10:]\n154/13:\ndef split_in_words(data):\n    return [re.sub(r'[.,\"():;!?\\\\]', '', pr[0].numpy().decode('utf-8').replace('<br />', '').replace(\"\\\\'\", \"'\")).split(' ') for pr in list(data)]\n    \n\nreviews_train = split_in_words(train_data)\nreviews_test = split_in_words(test_data)\nreviews_train[0]\n154/14:\ndef marks(data):\n    return [pr[1].numpy() for pr in list(data)]\n\nmarks_train = marks(train_data)\nmarks_test = marks(test_data)\n154/15: functools.reduce(lambda res, line: res + len(line), reviews_train, 0)\n154/16:\nwords_count = dict()\nfor review in reviews_train:\n    for word in review:\n        if word in words_count:\n            words_count[word] += 1\n        else:\n            words_count[word] = 1\n\nD_SIZE = 10000\n\nd = words_count.items()\nd = sorted(d, key=lambda e: -e[1])[:D_SIZE]\ndictionary = {\n    k: v\n    for k, v in d\n}\nwords_count['#UNK'] = 0\nwords_count['#END'] = 0\n\nwords_count = {\n    k: i\n    for i, k in enumerate(words_count.keys())\n}\n154/17: len(dictionary)\n154/18:\nwords_count = dict()\nfor review in reviews_train:\n    for word in review:\n        if word in words_count:\n            words_count[word] += 1\n        else:\n            words_count[word] = 1\n\nD_SIZE = 10000\n\nd = words_count.items()\nd = sorted(d, key=lambda e: -e[1])[:D_SIZE]\nwords_count = {\n    k: v\n    for k, v in d\n}\nwords_count['#UNK'] = 0\nwords_count['#END'] = 0\n\nwords_count = {\n    k: i\n    for i, k in enumerate(words_count.keys())\n}\n154/19: len(words_count)\n154/20:\ndef generate_data(data, marks, batch_size=32, seq_len=240):\n    x = list()\n    y = list()\n    while True:\n        x.clear()\n        y.clear()\n\n        while len(x) < batch_size:\n            index = random.randint(0, len(data) - 1)\n            line = data[index]\n            mark = marks[index]\n\n            words = [\n                words_count.get(word, words_count['#UNK'])\n                for word in line\n            ]\n            \n            words = (words + [words_count['#END']])[:seq_len]\n            words = words + [words_count['#UNK']] * (seq_len - len(words))\n\n            x.append(words)\n            y.append(mark)\n                \n        yield (to_categorical(np.array(x), D_SIZE + 2),\n                     to_categorical(np.array(y), 2))\n154/21:\ntrain_gen = generate_data(reviews_train, marks_train)\ntest_gen = generate_data(reviews_test, marks_test)\nx, y = next(train_gen)\nx.shape, y.shape\n154/22:\ninp = Input(shape=(None, D_SIZE + 2))\nnet = inp\n\nnet = Dense(128, activation=None)(net)\n\nnet = SimpleRNN(32, activation='relu', return_sequences=True)(net)\nnet = SimpleRNN(2, activation='softmax', return_sequences=False)(net)\n\nmodel = Model(inp, net)\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n154/23:\nmodel.fit(\n    train_gen,\n    steps_per_epoch=100,\n    epochs=1009,\n)\n154/24:\nmodel.fit(\n    train_gen,\n    steps_per_epoch=100,\n    epochs=100,\n    validation_data=test_gen,\n    validation_steps=15\n)\n157/1:\n\n!pip install wfdb\n!pip install py-ecg-detectors\n!pip install sklearn\n!pip install gensim\n!pip install -U scikit-learn\n157/2:\n\nimport os\nimport wfdb\nimport numpy as np\nimport matplotlib.pyplot as plt\n157/3:\n\nif os.path.isdir(\"mitdb\"):\n    print('You already have the data.')\nelse:\n    wfdb.dl_database('mitdb', 'mitdb')\n157/4:\n\nrecord = record = wfdb.rdsamp('mitdb/100', sampto=500000)\nrecord[1]\n# record[0][:,0]\n#annotation = wfdb.rdann('mitdb/100','atr', sampfrom=50)\n# record[1]\n# record[0][:,0]\n#sample = annotation.__dict__\n#sample\n157/5:\n\nfrom ecgdetectors import Detectors\nfs = 250\ndetectors = Detectors(fs)\nunfiltered_ecg = record[0][:, 1]\nr_peaks = detectors.engzee_detector(unfiltered_ecg)\n# Amendment for first peak\nr_peaks[0] += 20\n157/6:\n\nr_peaks_split = r_peaks[:11]\nplt.figure(figsize=(24,6))\nplt.plot(unfiltered_ecg[:3100])\nplt.plot(r_peaks_split, unfiltered_ecg[r_peaks_split], 'ro')\nplt.show()\n157/7:\n\n# Find heartbeat lenght\ndistances = []\nfor i in range(len(r_peaks)):\n  if i + 1 < len(r_peaks):\n    distances.append(r_peaks[i + 1] - r_peaks[i])\ndistances = np.array(distances)\nheart_beat_len = int(distances.mean())\nheart_beat_len\n157/8:\n\n# Beat EGC signal\nbeats = []\nfor i in range(len(r_peaks)):\n  if i != 0 :\n    r_index = r_peaks[i]\n    retreat = heart_beat_len // 2\n    beats.append(unfiltered_ecg[r_index - retreat:r_index + retreat])\nlen(beats)\n157/9:\n\nig, ax = plt.subplots(nrows=10, figsize=(3,36))\nplots = []\nfor beat in beats:\n  plots.append(beat)\n\n\n\ni = 0 \nfor plot in plots:\n  if i < 10:\n    ax[i].plot(plot)\n    ax[i].set_ylabel('Heartbeat {}'.format(i + 1))\n    ax[i].set_xlabel('Datapoints')\n  i += 1\n157/10:\n\np_waves = []\nqrs_waves = []\nt_waves = []\nfor j in range(len(beats)):\n  if j + 1 != len(beats):\n    p_waves.append(beats[j][:retreat - 15])\n    qrs_waves.append(beats[j][retreat - 15:retreat + 15])\n    t_waves.append(beats[j][retreat + 15:])\n\nprint(len(t_waves[0]))\nprint(len(p_waves))\n157/11:\n\n# P and T waves clustering (20 clusters)\nfrom sklearn.cluster import KMeans\npt_waves = np.array(p_waves + t_waves)\nprint(pt_waves.shape)\nkmeans = KMeans(init='k-means++', n_clusters=300, n_init=10)\nkmeans.fit(pt_waves)\npredict_pt = kmeans.predict(pt_waves)\npredict_pt.shape\n157/12:\n\n# QRS waves clustering (6 clusters)\nqrs_waves = np.array(qrs_waves)\nkmeans_1 = KMeans(init='k-means++', n_clusters=40, n_init=10)\nkmeans_1.fit(qrs_waves)\npredict_qrs = kmeans_1.predict(qrs_waves)\npredict_qrs.shape\n157/13:\n\nalphabet_for_pt = {0:'a', 1:'b', 2:'c', 3:'d', 4:'e', 5:'f', 6:'g', 7:'h', 8:'i',\n                   9:'j', 10:'k', 11:'l', 12:'m', 13:'n', 14:'o', 15:'p', 16:'q', 17:'r',\n                   18:'s', 19:'t'}\nalphabet_for_qrs = {0:'u', 1:'v', 2:'w', 3:'x', 4:'y', 5:'z'}\n\ndef get_symbol_pt(x):\n  return alphabet_for_pt[x]\n\ndef get_symbol_qrs(x):\n  return alphabet_for_qrs[x]\n\nvfunc = np.vectorize(get_symbol_pt)\nvfunc_2 = np.vectorize(get_symbol_qrs)\n\n# predict_pt_letters = vfunc(predict_pt)\n# predict_qrs_letters = vfunc_2(predict_qrs)\n157/14:\n\n#len(predict_qrs_letters)\n157/15:\n\n# words = []\n# for i in range(len(predict_qrs_letters)):\n#   word = ''\n#   word += predict_pt_letters[i]\n#   word += predict_qrs_letters[i]\n#   word += predict_pt_letters[i + len(predict_qrs_letters)//2]\n#   words.append(word)\n#\n# words\n157/16:\n\n# from gensim.models import Word2Vec\n# word2vec = Word2Vec([words,], min_count=0)\n157/17:\n\n# vocabulary = word2vec.wv.key_to_index\n# vocabulary\n157/18:\n\n#sim = word2vec.wv.most_similar('jwl')\n#sim\n157/19:\n\n#word2vec.wv['jwl']\n157/20:\n\n# from sklearn.datasets import make_blobs\n# X, y_true = make_blobs(n_samples=300, centers=4,\n#                        cluster_std=0.60, random_state=0)\n# from sklearn.cluster import KMeans\n# kmeansS = KMeans(n_clusters=4)\n# kmeansS.fit(X)\n# kmeansS.cluster_centers_.shape\n#\n# np.matrix([1, 2, 3])\n157/21:\n\nreversed_alphabet_for_pt = {}\nfor k,v in alphabet_for_pt.items():\n  reversed_alphabet_for_pt[v] = k\n\nreversed_alphabet_for_qrs = {}\nfor k,v in alphabet_for_qrs.items():\n  reversed_alphabet_for_qrs[v] = k\n\npr_centers = kmeans.cluster_centers_\nqrs_centers = kmeans_1.cluster_centers_\n# KMeans Original data: 3484x128\nprint(\"pr_centers.shape:\")\nprint(pr_centers.shape)\nprint(\"qrs_centers.shape:\")\nprint(qrs_centers.shape)\n\ndef get_pr_from_symbol(x):\n  return  pr_centers[x]\n\ndef get_qrs_from_symbol(x):\n  return qrs_centers[x]\n\nreverse_vfunc = np.vectorize(get_pr_from_symbol, signature='()->(n)')\nreverse_vfunc_2 = np.vectorize(get_qrs_from_symbol, signature='()->(n)')\n\npredict_pt_original = reverse_vfunc(predict_pt)\npredict_qrs_original = reverse_vfunc_2(predict_qrs)\nprint(\"predict_pt_original.shape:\")\nprint(predict_pt_original.shape)\nprint(\"predict_qrs_original.shape:\")\nprint(predict_qrs_original.shape)\n\nsplited_pt = np.split(predict_pt_original, 2)\npredict_t_original = splited_pt[0]\npredict_p_original = splited_pt[1]\nprint(str(predict_pt_original.shape) + \" = \" + str(predict_t_original.shape) + \" + \" + str(predict_p_original.shape))\n\nrestored_beats = np.concatenate((predict_t_original, predict_qrs_original, predict_p_original), axis=1)\nprint(\"restored_beats.shape = \" + str(restored_beats.shape))\n157/22:\n\nprint(\"Original:\")\nprint(beats[0].shape)\nprint(\"Restored:\")\nprint(restored_beats[0].shape)\n\nfig, ax = plt.subplots(nrows=10, ncols=2, figsize=(6,36))\nrestored_plots = []\nfor beat in restored_beats:\n  restored_plots.append(beat)\n\noriginal_plots = []\nfor beat in beats:\n  original_plots.append(beat)\n\nfor i in range(10):\n  ax1 = ax[i][0]\n  ax2 = ax[i][1]\n  ax1.plot(restored_plots[i])\n  ax1.set_ylabel('Heartbeat {}'.format(i + 1))\n  ax1.set_xlabel('Datapoints')\n\n  ax2.plot(original_plots[i])\n  ax2.set_ylabel('Heartbeat {}'.format(i + 1))\n  ax2.set_xlabel('Datapoints')\n\nplt.savefig(\"comparing.png\", transparent=False)\n157/23:\n\nfrom sklearn.metrics import mean_absolute_error\nfor i in range(10):\n  rmspe = np.sqrt(np.mean(np.square(((beats[i] - restored_beats[i]) / beats[i])), axis=0))\n  print(rmspe)\n\n\n# len(words)\n# len(vocabulary)\n# kmeans.cluster_centers_.shape\n# kmeans_1.cluster_centers_.shape\n# #kmeans.score(predict_pt)\n# predict_pt\n# #kmeans.cluster_centers_[0]\n157/24:\n\nr_peaks_split = r_peaks[:11]\nplt.figure(figsize=(24,6))\noriginal_full = np.concatenate(beats)[:3100]\nplt.plot(original_full)\nrestored_full = np.concatenate(restored_beats)[:3100]\nplt.plot(restored_full)\nplt.savefig(\"comparing_original.png\", transparent=False)\n157/25:\n\nnp.sqrt(np.mean(np.square(((original_full - restored_full) / original_full)), axis=0))\n158/1:\n\n!pip install wfdb\n!pip install py-ecg-detectors\n!pip install sklearn\n!pip install gensim\n!pip install -U scikit-learn\n158/2:\n\nimport os\nimport wfdb\nimport numpy as np\nimport matplotlib.pyplot as plt\n158/3:\n\nif os.path.isdir(\"../mitdb\"):\n    print('You already have the data.')\nelse:\n    wfdb.dl_database('../mitdb', 'mitdb')\n158/4:\n\nrecord = record = wfdb.rdsamp('mitdb/100', sampto=500000)\nrecord[1]\n# record[0][:,0]\n#annotation = wfdb.rdann('mitdb/100','atr', sampfrom=50)\n# record[1]\n# record[0][:,0]\n#sample = annotation.__dict__\n#sample\n158/5:\n\n!pip install wfdb\n!pip install py-ecg-detectors\n!pip install sklearn\n!pip install gensim\n!pip install -U scikit-learn\n158/6:\n\nimport os\nimport wfdb\nimport numpy as np\nimport matplotlib.pyplot as plt\n158/7:\n\nif os.path.isdir(\"../mitdb\"):\n    print('You already have the data.')\nelse:\n    wfdb.dl_database('../mitdb', 'mitdb')\n158/8:\n\nrecord = record = wfdb.rdsamp('mitdb/100', sampto=500000)\nrecord[1]\n# record[0][:,0]\n#annotation = wfdb.rdann('mitdb/100','atr', sampfrom=50)\n# record[1]\n# record[0][:,0]\n#sample = annotation.__dict__\n#sample\n159/1:\n\n!pip install wfdb\n!pip install py-ecg-detectors\n!pip install sklearn\n!pip install gensim\n!pip install wget\n!pip install -U scikit-learn\n159/2:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return len(r_peaks), heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data.')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/100')\n  annotation = wfdb.rdann(str(out_dir) + '/100', \"atr\")\n  print(record.fs)\n\n\nload(\"../mitdb\")\n159/3:\n\nimport os\nimport wfdb\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom ecgdetectors import Detectors\nfrom gensim.models import Word2Vec\n\nimport wget\nimport zipfile\nfrom pathlib import Path\n159/4:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return len(r_peaks), heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data.')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/100')\n  annotation = wfdb.rdann(str(out_dir) + '/100', \"atr\")\n  print(record.fs)\n\n\nload(\"../mitdb\")\n159/5:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return len(r_peaks), heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data.')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/100')\n  annotation = wfdb.rdann(str(out_dir) + '/100', \"atr\")\n  print(record.p_signal)\n\n\nload(\"../mitdb\")\n159/6:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return len(r_peaks), heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/100')\n  annotation = wfdb.rdann(str(out_dir) + '/100', \"atr\")\n  print(record.p_signal)\n\n  record2 = wfdb.rdsamp(str(out_dir)+'/100', sampto=500000)\n  unfiltered_ecg = record[0][:, 1]\n  print(unfiltered_ecg)\n\n\nload(\"../mitdb\")\n159/7:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return len(r_peaks), heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/100')\n  annotation = wfdb.rdann(str(out_dir) + '/100', \"atr\")\n  print(record.p_signal)\n\n  record2 = wfdb.rdsamp(str(out_dir)+'/100', sampto=500000)\n  unfiltered_ecg = record2[0][:, 1]\n  print(unfiltered_ecg)\n\n\nload(\"../mitdb\")\n159/8:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return len(r_peaks), heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/100')\n  annotation = wfdb.rdann(str(out_dir) + '/100', \"atr\")\n  print(record.p_signal)\n\n  record2 = wfdb.rdsamp(str(out_dir)+'/100', sampto=500000)\n  unfiltered_ecg = record2[0]\n  print(unfiltered_ecg)\n\n\nload(\"../mitdb\")\n159/9:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return len(r_peaks), heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/100')\n  annotation = wfdb.rdann(str(out_dir) + '/100', \"atr\")\n  print(record.p_signal)\n\n  record2 = wfdb.rdsamp(str(out_dir)+'/100', sampto=500000)\n  unfiltered_ecg = record2[0][:, 1]\n  print(unfiltered_ecg)\n\n\nload(\"../mitdb\")\n159/10:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return len(r_peaks), heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/100')\n  annotation = wfdb.rdann(str(out_dir) + '/100', \"atr\")\n  print(record.p_signal[:,1])\n\n  record2 = wfdb.rdsamp(str(out_dir)+'/100', sampto=500000)\n  unfiltered_ecg = record2[0][:, 1]\n  print(unfiltered_ecg)\n\n\nload(\"../mitdb\")\n159/11:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return len(r_peaks), heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/100')\n  annotation = wfdb.rdann(str(out_dir) + '/100', \"atr\")\n  print(record.p_signal[0][:,1])\n\n  record2 = wfdb.rdsamp(str(out_dir)+'/100', sampto=500000)\n  unfiltered_ecg = record2[0][:, 1]\n  print(unfiltered_ecg)\n\n\nload(\"../mitdb\")\n159/12:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return len(r_peaks), heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/100')\n  annotation = wfdb.rdann(str(out_dir) + '/100', \"atr\")\n  print(record.p_signal[:,1])\n\n  record2 = wfdb.rdsamp(str(out_dir)+'/100', sampto=500000)\n  unfiltered_ecg = record2[0][:, 1]\n  print(unfiltered_ecg)\n\n\nload(\"../mitdb\")\n159/13:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return len(r_peaks), heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/100')\n  annotation = wfdb.rdann(str(out_dir) + '/100', \"atr\")\n  print(record.p_signal[:,1])\n  plt.plot(record.p_signal[:,1])\n\n  record2 = wfdb.rdsamp(str(out_dir)+'/100', sampto=500000)\n  unfiltered_ecg = record2[0][:, 1]\n  print(unfiltered_ecg)\n\n\nload(\"../mitdb\")\n159/14:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return len(r_peaks), heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/100')\n  annotation = wfdb.rdann(str(out_dir) + '/100', \"atr\")\n  print(record.p_signal[:,1])\n  plt.plot(record.p_signal[:,1][:3100])\n\n  record2 = wfdb.rdsamp(str(out_dir)+'/100', sampto=500000)\n  unfiltered_ecg = record2[0][:, 1]\n  print(unfiltered_ecg)\n\n\nload(\"../mitdb\")\n159/15:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return len(r_peaks), heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/100')\n  annotation = wfdb.rdann(str(out_dir) + '/100', \"atr\")\n  print(record.p_signal[:,1])\n  #plt.plot(record.p_signal[:,1][:3100])\n\n  record2 = wfdb.rdsamp(str(out_dir)+'/100', sampto=500000)\n  unfiltered_ecg = record2[0][:, 1]\n  plt.plot(unfiltered_ecg[:3100])\n  print(unfiltered_ecg)\n\n\nload(\"../mitdb\")\n159/16:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return len(r_peaks), heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/100')\n  record_wave = record.p_signal[:,1]\n  annotation = wfdb.rdann(str(out_dir) + '/100', \"atr\")\n  atr_symbols = annotation.symbol\n  atr_sample = annotation.sample\n\n  r_peaks = define_r_peaks_indices(record_wave)\n  heartbeat_len = define_heartbeat_len(r_peaks)\n  beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  for symbol in atr_symbols:\n    label = classify_beat(symbol)\n    labels.append(label)\n\n  print(\"Beats len:\")\n  print(len(beats))\n  print(\"labels len:\")\n  print(len(labels))\n\n\nload(\"../mitdb\")\n159/17:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return len(r_peaks), heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/100')\n  record_wave = record.p_signal[:,1]\n  annotation = wfdb.rdann(str(out_dir) + '/100', \"atr\")\n  atr_symbols = annotation.symbol\n\n  r_peaks = define_r_peaks_indices(record_wave)\n  heartbeat_len = define_heartbeat_len(r_peaks)\n  beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  for symbol in atr_symbols:\n    label = classify_beat(symbol)\n    labels.append(label)\n\n  print(\"Beats len:\")\n  print(len(beats))\n  print(\"labels len:\")\n  print(len(labels))\n\n\nload(\"../mitdb\")\n159/18:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/100')\n  record_wave = record.p_signal[:,1]\n  annotation = wfdb.rdann(str(out_dir) + '/100', \"atr\")\n  atr_symbols = annotation.symbol\n\n  r_peaks = define_r_peaks_indices(record_wave)\n  heartbeat_len = define_heartbeat_len(r_peaks)\n  beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  for symbol in atr_symbols:\n    label = classify_beat(symbol)\n    labels.append(label)\n\n  print(\"Beats len:\")\n  print(len(beats))\n  print(\"labels len:\")\n  print(len(labels))\n\n\nload(\"../mitdb\")\n159/19:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n  \n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/100')\n  record_wave = record.p_signal[:,1]\n  annotation = wfdb.rdann(str(out_dir) + '/100', \"atr\")\n  atr_symbols = annotation.symbol\n\n  r_peaks = define_r_peaks_indices(record_wave)\n  heartbeat_len = define_heartbeat_len(r_peaks)\n  beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  for symbol in atr_symbols:\n    label = classify_beat(symbol)\n    labels.append(label)\n\n  print(\"Beats len:\")\n  print(len(beats))\n  print(\"labels len:\")\n  print(len(labels))\n\n\nload(\"../mitdb\")\n159/20:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/100')\n  record_wave = record.p_signal[:,1]\n  annotation = wfdb.rdann(str(out_dir) + '/100', \"atr\")\n  atr_symbols = annotation.symbol\n\n  r_peaks = define_r_peaks_indices(record_wave)\n  heartbeat_len = define_heartbeat_len(r_peaks)\n  beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  for symbol in atr_symbols:\n    label = classify_beat(symbol)\n    labels.append(label)\n\n  print(\"Beats len:\")\n  print(len(beats))\n  print(\"labels len:\")\n  print(len(labels))\n  return beats, labels\n159/21:\n\nbeats, labels = load(\"../mitdb\")\n159/22:\n\nplt.plot(beats[:3100])\n159/23:\n\nplt.plot(beats[:200])\n159/24:\n\nplt.plot(beats[0])\n159/25:\n\nplt.plot(beats[len(beats) - 1])\n159/26:\n\nplt.plot(beats[len(beats) - 4])\n159/27:\n\nplt.plot(beats[len(beats) - 3])\n159/28:\n\nplt.plot(beats[len(beats) - 2])\n159/29:\n\nplt.plot(beats[len(beats) - 1])\n159/30:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/100')\n  record_wave = record.p_signal[:,1]\n  annotation = wfdb.rdann(str(out_dir) + '/100', \"atr\")\n  atr_symbols = annotation.symbol\n\n  r_peaks = define_r_peaks_indices(record_wave)\n  heartbeat_len = define_heartbeat_len(r_peaks)\n  beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  for symbol in atr_symbols:\n    label = classify_beat(symbol)\n    labels.append(label)\n\n  beats = beats[:len(beats) - 1]\n  print(\"Beats len:\")\n  print(len(beats))\n  print(\"labels len:\")\n  print(len(labels))\n  assert len(beats) == len(labels)\n  return beats, labels[:len(beats)]\n159/31:\n\nbeats, labels = load(\"../mitdb\")\n159/32:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/100')\n  record_wave = record.p_signal[:,1]\n  annotation = wfdb.rdann(str(out_dir) + '/100', \"atr\")\n  atr_symbols = annotation.symbol\n\n  r_peaks = define_r_peaks_indices(record_wave)\n  heartbeat_len = define_heartbeat_len(r_peaks)\n  beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  for symbol in atr_symbols:\n    label = classify_beat(symbol)\n    labels.append(label)\n\n  correct_beats_len = min(len(beats) - 1, len(labels))\n\n  beats = beats[:correct_beats_len]\n  labels = labels[:correct_beats_len]\n  assert len(beats) == len(labels)\n  return beats, labels\n159/33:\n\nbeats, labels = load(\"../mitdb\")\n159/34:\n\nplt.plot(beats[10])\n159/35:\n\nplt.plot(beats[len(beats) - 1])\n159/36:\n\nplt.plot(beats[len(beats)])\n159/37:\n\nplt.plot(beats[0])\n159/38:\n\n!pip install wfdb\n!pip install py-ecg-detectors\n!pip install sklearn\n!pip install gensim\n!pip install wget\n!pip install -U scikit-learn\n159/39:\n\nimport os\nimport wfdb\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom ecgdetectors import Detectors\nfrom gensim.models import Word2Vec\n159/40:\n# Returns (p_waves, qrs_waves, t_waves)\ndef split_beats_in_p_qrt_t(beats_list):\n  c_p_waves = []\n  c_qrs_waves = []\n  c_t_waves = []\n  heart_beat_len = len(beats_list[0])\n  retreat_value = heart_beat_len // 2\n  for j in range(len(beats_list) - 1):\n    c_p_waves.append(beats_list[j][:retreat_value - 15])\n    c_qrs_waves.append(beats_list[j][retreat_value - 15:retreat_value + 15])\n    c_t_waves.append(beats_list[j][retreat_value + 15:])\n  return c_p_waves, c_qrs_waves, c_t_waves\n\n# Returns (kmeans, predicted)\ndef calculate_kmeans(waves, n_clusters, n_init):\n  c_kmeans = KMeans(init='k-means++', n_clusters=n_clusters, n_init=n_init)\n  c_kmeans.fit(waves)\n  c_predicted = c_kmeans.predict(waves)\n  return c_kmeans, c_predicted\n\n# Convert PT cluster predictions to letters\ndef convert_pt_predictions_to_letters(predictions):\n  alphabet_for_pt = {0:'a', 1:'b', 2:'c', 3:'d', 4:'e', 5:'f', 6:'g', 7:'h', 8:'i',\n                     9:'j', 10:'k', 11:'l', 12:'m', 13:'n', 14:'o', 15:'p', 16:'q', 17:'r',\n                     18:'s', 19:'t'}\n\n  def get_symbol_pt(x):\n    return alphabet_for_pt[x]\n\n  vfunc = np.vectorize(get_symbol_pt)\n\n  return vfunc(predictions)\n\n# Convert QRS clusters predictions to letters\ndef convert_qrs_predictions_to_letters(predictions):\n  alphabet_for_qrs = {0:'u', 1:'v', 2:'w', 3:'x', 4:'y', 5:'z'}\n\n\n  def get_symbol_qrs(x):\n    return alphabet_for_qrs[x]\n\n  vfunc_2 = np.vectorize(get_symbol_qrs)\n\n  return vfunc_2(predictions)\n\ndef join_waves_letters_to_words(qrs_letters, pt_letters):\n  words = []\n  signal_half_len = len(qrs_letters)//2\n  for i in range(len(qrs_letters)):\n    word = ''\n    word += pt_letters[i]\n    word += qrs_letters[i]\n    # T signal is encoded in second half\n    word += pt_letters[i + signal_half_len]\n    words.append(word)\n\n  return words\n\ndef convert_beats_to_words(beats_list):\n  (p_waves, qrs, t_waves) = split_beats_in_p_qrt_t(beats_list)\n  qrs_kmeans, qrs_predicted = calculate_kmeans(qrs, 6, 3)\n  c_pt_waves = np.array(p_waves + t_waves)\n  pt_kmeans, pt_predicted = calculate_kmeans(c_pt_waves, 20, 10)\n  \n  qrs_letters = convert_qrs_predictions_to_letters(qrs_predicted)\n  pt_letters = convert_pt_predictions_to_letters(pt_predicted)\n  return join_waves_letters_to_words(qrs_letters, pt_letters)\n\ndef teach_word2vec_or_get_from_path(words, will_be_trained, file_path):\n  \"\"\"Create and save model to file_path Word2Vec model.\n\n  :param words list of words\n  :param will_be_trained specify False if you don't want to train model any more and to make model more memory-efficient\n  :param file_path path to save model\n  \"\"\"\n  num_features = 300    # Word vector dimensionality\n  min_word_count = 40   # Minimum word count\n  num_workers = 3       # Number of threads to run in parallel\n  context = 10          # Context window size\n  downsampling = 1e-3   # Downsample setting for frequent words\n  if not os.path.exists(file_path):\n    model = Word2Vec([words,], workers=num_workers,\n                            vector_size=num_features, min_count = min_word_count,\n                            window = context, sample = downsampling)\n    if not will_be_trained:\n      model.init_sims(replace=True)\n    model.save(file_path)\n  else:\n    model = Word2Vec.load(file_path)\n\n  return model\n159/41:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/100')\n  record_wave = record.p_signal[:,1]\n  annotation = wfdb.rdann(str(out_dir) + '/100', \"atr\")\n  atr_symbols = annotation.symbol\n\n  r_peaks = define_r_peaks_indices(record_wave)\n  heartbeat_len = define_heartbeat_len(r_peaks)\n  beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  for symbol in atr_symbols:\n    label = classify_beat(symbol)\n    labels.append(label)\n\n  correct_beats_len = min(len(beats) - 1, len(labels))\n\n  beats = beats[:correct_beats_len]\n  labels = labels[:correct_beats_len]\n  assert len(beats) == len(labels)\n  return beats, labels\n159/42:\n\nbeats, labels = load(\"../mitdb\")\n159/43:\n\nwords = convert_beats_to_words(beats)\n159/44:\n\nlen(words)\n159/45:\n\nlen(beats)\n159/46:\n# Returns (p_waves, qrs_waves, t_waves)\ndef split_beats_in_p_qrt_t(beats_list):\n  c_p_waves = []\n  c_qrs_waves = []\n  c_t_waves = []\n  heart_beat_len = len(beats_list[0])\n  retreat_value = heart_beat_len // 2\n  for j in range(len(beats_list)):\n    c_p_waves.append(beats_list[j][:retreat_value - 15])\n    c_qrs_waves.append(beats_list[j][retreat_value - 15:retreat_value + 15])\n    c_t_waves.append(beats_list[j][retreat_value + 15:])\n  return c_p_waves, c_qrs_waves, c_t_waves\n\n# Returns (kmeans, predicted)\ndef calculate_kmeans(waves, n_clusters, n_init):\n  c_kmeans = KMeans(init='k-means++', n_clusters=n_clusters, n_init=n_init)\n  c_kmeans.fit(waves)\n  c_predicted = c_kmeans.predict(waves)\n  return c_kmeans, c_predicted\n\n# Convert PT cluster predictions to letters\ndef convert_pt_predictions_to_letters(predictions):\n  alphabet_for_pt = {0:'a', 1:'b', 2:'c', 3:'d', 4:'e', 5:'f', 6:'g', 7:'h', 8:'i',\n                     9:'j', 10:'k', 11:'l', 12:'m', 13:'n', 14:'o', 15:'p', 16:'q', 17:'r',\n                     18:'s', 19:'t'}\n\n  def get_symbol_pt(x):\n    return alphabet_for_pt[x]\n\n  vfunc = np.vectorize(get_symbol_pt)\n\n  return vfunc(predictions)\n\n# Convert QRS clusters predictions to letters\ndef convert_qrs_predictions_to_letters(predictions):\n  alphabet_for_qrs = {0:'u', 1:'v', 2:'w', 3:'x', 4:'y', 5:'z'}\n\n\n  def get_symbol_qrs(x):\n    return alphabet_for_qrs[x]\n\n  vfunc_2 = np.vectorize(get_symbol_qrs)\n\n  return vfunc_2(predictions)\n\ndef join_waves_letters_to_words(qrs_letters, pt_letters):\n  words = []\n  signal_half_len = len(qrs_letters)//2\n  for i in range(len(qrs_letters)):\n    word = ''\n    word += pt_letters[i]\n    word += qrs_letters[i]\n    # T signal is encoded in second half\n    word += pt_letters[i + signal_half_len]\n    words.append(word)\n\n  return words\n\ndef convert_beats_to_words(beats_list):\n  (p_waves, qrs, t_waves) = split_beats_in_p_qrt_t(beats_list)\n  qrs_kmeans, qrs_predicted = calculate_kmeans(qrs, 6, 3)\n  c_pt_waves = np.array(p_waves + t_waves)\n  pt_kmeans, pt_predicted = calculate_kmeans(c_pt_waves, 20, 10)\n\n  qrs_letters = convert_qrs_predictions_to_letters(qrs_predicted)\n  pt_letters = convert_pt_predictions_to_letters(pt_predicted)\n  return join_waves_letters_to_words(qrs_letters, pt_letters)\n\ndef teach_word2vec_or_get_from_path(words, will_be_trained, file_path):\n  \"\"\"Create and save model to file_path Word2Vec model.\n\n  :param words list of words\n  :param will_be_trained specify False if you don't want to train model any more and to make model more memory-efficient\n  :param file_path path to save model\n  \"\"\"\n  num_features = 300    # Word vector dimensionality\n  min_word_count = 40   # Minimum word count\n  num_workers = 3       # Number of threads to run in parallel\n  context = 10          # Context window size\n  downsampling = 1e-3   # Downsample setting for frequent words\n  if not os.path.exists(file_path):\n    model = Word2Vec([words,], workers=num_workers,\n                            vector_size=num_features, min_count = min_word_count,\n                            window = context, sample = downsampling)\n    if not will_be_trained:\n      model.init_sims(replace=True)\n    model.save(file_path)\n  else:\n    model = Word2Vec.load(file_path)\n\n  return model\n159/47:\n\nwords = convert_beats_to_words(beats)\n159/48:\n\nlen(beats)\n159/49:\n\nlen(words)\n159/50:\n\nabnormal_count = 0\nfor label in labels:\n  if label == 1:\n    abnormal_count = abnormal_count + 1\n\nabnormal_count\n159/51:\n\nabnormal_count = 0\nfor label in labels:\n  if label == 1:\n    abnormal_count = abnormal_count + 1\n\nabnormal_count / len(beats)\n159/52:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/101')\n  record_wave = record.p_signal[:,1]\n  annotation = wfdb.rdann(str(out_dir) + '/101', \"atr\")\n  atr_symbols = annotation.symbol\n\n  r_peaks = define_r_peaks_indices(record_wave)\n  heartbeat_len = define_heartbeat_len(r_peaks)\n  beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  for symbol in atr_symbols:\n    label = classify_beat(symbol)\n    labels.append(label)\n\n  correct_beats_len = min(len(beats) - 1, len(labels))\n\n  beats = beats[:correct_beats_len]\n  labels = labels[:correct_beats_len]\n  assert len(beats) == len(labels)\n  return beats, labels\n159/53:\n\nbeats, labels = load(\"../mitdb\")\n159/54:\n\nwords = convert_beats_to_words(beats)\n159/55:\n\nabnormal_count = 0\nfor label in labels:\n  if label == 1:\n    abnormal_count = abnormal_count + 1\n\nabnormal_count / len(beats)\n159/56:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/102')\n  record_wave = record.p_signal[:,1]\n  annotation = wfdb.rdann(str(out_dir) + '/102', \"atr\")\n  atr_symbols = annotation.symbol\n\n  r_peaks = define_r_peaks_indices(record_wave)\n  heartbeat_len = define_heartbeat_len(r_peaks)\n  beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  for symbol in atr_symbols:\n    label = classify_beat(symbol)\n    labels.append(label)\n\n  correct_beats_len = min(len(beats) - 1, len(labels))\n\n  beats = beats[:correct_beats_len]\n  labels = labels[:correct_beats_len]\n  assert len(beats) == len(labels)\n  return beats, labels\n159/57:\n\nbeats, labels = load(\"../mitdb\")\n159/58:\n\nabnormal_count = 0\nfor label in labels:\n  if label == 1:\n    abnormal_count = abnormal_count + 1\n\nabnormal_count / len(beats)\n159/59:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/102')\n  record_wave = record.p_signal[:,1]\n  annotation = wfdb.rdann(str(out_dir) + '/102', \"atr\")\n  atr_symbols = annotation.symbol\n  print(record.record_name)\n\n  r_peaks = define_r_peaks_indices(record_wave)\n  heartbeat_len = define_heartbeat_len(r_peaks)\n  beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  for symbol in atr_symbols:\n    label = classify_beat(symbol)\n    labels.append(label)\n\n  correct_beats_len = min(len(beats) - 1, len(labels))\n\n  beats = beats[:correct_beats_len]\n  labels = labels[:correct_beats_len]\n  assert len(beats) == len(labels)\n  return beats, labels\n159/60:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  record_wave = record.p_signal[:,1]\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n  print(record.record_name)\n\n  r_peaks = define_r_peaks_indices(record_wave)\n  heartbeat_len = define_heartbeat_len(r_peaks)\n  beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  for symbol in atr_symbols:\n    label = classify_beat(symbol)\n    labels.append(label)\n\n  correct_beats_len = min(len(beats) - 1, len(labels))\n\n  beats = beats[:correct_beats_len]\n  labels = labels[:correct_beats_len]\n  assert len(beats) == len(labels)\n  return beats, labels\n159/61:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/62:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  record_wave = record.p_signal[:,1]\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n  print(record.fs)\n\n  r_peaks = define_r_peaks_indices(record_wave)\n  heartbeat_len = define_heartbeat_len(r_peaks)\n  beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  for symbol in atr_symbols:\n    label = classify_beat(symbol)\n    labels.append(label)\n\n  correct_beats_len = min(len(beats) - 1, len(labels))\n\n  beats = beats[:correct_beats_len]\n  labels = labels[:correct_beats_len]\n  assert len(beats) == len(labels)\n  return beats, labels\n159/63:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/64:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/65:\n\nabnormal_count = 0\nfor label in labels:\n  if label == 1:\n    abnormal_count = abnormal_count + 1\n\nabnormal_count / len(beats)\n159/66:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  record_wave = record.p_signal[:,1]\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n\n  r_peaks = define_r_peaks_indices(record_wave)\n  heartbeat_len = define_heartbeat_len(r_peaks)\n  beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  for symbol in atr_symbols:\n    label = classify_beat(symbol)\n    labels.append(label)\n\n  correct_beats_len = min(len(beats) - 1, len(labels))\n\n  beats = beats[:correct_beats_len]\n  labels = labels[:correct_beats_len]\n  assert len(beats) == len(labels)\n  return beats, labels\n159/67:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/68:\n\nwords = convert_beats_to_words(beats)\nword2Vec_model = teach_word2vec_or_get_from_path(words, False, 'saves/model_102')\n159/69:\n\ndef teach_word2vec_or_get_from_path(words, will_be_trained, file_path):\n  \"\"\"Create and save model to file_path Word2Vec model.\n\n  :param words list of words\n  :param will_be_trained specify False if you don't want to train model any more and to make model more memory-efficient\n  :param file_path path to save model\n  \"\"\"\n  num_features = 300    # Word vector dimensionality\n  min_word_count = 5   # Minimum word count\n  num_workers = 3       # Number of threads to run in parallel\n  context = 10          # Context window size\n  downsampling = 1e-3   # Downsample setting for frequent words\n  if not os.path.exists(file_path):\n    model = Word2Vec([words,], workers=num_workers,\n                     vector_size=num_features, min_count = min_word_count,\n                     window = context, sample = downsampling)\n    if not will_be_trained:\n      model.init_sims(replace=True)\n    model.save(file_path)\n  else:\n    model = Word2Vec.load(file_path)\n\n  return model\n159/70:\n\nwords = convert_beats_to_words(beats)\n159/71: word2Vec_model = teach_word2vec_or_get_from_path(words, False, 'saves/model_102')\n159/72: word2Vec_model = teach_word2vec_or_get_from_path(words, False, 'model_102')\n159/73:\nnum_features = 300\nword2Vec_model = teach_word2vec_or_get_from_path(words, False, 'model_102', num_features)\n159/74:\n\ndef teach_word2vec_or_get_from_path(words, will_be_trained, file_path, num_features):\n  \"\"\"Create and save model to file_path Word2Vec model.\n\n  :param words list of words\n  :param will_be_trained specify False if you don't want to train model any more and to make model more memory-efficient\n  :param file_path path to save model\n  \"\"\"\n  min_word_count = 5   # Minimum word count\n  num_workers = 3       # Number of threads to run in parallel\n  context = 10          # Context window size\n  downsampling = 1e-3   # Downsample setting for frequent words\n  if not os.path.exists(file_path):\n    model = Word2Vec([words,], workers=num_workers,\n                     vector_size=num_features, min_count = min_word_count,\n                     window = context, sample = downsampling)\n    if not will_be_trained:\n      model.init_sims(replace=True)\n    model.save(file_path)\n  else:\n    model = Word2Vec.load(file_path)\n\n  return model\n159/75:\nnum_features = 300\nword2Vec_model = teach_word2vec_or_get_from_path(words, False, 'model_102_v2', num_features)\n159/76:\n\ndef create_beats_features(beats, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(beats),num_features), dtype='float32')\n\n  for i in range(len(beats)):\n    review_feature_vecs[i] = word2Vec_model[words[i]]\n\n  return review_feature_vecs\n\nword2Vec_model.index2word\n159/77:\n\ndef create_beats_features(beats, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(beats),num_features), dtype='float32')\n\n  for i in range(len(beats)):\n    review_feature_vecs[i] = word2Vec_model[words[i]]\n\n  return review_feature_vecs\n\nword2Vec_model.vector_size\n159/78:\n\ndef create_beats_features(beats, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(beats),num_features), dtype='float32')\n\n  for i in range(len(beats)):\n    review_feature_vecs[i] = word2Vec_model[words[i]]\n\n  return review_feature_vecs\n\nword2Vec_model.max_vocab_size\n159/79:\n\ndef create_beats_features(beats, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(beats),num_features), dtype='float32')\n\n  for i in range(len(beats)):\n    review_feature_vecs[i] = word2Vec_model[words[i]]\n\n  return review_feature_vecs\n\nword2Vec_model.max_vocab_size\n159/80:\n\ndef create_beats_features(beats, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(beats),num_features), dtype='float32')\n\n  for i in range(len(beats)):\n    review_feature_vecs[i] = word2Vec_model[words[i]]\n\n  return review_feature_vecs\n\nword2Vec_model.raw_vocab\n159/81:\n\ndef create_beats_features(beats, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(beats),num_features), dtype='float32')\n\n  for i in range(len(beats)):\n    review_feature_vecs[i] = word2Vec_model[words[i]]\n\n  return review_feature_vecs\n\nword2Vec_model.wv.key_to_index\n159/82:\n\ndef create_beats_features(beats, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(beats),num_features), dtype='float32')\n\n  for i in range(len(beats)):\n    review_feature_vecs[i] = word2Vec_model[words[i]]\n\n  return review_feature_vecs\n\nword2Vec_model.wv['twt']\n159/83:\n\ndef create_beats_features(beats, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(beats),num_features), dtype='float32')\n\n  for i in range(len(beats)):\n    review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n\n  return review_feature_vecs\n\nlen(beats)\n159/84:\n\ndef create_beats_features(beats, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(beats),num_features), dtype='float32')\n\n  for i in range(len(beats)):\n    review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n\n  return review_feature_vecs\n\nlen(word2Vec_model.vw)\n159/85:\n\ndef create_beats_features(beats, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(beats),num_features), dtype='float32')\n\n  for i in range(len(beats)):\n    review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n\n  return review_feature_vecs\n\nword2Vec_model.vw\n159/86:\n\ndef create_beats_features(beats, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(beats),num_features), dtype='float32')\n\n  for i in range(len(beats)):\n    review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n\n  return review_feature_vecs\n\nword2Vec_model.wv\n159/87:\n\ndef create_beats_features(beats, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(beats),num_features), dtype='float32')\n\n  for i in range(len(beats)):\n    review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n\n  return review_feature_vecs\n\nword2Vec_model.wv.key_to_index\n159/88:\n\ndef create_beats_features(beats, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(beats),num_features), dtype='float32')\n\n  for i in range(len(beats)):\n    review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n\n  return review_feature_vecs\n\nword2Vec_model.wv['abc']\n159/89:\n\ndef create_beats_features(beats, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(beats),num_features), dtype='float32')\n\n  for i in range(len(beats)):\n    review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n\n  return review_feature_vecs\n\nword2Vec_model.wv\n159/90:\n\ndef create_beats_features(beats, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(beats),num_features), dtype='float32')\n\n  for i in range(len(beats)):\n    review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n\n  return review_feature_vecs\n\nword2Vec_model.wv.key_to_index\n159/91:\n\ndef create_beats_features(beats, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(beats),num_features), dtype='float32')\n\n  for i in range(len(beats)):\n    review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n\n  return review_feature_vecs\n\nword2Vec_model.wv['kvb']\n159/92:\n\ndef create_beats_features(beats, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(beats),num_features), dtype='float32')\n\n  index2word_set =set(word2Vec_model.wv.index2word)\n\n  for i in range(len(beats)):\n    if index2word_set in index2word_set:\n      review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n    else:\n      review_feature_vecs[i] = word2Vec_model\n\n  return review_feature_vecs\n\nword2Vec_model.wv.most_similar('abc')\n159/93:\n\ndef create_beats_features(beats, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(beats),num_features), dtype='float32')\n\n  index2word_set =set(word2Vec_model.wv.index2word)\n\n  for i in range(len(beats)):\n    if index2word_set in index2word_set:\n      review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n    else:\n      review_feature_vecs[i] = word2Vec_model\n\n  return review_feature_vecs\n\n#word2Vec_model.wv.most_similar('abc')\nwords\n159/94:\n\ndef create_beats_features(beats, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(beats),num_features), dtype='float32')\n\n  index2word_set =set(word2Vec_model.wv.index2word)\n\n  for i in range(len(beats)):\n    if index2word_set in index2word_set:\n      review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n    else:\n      review_feature_vecs[i] = word2Vec_model\n\n  return review_feature_vecs\n\nword2Vec_model.wv.most_similar('gut')\n#words\n159/95:\n\ndef create_beats_features(words, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(words),num_features), dtype='float32')\n\n  index2word_set =set(word2Vec_model.wv.index2word)\n\n  for i in range(len(words)):\n    if index2word_set in index2word_set:\n      review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n    else:\n      most_similar = word2Vec_model.wv.most_similar(words[i])\n      review_feature_vecs[i] = np.median(dict(most_similar).values())\n\n  return review_feature_vecs\n\ncreate_beats_features(words, num_features, word2Vec_model)\n#words\n159/96:\n\ndef create_beats_features(words, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(words),num_features), dtype='float32')\n\n  index2word_set =set(word2Vec_model.wv.index_to_key)\n\n  for i in range(len(words)):\n    if index2word_set in index2word_set:\n      review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n    else:\n      most_similar = word2Vec_model.wv.most_similar(words[i])\n      review_feature_vecs[i] = np.median(dict(most_similar).values())\n\n  return review_feature_vecs\n\ncreate_beats_features(words, num_features, word2Vec_model)\n#words\n159/97:\n\ndef create_beats_features(words, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(words),num_features), dtype='float32')\n\n  index2word_set =set(word2Vec_model.wv.index_to_key)\n\n  for i in range(len(words)):\n    if index2word_set in index2word_set:\n      review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n    else:\n      most_similar = word2Vec_model.wv.most_similar(words[i])\n      review_feature_vecs[i] = np.median(list(dict(most_similar).values()))\n\n  return review_feature_vecs\n\ncreate_beats_features(words, num_features, word2Vec_model)\n#words\n159/98:\nnum_features = 300\nword2Vec_model = teach_word2vec_or_get_from_path(words, False, 'model_102_v2', num_features)\n159/99:\n\ndef create_beats_features(words, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(words),num_features), dtype='float32')\n\n  index2word_set =set(word2Vec_model.wv.index_to_key)\n\n  for i in range(len(words)):\n    if index2word_set in index2word_set:\n      review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n    else:\n      most_similar = word2Vec_model.wv.most_similar(words[i])\n      review_feature_vecs[i] = np.median(list(dict(most_similar).values()))\n\n  return review_feature_vecs\n\ncreate_beats_features(words, num_features, word2Vec_model)\n#words\n159/100:\n\ndef teach_word2vec_or_get_from_path(words, will_be_trained, file_path, num_features):\n  \"\"\"Create and save model to file_path Word2Vec model.\n\n  :param words list of words\n  :param will_be_trained specify False if you don't want to train model any more and to make model more memory-efficient\n  :param file_path path to save model\n  \"\"\"\n  min_word_count = 1   # Minimum word count\n  num_workers = 3       # Number of threads to run in parallel\n  context = 10          # Context window size\n  downsampling = 1e-3   # Downsample setting for frequent words\n  if not os.path.exists(file_path):\n    model = Word2Vec([words,], workers=num_workers,\n                     vector_size=num_features, min_count = min_word_count,\n                     window = context, sample = downsampling)\n    if not will_be_trained:\n      model.init_sims(replace=True)\n    model.save(file_path)\n  else:\n    model = Word2Vec.load(file_path)\n\n  return model\n159/101:\nnum_features = 300\nword2Vec_model = teach_word2vec_or_get_from_path(words, False, 'model_102_v2', num_features)\n159/102:\n\ndef create_beats_features(words, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(words),num_features), dtype='float32')\n\n  index2word_set =set(word2Vec_model.wv.index_to_key)\n\n  for i in range(len(words)):\n    if index2word_set in index2word_set:\n      review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n    else:\n      most_similar = word2Vec_model.wv.most_similar(words[i])\n      review_feature_vecs[i] = np.median(list(dict(most_similar).values()))\n\n  return review_feature_vecs\n\nfor word in words:\n  if word2Vec_model.wv.most_similar(word)\n\ncreate_beats_features(words, num_features, word2Vec_model)\n#words\n159/103:\n\ndef create_beats_features(words, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(words),num_features), dtype='float32')\n\n  index2word_set =set(word2Vec_model.wv.index_to_key)\n\n  for i in range(len(words)):\n    if index2word_set in index2word_set:\n      review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n    else:\n      most_similar = word2Vec_model.wv.most_similar(words[i])\n      review_feature_vecs[i] = np.median(list(dict(most_similar).values()))\n\n  return review_feature_vecs\n\ncreate_beats_features(words, num_features, word2Vec_model)\n#words\n159/104:\n\ndef create_beats_features(words, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(words),num_features), dtype='float32')\n\n  index2word_set =set(word2Vec_model.wv.index_to_key)\n\n  for i in range(len(words)):\n    if index2word_set in index2word_set:\n      review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n    else:\n      most_similar = word2Vec_model.wv.most_similar(words[i])\n      review_feature_vecs[i] = np.median(list(dict(most_similar).values()))\n\n  return review_feature_vecs\n\ncounter = 0\nfor word in words:\n  if word not in word2Vec_model.wv.vocab:\n    counter = counter + 1\n\n\n#create_beats_features(words, num_features, word2Vec_model)\ncounter\n159/105:\n\ndef create_beats_features(words, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(words),num_features), dtype='float32')\n\n  index2word_set =set(word2Vec_model.wv.index_to_key)\n\n  for i in range(len(words)):\n    if index2word_set in index2word_set:\n      review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n    else:\n      most_similar = word2Vec_model.wv.most_similar(words[i])\n      review_feature_vecs[i] = np.median(list(dict(most_similar).values()))\n\n  return review_feature_vecs\n\ncounter = 0\nfor word in words:\n  if word not in word2Vec_model.key_to_index:\n    counter = counter + 1\n\n\n#create_beats_features(words, num_features, word2Vec_model)\ncounter\n159/106:\n\ndef create_beats_features(words, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(words),num_features), dtype='float32')\n\n  index2word_set =set(word2Vec_model.wv.index_to_key)\n\n  for i in range(len(words)):\n    if index2word_set in index2word_set:\n      review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n    else:\n      most_similar = word2Vec_model.wv.most_similar(words[i])\n      review_feature_vecs[i] = np.median(list(dict(most_similar).values()))\n\n  return review_feature_vecs\n\ncounter = 0\nfor word in words:\n  if word not in word2Vec_model.wv.key_to_index:\n    counter = counter + 1\n\n\n#create_beats_features(words, num_features, word2Vec_model)\ncounter\n159/107:\n\ndef create_beats_features(words, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(words),num_features), dtype='float32')\n\n  index2word_set =set(word2Vec_model.wv.index_to_key)\n\n  for i in range(len(words)):\n    if index2word_set in index2word_set:\n      review_feature_vecs[i] = word2Vec_model.wv[words[i]]\n    else:\n      most_similar = word2Vec_model.wv.most_similar(words[i])\n      review_feature_vecs[i] = np.median(list(dict(most_similar).values()))\n\n  return review_feature_vecs\n\ncounter = 0\nfor word in words:\n  if word not in word2Vec_model.wv.key_to_index:\n    counter = counter + 1\n\n\n#create_beats_features(words, num_features, word2Vec_model)\ncounter / len(words)\n159/108:\n\ndef create_beats_features(beats, words, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(words),num_features), dtype='float32')\n\n  index2word_set =set(word2Vec_model.wv.index_to_key)\n\n  indices = []\n  valid_words_count = 0\n  for i in range(len(words)):\n    cur_word = words[i]\n    if cur_word in index2word_set:\n      review_feature_vecs[valid_words_count] = word2Vec_model.wv[cur_word]\n      valid_words_count = valid_words_count + 1\n      indices.append(i)\n\n  return indices, review_feature_vecs[:valid_words_count]\n\n\nvalid_beats_indices, features = create_beats_features(beats, words, num_features, word2Vec_model)\n159/109:\n\nvalid_beats_indices\n159/110:\n\nlen(valid_beats_indices)\n159/111:\nfeatures_labels = [labels[i] for i in valid_beats_indices]\nprint(len(features_labels))\nfeatures.shape\n159/112:\n\nforest = RandomForestClassifier(n_estimators = 100)\nforest = forest.fit(features, features_labels)\n159/113:\n\nimport os\nimport wfdb\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\nfrom ecgdetectors import Detectors\nfrom gensim.models import Word2Vec\n159/114:\n\nforest = RandomForestClassifier(n_estimators = 100)\nforest = forest.fit(features, features_labels)\n159/115:\nfeatures_labels = [labels[i] for i in valid_beats_indices][1:]\nfeatures = features[1:]\n159/116:\n\nforest = RandomForestClassifier(n_estimators = 100)\nforest = forest.fit(features, features_labels)\n159/117:\ncounter = 0\nfor label in features_labels:\n  if label is None:\n    counter = counter + 1\ncounter\n159/118:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  record_wave = record.p_signal[:,1]\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n\n  r_peaks = define_r_peaks_indices(record_wave)\n  heartbeat_len = define_heartbeat_len(r_peaks)\n  beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  valid_beats = []\n  for i in range(len(atr_symbols)):\n    label = classify_beat(atr_symbols[i])\n    if label is not None:\n      labels.append(label)\n      valid_beats.append(beats[i])\n\n  correct_beats_len = min(len(beats) - 1, len(labels))\n\n  beats = beats[:correct_beats_len]\n  labels = labels[:correct_beats_len]\n  assert len(beats) == len(labels)\n  return beats, labels\n159/119:\n\ndef teach_word2vec_or_get_from_path(words, will_be_trained, file_path, num_features, use_cached = False):\n  \"\"\"Create and save model to file_path Word2Vec model.\n\n  :param words list of words\n  :param will_be_trained specify False if you don't want to train model any more and to make model more memory-efficient\n  :param file_path path to save model\n  \"\"\"\n  min_word_count = 1   # Minimum word count\n  num_workers = 3       # Number of threads to run in parallel\n  context = 10          # Context window size\n  downsampling = 1e-3   # Downsample setting for frequent words\n  if not (os.path.exists(file_path) and use_cached):\n    model = Word2Vec([words,], workers=num_workers,\n                     vector_size=num_features, min_count = min_word_count,\n                     window = context, sample = downsampling)\n    if not will_be_trained:\n      model.init_sims(replace=True)\n    model.save(file_path)\n  else:\n    model = Word2Vec.load(file_path)\n\n  return model\n159/120:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/121:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef get_sequence(signal, beat_loc, window_sec, fs):\n  window_one_side = window_sec * fs\n  beat_start = beat_loc - window_one_side\n  beat_end = beat_loc + window_one_side\n  if beat_end < signal.shape[0]:\n    sequence = signal[beat_start:beat_end, 0]\n    return sequence.reshape(1, -1, 1)\n  else:\n    return np.array([])\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  record_wave = record.p_signal[:,1]\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n  atr_samples = annotation.sample\n  fs = record.fs\n\n  # r_peaks = define_r_peaks_indices(record_wave)\n  # heartbeat_len = define_heartbeat_len(r_peaks)\n  # beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  valid_beats = []\n  window_sec = 3\n  for i, i_sample in enumerate(atr_samples):\n    label = classify_beat(atr_symbols[i])\n    if label is not None:\n      sequence = get_sequence(record_wave, i_sample, window_sec, fs)\n      if sequence.size > 0:\n        labels.append(label)\n        valid_beats.append(sequence)\n\n  assert len(beats) == len(labels)\n  return beats, labels\n159/122:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/123:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef get_sequence(signal, beat_loc, window_sec, fs):\n  window_one_side = window_sec * fs\n  beat_start = beat_loc - window_one_side\n  beat_end = beat_loc + window_one_side\n  if beat_end < signal.shape[0]:\n    sequence = signal[beat_start:beat_end, 0]\n    return sequence.reshape(1, -1, 1)\n  else:\n    return np.array([])\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n  atr_samples = annotation.sample\n  fs = record.fs\n  scaler = StandardScaler()\n  signal = scaler.fit_transform(record.p_signal)\n\n  # r_peaks = define_r_peaks_indices(record_wave)\n  # heartbeat_len = define_heartbeat_len(r_peaks)\n  # beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  valid_beats = []\n  window_sec = 3\n  for i, i_sample in enumerate(atr_samples):\n    label = classify_beat(atr_symbols[i])\n    if label is not None:\n      sequence = get_sequence(signal, i_sample, window_sec, fs)\n      if sequence.size > 0:\n        labels.append(label)\n        valid_beats.append(sequence)\n\n  assert len(beats) == len(labels)\n  return beats, labels\n159/124:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/125:\n\nimport os\nimport wfdb\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom ecgdetectors import Detectors\nfrom gensim.models import Word2Vec\n159/126:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/127:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef get_sequence(signal, beat_loc, window_sec, fs):\n  window_one_side = window_sec * fs\n  beat_start = beat_loc - window_one_side\n  beat_end = beat_loc + window_one_side\n  if beat_end < signal.shape[0]:\n    sequence = signal[beat_start:beat_end, 0]\n    return sequence.reshape(1, -1, 1)\n  else:\n    return np.array([])\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n  atr_samples = annotation.sample\n  fs = record.fs\n  scaler = StandardScaler()\n  signal = scaler.fit_transform(record.p_signal)\n\n  # r_peaks = define_r_peaks_indices(record_wave)\n  # heartbeat_len = define_heartbeat_len(r_peaks)\n  # beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  valid_beats = []\n  window_sec = 3\n  for i, i_sample in enumerate(atr_samples):\n    label = classify_beat(atr_symbols[i])\n    if label is not None:\n      sequence = get_sequence(signal, i_sample, window_sec, fs)\n      if sequence.size > 0:\n        labels.append(label)\n        valid_beats.append(sequence)\n\n  assert len(valid_beats) == len(labels)\n  return valid_beats, labels\n",
      "159/128:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/129:\n\nwords = convert_beats_to_words(beats)\n159/130:\nplt.plot(beats[0])\nwords = convert_beats_to_words(beats)\n159/131:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef get_sequence(signal, beat_loc, window_sec, fs):\n  window_one_side = window_sec * fs\n  beat_start = beat_loc - window_one_side\n  beat_end = beat_loc + window_one_side\n  if beat_end < signal.shape[0]:\n    sequence = signal[beat_start:beat_end, 0]\n    return sequence.reshape(1, -1, 1)\n  else:\n    return np.array([])\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n  atr_samples = annotation.sample\n  fs = record.fs\n  scaler = StandardScaler()\n  signal = scaler.fit_transform(record.p_signal)\n  sequence = get_sequence(signal, atr_samples[1], 3, fs)\n  print(sequence.shape)\n\n  # r_peaks = define_r_peaks_indices(record_wave)\n  # heartbeat_len = define_heartbeat_len(r_peaks)\n  # beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  valid_beats = []\n  window_sec = 3\n  for i, i_sample in enumerate(atr_samples):\n    label = classify_beat(atr_symbols[i])\n    if label is not None:\n      sequence = get_sequence(signal, i_sample, window_sec, fs)\n      if sequence.size > 0:\n        labels.append(label)\n        valid_beats.append(sequence)\n\n  assert len(valid_beats) == len(labels)\n  return valid_beats, labels\n159/132:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/133:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/134:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef get_sequence(signal, beat_loc, window_sec, fs):\n  window_one_side = window_sec * fs\n  beat_start = beat_loc - window_one_side\n  beat_end = beat_loc + window_one_side\n  if beat_end < signal.shape[0]:\n    sequence = signal[beat_start:beat_end, 0]\n    return sequence.reshape(1, -1, 1)\n  else:\n    return np.array([])\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n  atr_samples = annotation.sample\n  fs = record.fs\n  scaler = StandardScaler()\n  signal = scaler.fit_transform(record.p_signal)\n  sequence = get_sequence(signal, atr_samples[1], 3, fs)\n  print(sequence)\n  plt.plot(sequence[0])\n\n  # r_peaks = define_r_peaks_indices(record_wave)\n  # heartbeat_len = define_heartbeat_len(r_peaks)\n  # beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  valid_beats = []\n  window_sec = 3\n  for i, i_sample in enumerate(atr_samples):\n    label = classify_beat(atr_symbols[i])\n    if label is not None:\n      sequence = get_sequence(signal, i_sample, window_sec, fs)\n      if sequence.size > 0:\n        labels.append(label)\n        valid_beats.append(sequence)\n\n  assert len(valid_beats) == len(labels)\n  return valid_beats, labels\n159/135:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/136:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef get_sequence(signal, beat_loc, window_sec, fs):\n  window_one_side = window_sec * fs\n  beat_start = beat_loc - window_one_side\n  beat_end = beat_loc + window_one_side\n  if beat_end < signal.shape[0]:\n    sequence = signal[beat_start:beat_end, 0]\n    return sequence.reshape(1, -1, 1)\n  else:\n    return np.array([])\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n  atr_samples = annotation.sample\n  fs = record.fs\n  scaler = StandardScaler()\n  signal = scaler.fit_transform(record.p_signal)\n  sequence = get_sequence(signal, atr_samples[2], 3, fs)\n  print(sequence)\n  plt.plot(sequence[0])\n\n  # r_peaks = define_r_peaks_indices(record_wave)\n  # heartbeat_len = define_heartbeat_len(r_peaks)\n  # beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  valid_beats = []\n  window_sec = 3\n  for i, i_sample in enumerate(atr_samples):\n    label = classify_beat(atr_symbols[i])\n    if label is not None:\n      sequence = get_sequence(signal, i_sample, window_sec, fs)\n      if sequence.size > 0:\n        labels.append(label)\n        valid_beats.append(sequence)\n\n  assert len(valid_beats) == len(labels)\n  return valid_beats, labels\n159/137:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/138:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef get_sequence(signal, beat_loc, window_sec, fs):\n  window_one_side = window_sec * fs\n  beat_start = beat_loc - window_one_side\n  beat_end = beat_loc + window_one_side\n  if beat_end < signal.shape[0]:\n    sequence = signal[beat_start:beat_end, 0]\n    return sequence.reshape(1, -1, 1)\n  else:\n    return np.array([])\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n  atr_samples = annotation.sample\n  fs = record.fs\n  scaler = StandardScaler()\n  signal = scaler.fit_transform(record.p_signal)\n  sequence = get_sequence(signal, atr_samples[3], 3, fs)\n  print(sequence)\n  plt.plot(sequence[0])\n\n  # r_peaks = define_r_peaks_indices(record_wave)\n  # heartbeat_len = define_heartbeat_len(r_peaks)\n  # beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  valid_beats = []\n  window_sec = 3\n  for i, i_sample in enumerate(atr_samples):\n    label = classify_beat(atr_symbols[i])\n    if label is not None:\n      sequence = get_sequence(signal, i_sample, window_sec, fs)\n      if sequence.size > 0:\n        labels.append(label)\n        valid_beats.append(sequence)\n\n  assert len(valid_beats) == len(labels)\n  return valid_beats, labels\n159/139:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/140:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef get_sequence(signal, beat_loc, window_sec, fs):\n  window_one_side = window_sec * fs\n  beat_start = beat_loc - window_one_side\n  beat_end = beat_loc + window_one_side\n  if beat_end < signal.shape[0]:\n    sequence = signal[beat_start:beat_end, 0]\n    return sequence.reshape(1, -1, 1)\n  else:\n    return np.array([])\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n  atr_samples = annotation.sample\n  fs = record.fs\n  scaler = StandardScaler()\n  signal = scaler.fit_transform(record.p_signal)\n  sequence = get_sequence(signal, atr_samples[10], 3, fs)\n  print(sequence)\n  plt.plot(sequence[0])\n\n  # r_peaks = define_r_peaks_indices(record_wave)\n  # heartbeat_len = define_heartbeat_len(r_peaks)\n  # beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  valid_beats = []\n  window_sec = 3\n  for i, i_sample in enumerate(atr_samples):\n    label = classify_beat(atr_symbols[i])\n    if label is not None:\n      sequence = get_sequence(signal, i_sample, window_sec, fs)\n      if sequence.size > 0:\n        labels.append(label)\n        valid_beats.append(sequence)\n\n  assert len(valid_beats) == len(labels)\n  return valid_beats, labels\n159/141:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/142:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef get_sequence(signal, beat_loc, window_sec, fs):\n  window_one_side = window_sec * fs\n  beat_start = beat_loc - window_one_side\n  beat_end = beat_loc + window_one_side\n  if beat_end < signal.shape[0]:\n    sequence = signal[beat_start:beat_end, 0]\n    return sequence.reshape(1, -1, 1)\n  else:\n    return np.array([])\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n  atr_samples = annotation.sample\n  fs = record.fs\n  scaler = StandardScaler()\n  signal = scaler.fit_transform(record.p_signal)\n  sequence = get_sequence(signal, atr_samples[10], 3, fs)\n  print(signal.shape)\n  plt.plot(sequence[0])\n\n  # r_peaks = define_r_peaks_indices(record_wave)\n  # heartbeat_len = define_heartbeat_len(r_peaks)\n  # beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  valid_beats = []\n  window_sec = 3\n  for i, i_sample in enumerate(atr_samples):\n    label = classify_beat(atr_symbols[i])\n    if label is not None:\n      sequence = get_sequence(signal, i_sample, window_sec, fs)\n      if sequence.size > 0:\n        labels.append(label)\n        valid_beats.append(sequence)\n\n  assert len(valid_beats) == len(labels)\n  return valid_beats, labels\n159/143:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/144:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef get_sequence(signal, beat_loc, window_sec, fs):\n  window_one_side = window_sec * fs\n  beat_start = beat_loc - window_one_side\n  beat_end = beat_loc + window_one_side\n  if beat_end < signal.shape[0]:\n    sequence = signal[beat_start:beat_end, 0]\n    print(sequence)\n    print(sequence.shape)\n    return sequence.reshape(1, -1, 1)\n  else:\n    return np.array([])\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n  atr_samples = annotation.sample\n  fs = record.fs\n  scaler = StandardScaler()\n  signal = scaler.fit_transform(record.p_signal)\n  sequence = get_sequence(signal, atr_samples[10], 3, fs)\n  print(signal.shape)\n  plt.plot(sequence[0])\n\n  # r_peaks = define_r_peaks_indices(record_wave)\n  # heartbeat_len = define_heartbeat_len(r_peaks)\n  # beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  valid_beats = []\n  window_sec = 3\n  for i, i_sample in enumerate(atr_samples):\n    label = classify_beat(atr_symbols[i])\n    if label is not None:\n      sequence = get_sequence(signal, i_sample, window_sec, fs)\n      if sequence.size > 0:\n        labels.append(label)\n        valid_beats.append(sequence)\n\n  assert len(valid_beats) == len(labels)\n  return valid_beats, labels\n159/145:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/146:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef get_sequence(signal, beat_loc, window_sec, fs):\n  window_one_side = window_sec * fs\n  beat_start = beat_loc - window_one_side\n  beat_end = beat_loc + window_one_side\n  if beat_end < signal.shape[0]:\n    sequence = signal[beat_start:beat_end, 0]\n    print(sequence)\n    print(sequence.shape)\n    return sequence.reshape(1, -1, 1)\n  else:\n    return np.array([])\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n  atr_samples = annotation.sample\n  fs = record.fs\n  scaler = StandardScaler()\n  signal = scaler.fit_transform(record.p_signal)\n  sequence = get_sequence(signal, atr_samples[10], 3, fs)\n\n  # r_peaks = define_r_peaks_indices(record_wave)\n  # heartbeat_len = define_heartbeat_len(r_peaks)\n  # beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  valid_beats = []\n  window_sec = 3\n  for i, i_sample in enumerate(atr_samples):\n    label = classify_beat(atr_symbols[i])\n    if label is not None:\n      sequence = get_sequence(signal, i_sample, window_sec, fs)\n      if sequence.size > 0:\n        labels.append(label)\n        valid_beats.append(sequence)\n\n  assert len(valid_beats) == len(labels)\n  return valid_beats, labels\n159/147:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/148:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef get_sequence(signal, beat_loc, window_sec, fs):\n  window_one_side = window_sec * fs\n  beat_start = beat_loc - window_one_side\n  beat_end = beat_loc + window_one_side\n  if beat_end < signal.shape[0]:\n    sequence = signal[beat_start:beat_end, 0]\n    return sequence\n  else:\n    return np.array([])\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n  atr_samples = annotation.sample\n  fs = record.fs\n  scaler = StandardScaler()\n  signal = scaler.fit_transform(record.p_signal)\n  sequence = get_sequence(signal, atr_samples[10], 3, fs)\n\n  # r_peaks = define_r_peaks_indices(record_wave)\n  # heartbeat_len = define_heartbeat_len(r_peaks)\n  # beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  valid_beats = []\n  window_sec = 3\n  for i, i_sample in enumerate(atr_samples):\n    label = classify_beat(atr_symbols[i])\n    if label is not None:\n      sequence = get_sequence(signal, i_sample, window_sec, fs)\n      if sequence.size > 0:\n        labels.append(label)\n        valid_beats.append(sequence)\n\n  assert len(valid_beats) == len(labels)\n  return valid_beats, labels\n159/149:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef get_sequence(signal, beat_loc, window_sec, fs):\n  window_one_side = window_sec * fs\n  beat_start = beat_loc - window_one_side\n  beat_end = beat_loc + window_one_side\n  if beat_end < signal.shape[0]:\n    sequence = signal[beat_start:beat_end, 0]\n    return sequence\n  else:\n    return np.array([])\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n  atr_samples = annotation.sample\n  fs = record.fs\n  scaler = StandardScaler()\n  signal = scaler.fit_transform(record.p_signal)\n  sequence = get_sequence(signal, atr_samples[10], 3, fs)\n\n  # r_peaks = define_r_peaks_indices(record_wave)\n  # heartbeat_len = define_heartbeat_len(r_peaks)\n  # beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  valid_beats = []\n  window_sec = 3\n  for i, i_sample in enumerate(atr_samples):\n    label = classify_beat(atr_symbols[i])\n    if label is not None:\n      sequence = get_sequence(signal, i_sample, window_sec, fs)\n      if sequence.size > 0:\n        labels.append(label)\n        valid_beats.append(sequence)\n\n  assert len(valid_beats) == len(labels)\n  return valid_beats, labels\n159/150:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/151:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef get_sequence(signal, beat_loc, window_sec, fs):\n  window_one_side = window_sec * fs\n  beat_start = beat_loc - window_one_side\n  beat_end = beat_loc + window_one_side\n  if beat_end < signal.shape[0]:\n    sequence = signal[beat_start:beat_end, 0]\n    return sequence\n  else:\n    return np.array([])\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n  atr_samples = annotation.sample\n  fs = record.fs\n  scaler = StandardScaler()\n  signal = scaler.fit_transform(record.p_signal)\n  sequence = get_sequence(signal, atr_samples[10], 3, fs)\n  plt.plot(sequence)\n\n  # r_peaks = define_r_peaks_indices(record_wave)\n  # heartbeat_len = define_heartbeat_len(r_peaks)\n  # beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  valid_beats = []\n  window_sec = 3\n  for i, i_sample in enumerate(atr_samples):\n    label = classify_beat(atr_symbols[i])\n    if label is not None:\n      sequence = get_sequence(signal, i_sample, window_sec, fs)\n      if sequence.size > 0:\n        labels.append(label)\n        valid_beats.append(sequence)\n\n  assert len(valid_beats) == len(labels)\n  return valid_beats, labels\n159/152:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/153:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef get_sequence(signal, beat_loc, window_sec, fs):\n  window_one_side = window_sec * fs\n  beat_start = beat_loc - window_one_side\n  beat_end = beat_loc + window_one_side\n  if beat_end < signal.shape[0]:\n    sequence = signal[beat_start:beat_end, 0]\n    return sequence\n  else:\n    return np.array([])\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n  atr_samples = annotation.sample\n  fs = record.fs\n  scaler = StandardScaler()\n  signal = scaler.fit_transform(record.p_signal)\n\n  # r_peaks = define_r_peaks_indices(record_wave)\n  # heartbeat_len = define_heartbeat_len(r_peaks)\n  # beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  valid_beats = []\n  window_sec = 3\n  for i, i_sample in enumerate(atr_samples):\n    label = classify_beat(atr_symbols[i])\n    if label is not None:\n      sequence = get_sequence(signal, i_sample, window_sec, fs)\n      if sequence.size > 0:\n        labels.append(label)\n        valid_beats.append(sequence)\n\n  assert len(valid_beats) == len(labels)\n  return valid_beats, labels\n159/154:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef get_sequence(signal, beat_loc, window_sec, fs):\n  window_one_side = window_sec * fs\n  beat_start = beat_loc - window_one_side\n  beat_end = beat_loc + window_one_side\n  if beat_end < signal.shape[0]:\n    sequence = signal[beat_start:beat_end, 0]\n    return sequence\n  else:\n    return np.array([])\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n  atr_samples = annotation.sample\n  fs = record.fs\n  scaler = StandardScaler()\n  signal = scaler.fit_transform(record.p_signal)\n\n  # r_peaks = define_r_peaks_indices(record_wave)\n  # heartbeat_len = define_heartbeat_len(r_peaks)\n  # beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  valid_beats = []\n  window_sec = 3\n  for i, i_sample in enumerate(atr_samples):\n    label = classify_beat(atr_symbols[i])\n    if label is not None:\n      sequence = get_sequence(signal, i_sample, window_sec, fs)\n      if sequence.size > 0:\n        labels.append(label)\n        valid_beats.append(sequence)\n\n  assert len(valid_beats) == len(labels)\n  return valid_beats, labels\n159/155:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/156:\nplt.plot(beats[0])\nwords = convert_beats_to_words(beats)\n159/157:\nnum_features = 300\nword2Vec_model = teach_word2vec_or_get_from_path(words, False, 'model_102_v2', num_features)\n159/158:\n\ndef teach_word2vec_or_get_from_path(words, file_path, num_features, use_cached = False):\n  \"\"\"Create and save model to file_path Word2Vec model.\n\n  :param words list of words\n  :param file_path path to save model\n  \"\"\"\n  min_word_count = 1   # Minimum word count\n  num_workers = 3       # Number of threads to run in parallel\n  context = 10          # Context window size\n  downsampling = 1e-3   # Downsample setting for frequent words\n  if not (os.path.exists(file_path) and use_cached):\n    model = Word2Vec([words,], workers=num_workers,\n                     vector_size=num_features, min_count = min_word_count,\n                     window = context, sample = downsampling)\n    model.save(file_path)\n  else:\n    model = Word2Vec.load(file_path)\n\n  return model\n159/159:\n\ndef create_beats_features(words, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(words),num_features), dtype='float32')\n\n  index2word_set =set(word2Vec_model.wv.index_to_key)\n\n  indices = []\n  valid_words_count = 0\n  for i in range(len(words)):\n    cur_word = words[i]\n    if cur_word in index2word_set:\n      review_feature_vecs[valid_words_count] = word2Vec_model.wv[cur_word]\n      valid_words_count = valid_words_count + 1\n      indices.append(i)\n\n  return indices, review_feature_vecs[:valid_words_count]\n\n\nvalid_beats_indices, features = create_beats_features(words, num_features, word2Vec_model)\n159/160:\nfeatures_labels = [labels[i] for i in valid_beats_indices][1:]\nfeatures = features[1:]\n159/161:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/162: words = convert_beats_to_words(beats)\n159/163:\nnum_features = 300\nword2Vec_model = teach_word2vec_or_get_from_path(words, False, 'model_102_v2', num_features)\n159/164:\n\n!pip install wfdb\n!pip install py-ecg-detectors\n!pip install sklearn\n!pip install gensim\n!pip install wget\n!pip install -U scikit-learn\n159/165:\n\nimport os\nimport wfdb\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom ecgdetectors import Detectors\nfrom gensim.models import Word2Vec\n159/166:\n# Returns (p_waves, qrs_waves, t_waves)\ndef split_beats_in_p_qrt_t(beats_list):\n  c_p_waves = []\n  c_qrs_waves = []\n  c_t_waves = []\n  heart_beat_len = len(beats_list[0])\n  retreat_value = heart_beat_len // 2\n  for j in range(len(beats_list)):\n    c_p_waves.append(beats_list[j][:retreat_value - 15])\n    c_qrs_waves.append(beats_list[j][retreat_value - 15:retreat_value + 15])\n    c_t_waves.append(beats_list[j][retreat_value + 15:])\n  return c_p_waves, c_qrs_waves, c_t_waves\n\n# Returns (kmeans, predicted)\ndef calculate_kmeans(waves, n_clusters, n_init):\n  c_kmeans = KMeans(init='k-means++', n_clusters=n_clusters, n_init=n_init)\n  c_kmeans.fit(waves)\n  c_predicted = c_kmeans.predict(waves)\n  return c_kmeans, c_predicted\n\n# Convert PT cluster predictions to letters\ndef convert_pt_predictions_to_letters(predictions):\n  alphabet_for_pt = {0:'a', 1:'b', 2:'c', 3:'d', 4:'e', 5:'f', 6:'g', 7:'h', 8:'i',\n                     9:'j', 10:'k', 11:'l', 12:'m', 13:'n', 14:'o', 15:'p', 16:'q', 17:'r',\n                     18:'s', 19:'t'}\n\n  def get_symbol_pt(x):\n    return alphabet_for_pt[x]\n\n  vfunc = np.vectorize(get_symbol_pt)\n\n  return vfunc(predictions)\n\n# Convert QRS clusters predictions to letters\ndef convert_qrs_predictions_to_letters(predictions):\n  alphabet_for_qrs = {0:'u', 1:'v', 2:'w', 3:'x', 4:'y', 5:'z'}\n\n\n  def get_symbol_qrs(x):\n    return alphabet_for_qrs[x]\n\n  vfunc_2 = np.vectorize(get_symbol_qrs)\n\n  return vfunc_2(predictions)\n\ndef join_waves_letters_to_words(qrs_letters, pt_letters):\n  words = []\n  signal_half_len = len(qrs_letters)//2\n  for i in range(len(qrs_letters)):\n    word = ''\n    word += pt_letters[i]\n    word += qrs_letters[i]\n    # T signal is encoded in second half\n    word += pt_letters[i + signal_half_len]\n    words.append(word)\n\n  return words\n\ndef convert_beats_to_words(beats_list):\n  (p_waves, qrs, t_waves) = split_beats_in_p_qrt_t(beats_list)\n  qrs_kmeans, qrs_predicted = calculate_kmeans(qrs, 6, 3)\n  c_pt_waves = np.array(p_waves + t_waves)\n  pt_kmeans, pt_predicted = calculate_kmeans(c_pt_waves, 20, 10)\n\n  qrs_letters = convert_qrs_predictions_to_letters(qrs_predicted)\n  pt_letters = convert_pt_predictions_to_letters(pt_predicted)\n  return join_waves_letters_to_words(qrs_letters, pt_letters)\n159/167:\n\ndef teach_word2vec_or_get_from_path(words, file_path, num_features, use_cached = False):\n  \"\"\"Create and save model to file_path Word2Vec model.\n  \"\"\"\n  min_word_count = 1   # Minimum word count\n  num_workers = 3       # Number of threads to run in parallel\n  context = 10          # Context window size\n  downsampling = 1e-3   # Downsample setting for frequent words\n  if not (os.path.exists(file_path) and use_cached):\n    model = Word2Vec([words,], workers=num_workers,\n                     vector_size=num_features, min_count = min_word_count,\n                     window = context, sample = downsampling)\n    model.save(file_path)\n  else:\n    model = Word2Vec.load(file_path)\n\n  return model\n159/168:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef get_sequence(signal, beat_loc, window_sec, fs):\n  window_one_side = window_sec * fs\n  beat_start = beat_loc - window_one_side\n  beat_end = beat_loc + window_one_side\n  if beat_end < signal.shape[0]:\n    sequence = signal[beat_start:beat_end, 0]\n    return sequence\n  else:\n    return np.array([])\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n  atr_samples = annotation.sample\n  fs = record.fs\n  scaler = StandardScaler()\n  signal = scaler.fit_transform(record.p_signal)\n\n  # r_peaks = define_r_peaks_indices(record_wave)\n  # heartbeat_len = define_heartbeat_len(r_peaks)\n  # beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  valid_beats = []\n  window_sec = 3\n  for i, i_sample in enumerate(atr_samples):\n    label = classify_beat(atr_symbols[i])\n    if label is not None:\n      sequence = get_sequence(signal, i_sample, window_sec, fs)\n      if sequence.size > 0:\n        labels.append(label)\n        valid_beats.append(sequence)\n\n  assert len(valid_beats) == len(labels)\n  return valid_beats, labels\n159/169:\n\nbeats, labels = load(\"../mitdb\", 102)\n159/170: words = convert_beats_to_words(beats)\n159/171:\nnum_features = 300\nword2Vec_model = teach_word2vec_or_get_from_path(words, False, 'model_102_v2', num_features)\n159/172:\nnum_features = 300\nword2Vec_model = teach_word2vec_or_get_from_path(words, 'model_102_v2', num_features, False)\n159/173:\n\ndef create_beats_features(words, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(words),num_features), dtype='float32')\n\n  index2word_set =set(word2Vec_model.wv.index_to_key)\n\n  indices = []\n  valid_words_count = 0\n  for i in range(len(words)):\n    cur_word = words[i]\n    if cur_word in index2word_set:\n      review_feature_vecs[valid_words_count] = word2Vec_model.wv[cur_word]\n      valid_words_count = valid_words_count + 1\n      indices.append(i)\n\n  return indices, review_feature_vecs[:valid_words_count]\n\n\nvalid_beats_indices, features = create_beats_features(words, num_features, word2Vec_model)\n159/174:\n\ndef create_beats_features(words, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(words),num_features), dtype='float32')\n\n  index2word_set =set(word2Vec_model.wv.index_to_key)\n\n  indices = []\n  valid_words_count = 0\n  for i in range(len(words)):\n    cur_word = words[i]\n    if cur_word in index2word_set:\n      review_feature_vecs[valid_words_count] = word2Vec_model.wv[cur_word]\n      valid_words_count = valid_words_count + 1\n      indices.append(i)\n\n  return indices, review_feature_vecs[:valid_words_count]\n\n\nvalid_beats_indices, features = create_beats_features(words, num_features, word2Vec_model)\n\nvalid_beats_labels = [labels[i] for i in valid_beats_indices]\n159/175:\nforest = RandomForestClassifier(n_estimators = 100)\nforest = forest.fit(features, labels)\n159/176:\nforest = RandomForestClassifier(n_estimators = 100)\nforest = forest.fit(features, valid_beats_labels)\n159/177:\nresult = forest.predict(features)\n\nclassification_report(valid_beats_indices, result)\n159/178:\n\nimport os\nimport wfdb\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom ecgdetectors import Detectors\nfrom gensim.models import Word2Vec\n159/179:\nresult = forest.predict(features)\n\nclassification_report(valid_beats_indices, result)\n159/180:\nresult = forest.predict(features)\n\nprint(classification_report(valid_beats_indices, result))\n159/181:\nresult = forest.predict(features)\n\nprint(classification_report(valid_beats_labels, result))\n159/182:\n\ndef teach_word2vec_or_get_from_path(words, file_path, num_features, use_cached = False):\n  \"\"\"Create and save model to file_path Word2Vec model.\n  \"\"\"\n  min_word_count = 1   # Minimum word count\n  num_workers = 3       # Number of threads to run in parallel\n  context = 10          # Context window size\n  downsampling = 1e-3   # Downsample setting for frequent words\n  if not (os.path.exists(file_path) and use_cached):\n    model = Word2Vec([words,], workers=num_workers,\n                     vector_size=num_features, min_count = min_word_count,\n                     window = context, sample = downsampling)\n    model.save(file_path)\n  else:\n    model = Word2Vec.load(file_path)\n\n  return model\n\ndef create_beats_features(words, num_features, word2Vec_model):\n  review_feature_vecs = np.zeros((len(words),num_features), dtype='float32')\n\n  index2word_set =set(word2Vec_model.wv.index_to_key)\n\n  indices = []\n  valid_words_count = 0\n  for i in range(len(words)):\n    cur_word = words[i]\n    if cur_word in index2word_set:\n      review_feature_vecs[valid_words_count] = word2Vec_model.wv[cur_word]\n      valid_words_count = valid_words_count + 1\n      indices.append(i)\n\n  return indices, review_feature_vecs[:valid_words_count]\n159/183:\n\nimport os\nimport wfdb\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom ecgdetectors import Detectors\nfrom gensim.models import Word2Vec\n159/184:\n\nimport os\nimport wfdb\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom ecgdetectors import Detectors\nfrom gensim.models import Word2Vec\n159/185:\n\ndef load_word_beats_and_features(set_number):\n  beats, labels, normal_percentage = load(\"../mitdb\", 102)\n  words = convert_beats_to_words(beats)\n  return words, labels, normal_percentage\n\ndef combine_sets_word_beats_and_features(sets_numbers):\n  sets_data = []\n  for set_number in sets_numbers:\n    words, labels, normal_percentage = load_word_beats_and_features(set_number)\n    sets_data.append({\n      \"words\": words,\n      \"labels\": labels,\n      \"normal_percentage\": normal_percentage\n    })\n  return sets_data\n\ndef vectorize_using_word2vec(words, labels):\n  num_features = 300\n  word2Vec_model = teach_word2vec_or_get_from_path(words, 'model_all', num_features, False)\n  valid_beats_indices, features = create_beats_features(words, num_features, word2Vec_model)\n\n  valid_beats_labels = [labels[i] for i in valid_beats_indices]\n  return features, valid_beats_labels\n\ndef prepare_train_and_test_sets():\n  data_frame = pd.DataFrame(combine_sets_word_beats_and_features([100, 101, 102, 103]))\n  bins = [0, 0.2, 0.6, 1.0]\n  #data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  #train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  print(data_frame)\n159/186: prepare_train_and_test_sets()\n159/187:\n\ninvalid_beat = [\n  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n]\n\nabnormal_beats = [\n  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n]\n\ndef classify_beat(symbol):\n  if symbol in abnormal_beats:\n    return 1\n  elif symbol == \"N\" or symbol == \".\":\n    return 0\n\ndef define_r_peaks_indices(wave):\n  # Defining r-peaks\n  fs = 250\n  detectors = Detectors(fs)\n  r_peaks = detectors.engzee_detector(wave)\n  # Amendment for first peak\n  r_peaks[0] += 20\n  return r_peaks\n\ndef define_heartbeat_len(r_peaks):\n  distances = []\n  for i in range(len(r_peaks)):\n    if i + 1 < len(r_peaks):\n      distances.append(r_peaks[i + 1] - r_peaks[i])\n  distances = np.array(distances)\n  heart_beat_len = int(distances.mean())\n  return heart_beat_len\n\ndef get_sequence(signal, beat_loc, window_sec, fs):\n  window_one_side = window_sec * fs\n  beat_start = beat_loc - window_one_side\n  beat_end = beat_loc + window_one_side\n  if beat_end < signal.shape[0]:\n    sequence = signal[beat_start:beat_end, 0]\n    return sequence\n  else:\n    return np.array([])\ndef split_in_beats(wave, heart_beat_len, r_peaks):\n  beats = []\n  for i in range(len(r_peaks)):\n    if i != 0 :\n      r_index = r_peaks[i]\n      retreat = heart_beat_len // 2\n      beats.append(wave[r_index - retreat:r_index + retreat])\n\n  return beats\n\ndef download_dataset(out_dir):\n  if os.path.isdir(out_dir):\n    print('You already have the data')\n  else:\n    wfdb.dl_database('mitdb', out_dir)\n\ndef load(out_dir, record_number):\n  download_dataset(out_dir)\n  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n  atr_symbols = annotation.symbol\n  atr_samples = annotation.sample\n  fs = record.fs\n  scaler = StandardScaler()\n  signal = scaler.fit_transform(record.p_signal)\n\n  # r_peaks = define_r_peaks_indices(record_wave)\n  # heartbeat_len = define_heartbeat_len(r_peaks)\n  # beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n  labels = []\n  valid_beats = []\n  window_sec = 3\n  for i, i_sample in enumerate(atr_samples):\n    label = classify_beat(atr_symbols[i])\n    if label is not None:\n      sequence = get_sequence(signal, i_sample, window_sec, fs)\n      if sequence.size > 0:\n        labels.append(label)\n        valid_beats.append(sequence)\n\n  normal_percentage = sum(labels) / len(labels)\n\n  assert len(valid_beats) == len(labels)\n  return valid_beats, labels, normal_percentage\n159/188:\n\ndef load_word_beats_and_features(set_number):\n  beats, labels, normal_percentage = load(\"../mitdb\", 102)\n  words = convert_beats_to_words(beats)\n  return words, labels, normal_percentage\n\ndef combine_sets_word_beats_and_features(sets_numbers):\n  sets_data = []\n  for set_number in sets_numbers:\n    words, labels, normal_percentage = load_word_beats_and_features(set_number)\n    sets_data.append({\n      \"words\": words,\n      \"labels\": labels,\n      \"normal_percentage\": normal_percentage\n    })\n  return sets_data\n\ndef vectorize_using_word2vec(words, labels):\n  num_features = 300\n  word2Vec_model = teach_word2vec_or_get_from_path(words, 'model_all', num_features, False)\n  valid_beats_indices, features = create_beats_features(words, num_features, word2Vec_model)\n\n  valid_beats_labels = [labels[i] for i in valid_beats_indices]\n  return features, valid_beats_labels\n\ndef prepare_train_and_test_sets():\n  data_frame = pd.DataFrame(combine_sets_word_beats_and_features([100, 101, 102, 103]))\n  bins = [0, 0.2, 0.6, 1.0]\n  #data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  #train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  print(data_frame)\n159/189: prepare_train_and_test_sets()\n159/190:\n\ndef load_word_beats_and_features(set_number):\n  beats, labels, normal_percentage = load(\"../mitdb\", 102)\n  words = convert_beats_to_words(beats)\n  return words, labels, normal_percentage\n\ndef combine_sets_word_beats_and_features(sets_numbers):\n  sets_data = []\n  for set_number in sets_numbers:\n    words, labels, normal_percentage = load_word_beats_and_features(set_number)\n    sets_data.append({\n      \"words\": words,\n      \"labels\": labels,\n      \"normal_percentage\": normal_percentage\n    })\n  return sets_data\n\ndef vectorize_using_word2vec(words, labels):\n  num_features = 300\n  word2Vec_model = teach_word2vec_or_get_from_path(words, 'model_all', num_features, False)\n  valid_beats_indices, features = create_beats_features(words, num_features, word2Vec_model)\n\n  valid_beats_labels = [labels[i] for i in valid_beats_indices]\n  return features, valid_beats_labels\n\ndef prepare_train_and_test_sets():\n  data_frame = pd.DataFrame(combine_sets_word_beats_and_features([100, 101]))\n  bins = [0, 0.2, 0.6, 1.0]\n  #data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  #train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  print(data_frame)\n159/191:\n\ndef load_word_beats_and_features(set_number):\n  beats, labels, normal_percentage = load(\"../mitdb\", 102)\n  words = convert_beats_to_words(beats)\n  return words, labels, normal_percentage\n\ndef combine_sets_word_beats_and_features(sets_numbers):\n  sets_data = []\n  for set_number in sets_numbers:\n    words, labels, normal_percentage = load_word_beats_and_features(set_number)\n    sets_data.append({\n      \"words\": words,\n      \"labels\": labels,\n      \"normal_percentage\": normal_percentage\n    })\n  return sets_data\n\ndef vectorize_using_word2vec(words, labels):\n  num_features = 300\n  word2Vec_model = teach_word2vec_or_get_from_path(words, 'model_all', num_features, False)\n  valid_beats_indices, features = create_beats_features(words, num_features, word2Vec_model)\n\n  valid_beats_labels = [labels[i] for i in valid_beats_indices]\n  return features, valid_beats_labels\n\ndef prepare_train_and_test_sets():\n  data_frame = pd.DataFrame(combine_sets_word_beats_and_features([100, 101]))\n  bins = [0, 0.2, 0.6, 1.0]\n  #data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  #train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  return data_frame\n159/192: data_frame = prepare_train_and_test_sets()\n159/193: data_frame.shape\n159/194: data_frame[\"words\"]\n159/195:\ndata_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\ntrain, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n159/196:\nbins = [0, 0.2, 0.6, 1.0]\ndata_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\ntrain, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n159/197: train\n159/198: train[\"words\"]\n159/199: len(train[\"words\"])\n159/200: train[\"words\"]\n159/201:\nfor i, row in train[\"words\"].iterrows():\n  print(row)\n159/202:\nfor i, row in train[\"words\"].iterrows():\n  print(row)\n159/203:\ntrain[\"words\"]\n# for i, row in train[\"words\"].iterrows():\n#   print(row)\n159/204:\ntrain.shape\n# for i, row in train[\"words\"].iterrows():\n#   print(row)\n159/205:\ntrain[\"words\"][0]\n# for i, row in train[\"words\"].iterrows():\n#   print(row)\n159/206:\ntrain[\"words\"]\n# for i, row in train[\"words\"].iterrows():\n#   print(row)\n159/207:\ntrain[\"words\"].to_numpy()\n# for i, row in train[\"words\"].iterrows():\n#   print(row)\n159/208:\ntrain[\"words\"].to_numpy().shape\n# for i, row in train[\"words\"].iterrows():\n#   print(row)\n159/209:\ntrain[\"words\"].to_numpy()[0]\n# for i, row in train[\"words\"].iterrows():\n#   print(row)\n159/210:\ndata_frame\n# for i, row in train[\"words\"].iterrows():\n#   print(row)\n159/211:\n\ndef load_word_beats_and_features(set_number):\n  beats, labels, normal_percentage = load(\"../mitdb\", 102)\n  words = convert_beats_to_words(beats)\n  return words, labels, normal_percentage\n\ndef combine_sets_word_beats_and_features(sets_numbers):\n  sets_data = []\n  for set_number in sets_numbers:\n    words, labels, normal_percentage = load_word_beats_and_features(set_number)\n    sets_data.append({\n      \"words\": words,\n      \"labels\": labels,\n      \"normal_percentage\": normal_percentage\n    })\n  return sets_data\n\ndef vectorize_using_word2vec(words, labels):\n  num_features = 300\n  word2Vec_model = teach_word2vec_or_get_from_path(words, 'model_all', num_features, False)\n  valid_beats_indices, features = create_beats_features(words, num_features, word2Vec_model)\n\n  valid_beats_labels = [labels[i] for i in valid_beats_indices]\n  return features, valid_beats_labels\n\ndef prepare_train_and_test_sets():\n  sets = combine_sets_word_beats_and_features([100, 101])\n  all_data = {\n    \"words\": [],\n    \"labels\": [],\n    \"normal_percentage\": []\n  }\n  for set in sets:\n    for key in set.keys():\n      all_data[key] = np.concatenate((all_data[key], set[key]))\n  data_frame = pd.DataFrame(all_data)\n  bins = [0, 0.2, 0.6, 1.0]\n  #data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  #train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  return data_frame\n159/212: data_frame = prepare_train_and_test_sets()\n159/213:\n\ndef load_word_beats_and_features(set_number):\n  beats, labels, normal_percentage = load(\"../mitdb\", 102)\n  words = convert_beats_to_words(beats)\n  return words, labels, normal_percentage\n\ndef combine_sets_word_beats_and_features(sets_numbers):\n  sets_data = []\n  for set_number in sets_numbers:\n    words, labels, normal_percentage = load_word_beats_and_features(set_number)\n    sets_data.append({\n      \"words\": words,\n      \"labels\": labels,\n      \"normal_percentage\": normal_percentage\n    })\n  return sets_data\n\ndef vectorize_using_word2vec(words, labels):\n  num_features = 300\n  word2Vec_model = teach_word2vec_or_get_from_path(words, 'model_all', num_features, False)\n  valid_beats_indices, features = create_beats_features(words, num_features, word2Vec_model)\n\n  valid_beats_labels = [labels[i] for i in valid_beats_indices]\n  return features, valid_beats_labels\n\nsets = combine_sets_word_beats_and_features([100, 101])\ndef prepare_train_and_test_sets():\n  all_data = {\n    \"words\": [],\n    \"labels\": [],\n    \"normal_percentage\": []\n  }\n  for set in sets:\n    for key in set.keys():\n      all_data[key] = np.concatenate((all_data[key], set[key]),axis=0)\n  data_frame = pd.DataFrame(all_data)\n  bins = [0, 0.2, 0.6, 1.0]\n  #data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  #train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  return data_frame\n159/214: data_frame = prepare_train_and_test_sets()\n159/215:\nall_data = {\n  \"words\": [],\n  \"labels\": [],\n  \"normal_percentage\": []\n}\nfor set in sets:\n  for key in set.keys():\n    all_data[key] = np.concatenate((all_data[key], set[key]),axis=0)\ndata_frame = pd.DataFrame(all_data)\n159/216:\nall_data = {\n  \"words\": [],\n  \"labels\": [],\n  \"normal_percentage\": []\n}\nfor set in sets:\n  for key in set.keys():\n    all_data[key] = np.concatenate((all_data[key], set[key]),axis=1)\ndata_frame = pd.DataFrame(all_data)\n159/217:\nall_data = {\n  \"words\": [],\n  \"labels\": [],\n  \"normal_percentage\": []\n}\nfor set in sets:\n  for key in set.keys():\n    all_data[key] = np.concatenate((all_data[key], set[key]),axis=0)\ndata_frame = pd.DataFrame(all_data)\n159/218:\nall_data = {\n  \"words\": [],\n  \"labels\": [],\n  \"normal_percentage\": []\n}\nfor set in sets:\n  for key in set.keys():\n    print(all_data[key].shape)\n    print(set[key].shape)\n    all_data[key] = np.concatenate((all_data[key], set[key]),axis=0)\ndata_frame = pd.DataFrame(all_data)\n159/219:\nall_data = {\n  \"words\": np.empty(1),\n  \"labels\": np.empty(1),\n  \"normal_percentage\": np.empty(1)\n}\nfor set in sets:\n  for key in set.keys():\n    print(all_data[key].shape)\n    print(set[key].shape)\n    all_data[key] = np.concatenate((all_data[key], set[key]),axis=0)\ndata_frame = pd.DataFrame(all_data)\n159/220:\nall_data = {\n  \"words\": np.empty(1),\n  \"labels\": np.empty(1),\n  \"normal_percentage\": np.empty(1)\n}\nfor set in sets:\n  for key in set.keys():\n    print(all_data[key].shape)\n    print(set[key].shape)\n    all_data[key] = np.concatenate((all_data[key], set[key]),axis=0)\ndata_frame = pd.DataFrame(all_data)\n159/221:\nall_data = {\n  \"words\": np.empty(1),\n  \"labels\": np.empty(1),\n  \"normal_percentage\": np.empty(1)\n}\nfor set in sets:\n  for key in set.keys():\n    all_data[key] = np.concatenate((all_data[key], set[key]),axis=0)\ndata_frame = pd.DataFrame(all_data)\n159/222:\nall_data = {\n  \"words\": np.empty(1),\n  \"labels\": np.empty(1),\n  \"normal_percentage\": np.empty(1)\n}\nfor set in sets:\n  for key in set.keys():\n    all_data[key] = np.concatenate((all_data[key], np.array(set[key])),axis=0)\ndata_frame = pd.DataFrame(all_data)\n159/223:\nall_data = {\n  \"words\": np.empty(1),\n  \"labels\": np.empty(1),\n  \"normal_percentage\": np.empty(1)\n}\nfor set in sets:\n  for key in set.keys():\n    print(np.array(set[key]).shape)\n    print(all_data[key].shape)\n    all_data[key] = np.concatenate((all_data[key], ),axis=0)\ndata_frame = pd.DataFrame(all_data)\n159/224:\nall_data = {\n  \"words\": np.empty(1),\n  \"labels\": np.empty(1),\n  \"normal_percentage\": np.empty(1)\n}\nfor set in sets:\n  for key in set.keys():\n    print(np.array(set[key]).shape)\n    print(all_data[key].shape)\n    all_data[key] = np.concatenate((all_data[key], np.array(set[key])),axis=1)\ndata_frame = pd.DataFrame(all_data)\n159/225:\na = np.array([1, 2])\nb = np.array([5, 6])\nnp.concatenate((a, b))\n\nall_data = {\n  \"words\": np.empty(1),\n  \"labels\": np.empty(1),\n  \"normal_percentage\": np.empty(1)\n}\nfor set in sets:\n  for key in set.keys():\n    print(np.array(set[key]).shape)\n    print(all_data[key].shape)\n    all_data[key] = np.concatenate((all_data[key], np.array(set[key])),axis=1)\ndata_frame = pd.DataFrame(all_data)\n159/226:\na = np.array([1, 2])\nb = np.array([5, 6])\nprint(np.concatenate((a, b)))\n\nall_data = {\n  \"words\": np.empty(1),\n  \"labels\": np.empty(1),\n  \"normal_percentage\": np.empty(1)\n}\nfor set in sets:\n  for key in set.keys():\n    print(np.array(set[key]).shape)\n    print(all_data[key].shape)\n    all_data[key] = np.concatenate((all_data[key], np.array(set[key])),axis=1)\ndata_frame = pd.DataFrame(all_data)\n159/227:\na = np.array([])\nb = np.array([5, 6])\nprint(np.concatenate((a, b)))\n\nall_data = {\n  \"words\": np.empty(1),\n  \"labels\": np.empty(1),\n  \"normal_percentage\": np.empty(1)\n}\nfor set in sets:\n  for key in set.keys():\n    print(np.array(set[key]).shape)\n    print(all_data[key].shape)\n    all_data[key] = np.concatenate((all_data[key], np.array(set[key])),axis=1)\ndata_frame = pd.DataFrame(all_data)\n159/228:\na = np.array([])\nprint(a.shape)\nb = np.array([5, 6])\nprint(np.concatenate((a, b)))\n\nall_data = {\n  \"words\": np.empty(1),\n  \"labels\": np.empty(1),\n  \"normal_percentage\": np.empty(1)\n}\nfor set in sets:\n  for key in set.keys():\n    print(np.array(set[key]).shape)\n    print(all_data[key].shape)\n    all_data[key] = np.concatenate((all_data[key], np.array(set[key])),axis=1)\ndata_frame = pd.DataFrame(all_data)\n159/229:\na = np.array([])\nprint(a.shape)\nb = np.array([5, 6])\nprint(np.concatenate((a, b)))\n\nall_data = {\n  \"words\": np.empty(0),\n  \"labels\": np.empty(0),\n  \"normal_percentage\": np.empty(0)\n}\nfor set in sets:\n  for key in set.keys():\n    print(np.array(set[key]).shape)\n    print(all_data[key].shape)\n    all_data[key] = np.concatenate((all_data[key], np.array(set[key])),axis=1)\ndata_frame = pd.DataFrame(all_data)\n159/230:\na = np.array([])\nprint(a.shape)\nb = np.array([5, 6])\nprint(np.concatenate((a, b)))\n\nall_data = {\n  \"words\": np.empty(0),\n  \"labels\": np.empty(0),\n  \"normal_percentage\": np.empty(0)\n}\nfor set in sets:\n  for key in set.keys():\n    print(np.array(set[key]).shape)\n    print(all_data[key].shape)\n    all_data[key] = np.concatenate((all_data[key], np.array(set[key])),axis=0)\ndata_frame = pd.DataFrame(all_data)\n159/231:\na = np.empty(0)\nprint(a.shape)\nb = np.array([5, 6])\nprint(np.concatenate((a, b)))\n\nall_data = {\n  \"words\": np.empty(0),\n  \"labels\": np.empty(0),\n  \"normal_percentage\": np.empty(0)\n}\nfor set in sets:\n  for key in set.keys():\n    print(np.array(set[key]).shape)\n    print(all_data[key].shape)\n    all_data[key] = np.concatenate((all_data[key], np.array(set[key])),axis=0)\ndata_frame = pd.DataFrame(all_data)\n159/232:\na = np.empty(0)\nprint(a.shape)\nb = np.array([5, 6])\nprint(np.concatenate((a, b)))\n\nall_data = {\n  \"words\": np.empty(0),\n  \"labels\": np.empty(0),\n  \"normal_percentage\": np.empty(0)\n}\nfor set in sets:\n  for key in set.keys():\n    print(np.array(set[key]).shape)\n    print(all_data[key].shape)\n    all_data[key] = np.concatenate((all_data[key], np.array(set[key])))\ndata_frame = pd.DataFrame(all_data)\n159/233:\na = np.empty(0)\nprint(a.shape)\nb = np.array([5, 6])\nprint(np.concatenate((a, b)))\n\nall_data = {\n  \"words\": np.empty(0),\n  \"labels\": np.empty(0),\n  \"normal_percentage\": np.empty(0)\n}\nfor set in sets:\n  for key in set.keys():\n    print(\"concat:\")\n    print(np.array(set[key]).shape)\n    print(all_data[key].shape)\n    all_data[key] = np.concatenate((all_data[key], np.array(set[key])))\ndata_frame = pd.DataFrame(all_data)\n159/234:\na = np.empty(0)\nprint(a.shape)\nb = np.array([5, 6])\nprint(np.concatenate((a, b)))\n\nall_data = {\n  \"words\": np.empty(0),\n  \"labels\": np.empty(0),\n  \"normal_percentage\": np.empty(0)\n}\nfor set in sets:\n  print(set.keys())\n  for key in set.keys():\n    print(\"concat:\")\n    print(np.array(set[key]).shape)\n    print(all_data[key].shape)\n    all_data[key] = np.concatenate((all_data[key], np.array(set[key])))\ndata_frame = pd.DataFrame(all_data)\n159/235:\na = np.empty(0)\nprint(a.shape)\nb = np.array([5, 6])\nprint(np.concatenate((a, b)))\n\nall_data = {\n  \"words\": np.empty(0),\n  \"labels\": np.empty(0),\n  \"normal_percentage\": np.empty(0)\n}\n\nprint(sets[0][\"normal_percentage\"])\nfor set in sets:\n  print(set.keys())\n  for key in set.keys():\n    print(\"concat:\")\n    print(np.array(set[key]).shape)\n    print(all_data[key].shape)\n    all_data[key] = np.concatenate((all_data[key], np.array(set[key])))\ndata_frame = pd.DataFrame(all_data)\n159/236:\n\ndef load_word_beats_and_features(set_number):\n  beats, labels, normal_percentage = load(\"../mitdb\", 102)\n  words = convert_beats_to_words(beats)\n  return words, labels, normal_percentage\n\ndef combine_sets_word_beats_and_features(sets_numbers):\n  sets_data = []\n  for set_number in sets_numbers:\n    words, labels, normal_percentage = load_word_beats_and_features(set_number)\n    sets_data.append({\n      \"words\": words,\n      \"labels\": labels,\n      \"normal_percentage\": normal_percentage\n    })\n  return sets_data\n\ndef vectorize_using_word2vec(words, labels):\n  num_features = 300\n  word2Vec_model = teach_word2vec_or_get_from_path(words, 'model_all', num_features, False)\n  valid_beats_indices, features = create_beats_features(words, num_features, word2Vec_model)\n\n  valid_beats_labels = [labels[i] for i in valid_beats_indices]\n  return features, valid_beats_labels\n\ndef prepare_train_and_test_sets():\n  data_frame = pd.DataFrame(combine_sets_word_beats_and_features([100, 101]))\n  bins = [0, 0.2, 0.6, 1.0]\n  #data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  #train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  return data_frame\n159/237:\n\ndef load_word_beats_and_features(set_number):\n  beats, labels, normal_percentage = load(\"../mitdb\", 102)\n  words = convert_beats_to_words(beats)\n  return words, labels, normal_percentage\n\ndef combine_sets_word_beats_and_features(sets_numbers):\n  sets_data = []\n  for set_number in sets_numbers:\n    words, labels, normal_percentage = load_word_beats_and_features(set_number)\n    sets_data.append({\n      \"words\": words,\n      \"labels\": labels,\n      \"normal_percentage\": normal_percentage\n    })\n  return sets_data\n\ndef vectorize_using_word2vec(words, labels):\n  num_features = 300\n  word2Vec_model = teach_word2vec_or_get_from_path(words, 'model_all', num_features, False)\n  valid_beats_indices, features = create_beats_features(words, num_features, word2Vec_model)\n\n  valid_beats_labels = [labels[i] for i in valid_beats_indices]\n  return features, valid_beats_labels\n\ndef prepare_train_and_test_sets():\n  data_frame = pd.DataFrame(combine_sets_word_beats_and_features([100, 101, 102]))\n  bins = [0, 0.2, 0.6, 1.0]\n  #data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  #train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  return data_frame\n159/238:\n\ntrain, validation = prepare_train_and_test_sets()\n159/239:\n\ndef load_word_beats_and_features(set_number):\n  beats, labels, normal_percentage = load(\"../mitdb\", 102)\n  words = convert_beats_to_words(beats)\n  return words, labels, normal_percentage\n\ndef combine_sets_word_beats_and_features(sets_numbers):\n  sets_data = []\n  for set_number in sets_numbers:\n    words, labels, normal_percentage = load_word_beats_and_features(set_number)\n    sets_data.append({\n      \"words\": words,\n      \"labels\": labels,\n      \"normal_percentage\": normal_percentage\n    })\n  return sets_data\n\ndef vectorize_using_word2vec(words, labels):\n  num_features = 300\n  word2Vec_model = teach_word2vec_or_get_from_path(words, 'model_all', num_features, False)\n  valid_beats_indices, features = create_beats_features(words, num_features, word2Vec_model)\n\n  valid_beats_labels = [labels[i] for i in valid_beats_indices]\n  return features, valid_beats_labels\n\ndef prepare_train_and_test_sets():\n  data_frame = pd.DataFrame(combine_sets_word_beats_and_features([100, 101, 102]))\n  bins = [0, 0.2, 0.6, 1.0]\n  data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  return train, validation\n159/240:\n\ntrain, validation = prepare_train_and_test_sets()\n159/241:\ntrain\n# for i, row in train[\"words\"].iterrows():\n#   print(row)\n159/242:\nvalidation\n# for i, row in train[\"words\"].iterrows():\n#   print(row)\n159/243:\ntrain.agg({'words': lambda x: list(x)})\n# for i, row in train[\"words\"].iterrows():\n#   print(row)\n159/244:\ntrain.groupby().agg({'words': lambda x: list(x)})\n# for i, row in train[\"words\"].iterrows():\n#   print(row)\n159/245:\ntrain.groupby(lambda _ : True).agg({'words': lambda x: list(x)})\n# for i, row in train[\"words\"].iterrows():\n#   print(row)\n159/246:\ntrain.groupby(lambda _ : True).agg({'words': lambda x: x})\n# for i, row in train[\"words\"].iterrows():\n#   print(row)\n159/247:\ntrain.groupby(lambda _ : True).agg({'words': lambda x: list(x)})\n# for i, row in train[\"words\"].iterrows():\n#   print(row)\n159/248:\nfor i, row in train.iterrows():\n  print(row)\n# for i, row in train[\"words\"].iterrows():\n#   print(row)\n159/249:\nall_words = np.empty(0)\nall_labels = np.empty(0)\nfor i, row in train.iterrows():\n  all_words = np.concatenate(all_words, row[\"words\"])\n  all_labels = np.concatenate(all_words, row[\"labels\"])\n# for i, row in train[\"words\"].iterrows():\n#   print(row)\n159/250:\nall_words = np.empty(0)\nall_labels = np.empty(0)\nfor i, row in train.iterrows():\n  all_words = np.concatenate((all_words, row[\"words\"]))\n  all_labels = np.concatenate((all_words, row[\"labels\"]))\n# for i, row in train[\"words\"].iterrows():\n#   print(row)\n159/251:\n\ndef load_word_beats_and_features(set_number):\n  beats, labels, normal_percentage = load(\"../mitdb\", 102)\n  words = convert_beats_to_words(beats)\n  return words, labels, normal_percentage\n\ndef combine_sets_word_beats_and_features(sets_numbers):\n  sets_data = []\n  for set_number in sets_numbers:\n    words, labels, normal_percentage = load_word_beats_and_features(set_number)\n    sets_data.append({\n      \"words\": words,\n      \"labels\": labels,\n      \"normal_percentage\": normal_percentage\n    })\n  return sets_data\n\ndef vectorize_using_word2vec(words, labels):\n  num_features = 300\n  word2Vec_model = teach_word2vec_or_get_from_path(words, 'model_all', num_features, False)\n  valid_beats_indices, features = create_beats_features(words, num_features, word2Vec_model)\n\n  valid_beats_labels = [labels[i] for i in valid_beats_indices]\n  return features, valid_beats_labels\n\ndef concatenate_rows(dataframe):\n  all_words = np.empty(0)\n  all_labels = np.empty(0)\n  for i, row in dataframe.iterrows():\n    all_words = np.concatenate((all_words, row[\"words\"]))\n    all_labels = np.concatenate((all_words, row[\"labels\"]))\n  return all_words, all_labels\n\ndef prepare_train_and_test_sets():\n  data_frame = pd.DataFrame(combine_sets_word_beats_and_features([100, 101, 102]))\n  bins = [0, 0.2, 0.6, 1.0]\n  data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  return concatenate_rows(train), concatenate_rows(validation)\n159/252:\n\ndef load_word_beats_and_features(set_number):\n  beats, labels, normal_percentage = load(\"../mitdb\", set_number)\n  words = convert_beats_to_words(beats)\n  return words, labels, normal_percentage\n\ndef combine_sets_word_beats_and_features(sets_numbers):\n  sets_data = []\n  for set_number in sets_numbers:\n    words, labels, normal_percentage = load_word_beats_and_features(set_number)\n    sets_data.append({\n      \"words\": words,\n      \"labels\": labels,\n      \"normal_percentage\": normal_percentage\n    })\n  return sets_data\n\ndef vectorize_using_prepared_word2vec(words, labels, word2Vec_model, num_features):\n  valid_beats_indices, features = create_beats_features(words, num_features, word2Vec_model)\n\n  valid_beats_labels = [labels[i] for i in valid_beats_indices]\n  return features, valid_beats_labels\n\ndef concatenate_rows(dataframe):\n  all_words = np.empty(0)\n  all_labels = np.empty(0)\n  for i, row in dataframe.iterrows():\n    all_words = np.concatenate((all_words, row[\"words\"]))\n    all_labels = np.concatenate((all_words, row[\"labels\"]))\n  return all_words, all_labels\n\ndef prepare_train_and_test_sets():\n  data_frame = pd.DataFrame(combine_sets_word_beats_and_features([100, 101, 102]))\n  bins = [0, 0.2, 0.6, 1.0]\n  data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  return concatenate_rows(train), concatenate_rows(validation)\n159/253:\n\ndef prepare_data_for_random_forest():\n  (train_words, train_labels), (validation_words, validation_labels) = prepare_train_and_test_sets()\n\n  # Create Word2Vec model based on train data\n  num_features = 300\n  word2Vec_model = teach_word2vec_or_get_from_path(train_words, 'model_all', num_features, False)\n\n  # Vectorize train data using Word2Vec\n  train_data = vectorize_using_prepared_word2vec(train_words, train_labels, word2Vec_model, num_features)\n\n  # Vectorize test data using trained Word2Vec\n  test_data = vectorize_using_prepared_word2vec(validation_words, validation_labels, word2Vec_model, num_features)\n  return train_data, test_data\n159/254:\n\ndef execute_random_forest_test():\n  (train_x, train_y), (validation_x, validation_y) = prepare_data_for_random_forest()\n  \n  forest = RandomForestClassifier(n_estimators = 100)\n  forest = forest.fit(train_x, train_y)\n  \n  result = forest.predict(validation_x)\n\n  print(classification_report(validation_y, result))\n\n# for i, row in train[\"words\"].iterrows():\n#   print(row)\n159/255:\n\ndef prepare_data_for_random_forest():\n  (train_words, train_labels), (validation_words, validation_labels) = prepare_train_and_test_sets()\n\n  # Create Word2Vec model based on train data\n  num_features = 300\n  word2Vec_model = teach_word2vec_or_get_from_path(train_words, 'model_all', num_features, False)\n\n  # Vectorize train data using Word2Vec\n  train_data = vectorize_using_prepared_word2vec(train_words, train_labels, word2Vec_model, num_features)\n\n  # Vectorize test data using trained Word2Vec\n  test_data = vectorize_using_prepared_word2vec(validation_words, validation_labels, word2Vec_model, num_features)\n  return train_data, test_data\n159/256:\n\ndef execute_random_forest_test():\n  (train_x, train_y), (validation_x, validation_y) = prepare_data_for_random_forest()\n\n  forest = RandomForestClassifier(n_estimators = 100)\n  forest = forest.fit(train_x, train_y)\n\n  result = forest.predict(validation_x)\n\n  print(classification_report(validation_y, result))\n\n# for i, row in train[\"words\"].iterrows():\n#   print(row)\n159/257: execute_random_forest_test()\n159/258:\n\ndef load_word_beats_and_features(set_number):\n  beats, labels, normal_percentage = load(\"../mitdb\", set_number)\n  words = convert_beats_to_words(beats)\n  return words, labels, normal_percentage\n\ndef combine_sets_word_beats_and_features(sets_numbers):\n  sets_data = []\n  for set_number in sets_numbers:\n    words, labels, normal_percentage = load_word_beats_and_features(set_number)\n    sets_data.append({\n      \"words\": words,\n      \"labels\": labels,\n      \"normal_percentage\": normal_percentage\n    })\n  return sets_data\n\ndef vectorize_using_prepared_word2vec(words, labels, word2Vec_model, num_features):\n  valid_beats_indices, features = create_beats_features(words, num_features, word2Vec_model)\n\n  valid_beats_labels = [labels[i] for i in valid_beats_indices]\n  return features, valid_beats_labels\n\ndef concatenate_rows(dataframe):\n  all_words = np.empty(0)\n  all_labels = np.empty(0)\n  for i, row in dataframe.iterrows():\n    all_words = np.concatenate((all_words, row[\"words\"]))\n    all_labels = np.concatenate((all_words, row[\"labels\"]))\n  return all_words, all_labels\n\n\ndata_frame = pd.DataFrame(combine_sets_word_beats_and_features([100, 101, 102, 103]))\n159/259:\ndef prepare_train_and_test_sets():\n  bins = [0, 0.2, 0.6, 1.0]\n  data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  return concatenate_rows(train), concatenate_rows(validation)\n\nprepare_train_and_test_sets()\n159/260:\ndef prepare_train_and_test_sets():\n  bins = [0, 0.2, 0.6, 1.0]\n  data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  return concatenate_rows(train), concatenate_rows(validation)\n\ndata_frame\n#prepare_train_and_test_sets()\n159/261:\ndef get_sets_names(dir_path):\n  from os import listdir\n  from os.path import isfile, join\n  onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n  return onlyfiles\n\ndef prepare_train_and_test_sets():\n  bins = [0, 0.2, 0.6, 1.0]\n  data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  return concatenate_rows(train), concatenate_rows(validation)\n\n\nprint(get_sets_names(\"../mitdb\"))\ndata_frame\n#prepare_train_and_test_sets()\n159/262:\ndef get_sets_names(dir_path):\n  from os import listdir\n  from os.path import isfile, join\n  onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n  def get_file_name(file):\n    return file.split(\".\", 1)[0]\n  return list(set(map(get_file_name, onlyfiles)))\n\ndef prepare_train_and_test_sets():\n  bins = [0, 0.2, 0.6, 1.0]\n  data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  return concatenate_rows(train), concatenate_rows(validation)\n\n\nprint(get_sets_names(\"../mitdb\"))\ndata_frame\n#prepare_train_and_test_sets()\n159/263:\ndef get_sets_names(dir_path):\n  from os import listdir\n  from os.path import isfile, join\n  onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n  def get_file_name(file):\n    return file.split(\".\", 1)[0]\n  return map(get_file_name, onlyfiles)#list(set())\n\ndef prepare_train_and_test_sets():\n  bins = [0, 0.2, 0.6, 1.0]\n  data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  return concatenate_rows(train), concatenate_rows(validation)\n\n\nprint(get_sets_names(\"../mitdb\"))\ndata_frame\n#prepare_train_and_test_sets()\n159/264:\ndef get_sets_names(dir_path):\n  from os import listdir\n  from os.path import isfile, join\n  onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n  def get_file_name(file):\n    return file.split(\".\", 1)[0]\n  return list(map(get_file_name, onlyfiles))  #list(set())\n\ndef prepare_train_and_test_sets():\n  bins = [0, 0.2, 0.6, 1.0]\n  data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  return concatenate_rows(train), concatenate_rows(validation)\n\n\nprint(get_sets_names(\"../mitdb\"))\ndata_frame\n#prepare_train_and_test_sets()\n159/265:\ndef get_sets_names(dir_path):\n  from os import listdir\n  from os.path import isfile, join\n  onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n  def get_file_name(file):\n    return file.split(\".\", 1)[0]\n  return set(list(map(get_file_name, onlyfiles)))  #list(set())\n\ndef prepare_train_and_test_sets():\n  bins = [0, 0.2, 0.6, 1.0]\n  data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  return concatenate_rows(train), concatenate_rows(validation)\n\n\nprint(get_sets_names(\"../mitdb\"))\ndata_frame\n#prepare_train_and_test_sets()\n159/266:\n\ndef load_word_beats_and_features(set_number):\n  beats, labels, normal_percentage = load(\"../mitdb\", set_number)\n  words = convert_beats_to_words(beats)\n  return words, labels, normal_percentage\n\ndef combine_sets_word_beats_and_features(sets_numbers):\n  sets_data = []\n  for set_number in sets_numbers:\n    words, labels, normal_percentage = load_word_beats_and_features(set_number)\n    sets_data.append({\n      \"words\": words,\n      \"labels\": labels,\n      \"normal_percentage\": normal_percentage\n    })\n  return sets_data\n\ndef vectorize_using_prepared_word2vec(words, labels, word2Vec_model, num_features):\n  valid_beats_indices, features = create_beats_features(words, num_features, word2Vec_model)\n\n  valid_beats_labels = [labels[i] for i in valid_beats_indices]\n  return features, valid_beats_labels\n\ndef concatenate_rows(dataframe):\n  all_words = np.empty(0)\n  all_labels = np.empty(0)\n  for i, row in dataframe.iterrows():\n    all_words = np.concatenate((all_words, row[\"words\"]))\n    all_labels = np.concatenate((all_words, row[\"labels\"]))\n  return all_words, all_labels\n\ndef load_and_save_set(filename, reload=False):\n  if os.path.exists(filename) and not reload:\n    return pd.read_pickle(filename)\n  else:\n    data_frame = pd.DataFrame(combine_sets_word_beats_and_features([100, 101, 102, 103]))\n    data_frame.to_pickle(filename)\n    return data_frame\n\ndef prepare_train_and_test_sets():\n  data_frame = load_and_save_set(\"database_cache.pkl\")\n  bins = [0, 0.2, 0.6, 1.0]\n  data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  return concatenate_rows(train), concatenate_rows(validation)\n159/267:\nload_and_save_set(\"database_cache.pkl\")\n\ndef get_sets_names(dir_path):\n  from os import listdir\n  from os.path import isfile, join\n  onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n  def get_file_name(file):\n    return file.split(\".\", 1)[0]\n  return list(set(list(map(get_file_name, onlyfiles))))\n\n\n\n\n#print(get_sets_names(\"../mitdb\"))\n\n#prepare_train_and_test_sets()\n159/268:\nload_and_save_set(\"database_cache.pkl\")\n\ndef get_sets_names(dir_path):\n  from os import listdir\n  from os.path import isfile, join\n  onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n  def get_file_name(file):\n    return file.split(\".\", 1)[0]\n  return list(set(list(map(get_file_name, onlyfiles))))\n\n\n\n\n#print(get_sets_names(\"../mitdb\"))\n\n#prepare_train_and_test_sets()\n159/269:\nprint(load_and_save_set(\"database_cache.pkl\"))\n\ndef get_sets_names(dir_path):\n  from os import listdir\n  from os.path import isfile, join\n  onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n  def get_file_name(file):\n    return file.split(\".\", 1)[0]\n  return list(set(list(map(get_file_name, onlyfiles))))\n\n\n\n\n#print(get_sets_names(\"../mitdb\"))\n\n#prepare_train_and_test_sets()\n159/270:\ndel set\ndef get_sets_names(dir_path):\n  from os import listdir\n  from os.path import isfile, join\n  onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n  def get_file_name(file):\n    return file.split(\".\", 1)[0]\n  return list(set(list(map(get_file_name, onlyfiles))))\n\n\n\n\n#print(get_sets_names(\"../mitdb\"))\n\n#prepare_train_and_test_sets()\n159/271:\ndef get_sets_names(dir_path):\n  from os import listdir\n  from os.path import isfile, join\n  onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n  def get_file_name(file):\n    return file.split(\".\", 1)[0]\n  return list(set(list(map(get_file_name, onlyfiles))))\n\n\n\n\nprint(get_sets_names(\"../mitdb\"))\n\n#prepare_train_and_test_sets()\n159/272:\ndef get_sets_names(dir_path):\n  from os import listdir\n  from os.path import isfile, join\n  onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n  def get_file_name(file):\n    return file.split(\".\", 1)[0]\n  return [int(x) for x in list(set(list(map(get_file_name, onlyfiles))))]\n\n\n\n\nprint(get_sets_names(\"../mitdb\"))\n\n#prepare_train_and_test_sets()\n159/273:\ndef get_sets_names(dir_path):\n  from os import listdir\n  from os.path import isfile, join\n  onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n  def get_file_name(file):\n    return file.split(\".\", 1)[0]\n  return [int(x) for x in list(set(list(map(get_file_name, onlyfiles))))].sort()\n\n\n\n\nprint(get_sets_names(\"../mitdb\"))\n\n#prepare_train_and_test_sets()\n159/274:\ndef get_sets_names(dir_path):\n  from os import listdir\n  from os.path import isfile, join\n  onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n  def get_file_name(file):\n    return file.split(\".\", 1)[0]\n\n  data = [int(x) for x in list(set(list(map(get_file_name, onlyfiles))))]\n  data.sort()\n  return data\n\n\n\n\nprint(get_sets_names(\"../mitdb\"))\n\n#prepare_train_and_test_sets()\n159/275:\n\ndef load_word_beats_and_features(set_number):\n  beats, labels, normal_percentage = load(\"../mitdb\", set_number)\n  words = convert_beats_to_words(beats)\n  return words, labels, normal_percentage\n\ndef combine_sets_word_beats_and_features(sets_numbers):\n  sets_data = []\n  for set_number in sets_numbers:\n    words, labels, normal_percentage = load_word_beats_and_features(set_number)\n    sets_data.append({\n      \"words\": words,\n      \"labels\": labels,\n      \"normal_percentage\": normal_percentage\n    })\n  return sets_data\n\ndef vectorize_using_prepared_word2vec(words, labels, word2Vec_model, num_features):\n  valid_beats_indices, features = create_beats_features(words, num_features, word2Vec_model)\n\n  valid_beats_labels = [labels[i] for i in valid_beats_indices]\n  return features, valid_beats_labels\n\ndef concatenate_rows(dataframe):\n  all_words = np.empty(0)\n  all_labels = np.empty(0)\n  for i, row in dataframe.iterrows():\n    all_words = np.concatenate((all_words, row[\"words\"]))\n    all_labels = np.concatenate((all_words, row[\"labels\"]))\n  return all_words, all_labels\n\ndef get_sets_names(dir_path):\n  from os import listdir\n  from os.path import isfile, join\n  onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n  def get_file_name(file):\n    return file.split(\".\", 1)[0]\n\n  data = [int(x) for x in list(set(list(map(get_file_name, onlyfiles))))]\n  data.sort()\n  return data\n\ndef load_and_save_set(filename, reload=False):\n  if os.path.exists(filename) and not reload:\n    return pd.read_pickle(filename)\n  else:\n    data_frame = pd.DataFrame(combine_sets_word_beats_and_features(get_sets_names(\"../mitdb\")))\n    data_frame.to_pickle(filename)\n    return data_frame\n\ndef prepare_train_and_test_sets():\n  data_frame = load_and_save_set(\"database_cache.pkl\", True)\n  bins = [0, 0.2, 0.6, 1.0]\n  data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  return concatenate_rows(train), concatenate_rows(validation)\n159/276:\n\ndef load_word_beats_and_features(set_number):\n  beats, labels, normal_percentage = load(\"../mitdb\", set_number)\n  words = convert_beats_to_words(beats)\n  return words, labels, normal_percentage\n\ndef combine_sets_word_beats_and_features(sets_numbers):\n  sets_data = []\n  for set_number in sets_numbers:\n    words, labels, normal_percentage = load_word_beats_and_features(set_number)\n    sets_data.append({\n      \"words\": words,\n      \"labels\": labels,\n      \"normal_percentage\": normal_percentage\n    })\n  return sets_data\n\ndef vectorize_using_prepared_word2vec(words, labels, word2Vec_model, num_features):\n  valid_beats_indices, features = create_beats_features(words, num_features, word2Vec_model)\n\n  valid_beats_labels = [labels[i] for i in valid_beats_indices]\n  return features, valid_beats_labels\n\ndef concatenate_rows(dataframe):\n  all_words = np.empty(0)\n  all_labels = np.empty(0)\n  for i, row in dataframe.iterrows():\n    all_words = np.concatenate((all_words, row[\"words\"]))\n    all_labels = np.concatenate((all_words, row[\"labels\"]))\n  return all_words, all_labels\n\ndef get_sets_names(dir_path):\n  from os import listdir\n  from os.path import isfile, join\n  onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n  def get_file_name(file):\n    return file.split(\".\", 1)[0]\n\n  data = [int(x) for x in list(set(list(map(get_file_name, onlyfiles))))]\n  data.sort()\n  return data\n\ndef load_and_save_set(filename, reload=False):\n  if os.path.exists(filename) and not reload:\n    return pd.read_pickle(filename)\n  else:\n    data_frame = pd.DataFrame(combine_sets_word_beats_and_features(get_sets_names(\"../mitdb\")[:2]))\n    data_frame.to_pickle(filename)\n    return data_frame\n\ndef prepare_train_and_test_sets():\n  data_frame = load_and_save_set(\"database_cache.pkl\", True)\n  bins = [0, 0.2, 0.6, 1.0]\n  data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  return concatenate_rows(train), concatenate_rows(validation)\n159/277:\n\n\ndata_frame = load_and_save_set(\"database_cache.pkl\", True)\n\n\n\nprint(get_sets_names(\"../mitdb\"))\n\n#prepare_train_and_test_sets()\n159/278:\n\ndef load_word_beats_and_features(set_number):\n  beats, labels, normal_percentage = load(\"../mitdb\", set_number)\n  words = convert_beats_to_words(beats)\n  return words, labels, normal_percentage\n\ndef combine_sets_word_beats_and_features(sets_numbers):\n  sets_data = []\n  for set_number in sets_numbers:\n    print(\"Loading set: \" + str(set_number))\n    words, labels, normal_percentage = load_word_beats_and_features(set_number)\n    sets_data.append({\n      \"words\": words,\n      \"labels\": labels,\n      \"normal_percentage\": normal_percentage\n    })\n  return sets_data\n\ndef vectorize_using_prepared_word2vec(words, labels, word2Vec_model, num_features):\n  valid_beats_indices, features = create_beats_features(words, num_features, word2Vec_model)\n\n  valid_beats_labels = [labels[i] for i in valid_beats_indices]\n  return features, valid_beats_labels\n\ndef concatenate_rows(dataframe):\n  all_words = np.empty(0)\n  all_labels = np.empty(0)\n  for i, row in dataframe.iterrows():\n    all_words = np.concatenate((all_words, row[\"words\"]))\n    all_labels = np.concatenate((all_words, row[\"labels\"]))\n  return all_words, all_labels\n\ndef get_sets_names(dir_path):\n  from os import listdir\n  from os.path import isfile, join\n  onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n  def get_file_name(file):\n    return file.split(\".\", 1)[0]\n\n  data = [int(x) for x in list(set(list(map(get_file_name, onlyfiles))))]\n  data.sort()\n  return data\n\ndef load_and_save_set(filename, reload=False):\n  if os.path.exists(filename) and not reload:\n    return pd.read_pickle(filename)\n  else:\n    data_frame = pd.DataFrame(combine_sets_word_beats_and_features(get_sets_names(\"../mitdb\")))\n    data_frame.to_pickle(filename)\n    return data_frame\n\ndef prepare_train_and_test_sets():\n  data_frame = load_and_save_set(\"database_cache.pkl\", True)\n  bins = [0, 0.2, 0.6, 1.0]\n  data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  return concatenate_rows(train), concatenate_rows(validation)\n159/279:\n\n\ndata_frame = load_and_save_set(\"database_cache.pkl\", True)\n\n\n\nprint(get_sets_names(\"../mitdb\"))\n\n#prepare_train_and_test_sets()\n159/280:\n\ndef load_word_beats_and_features(set_number):\n  beats, labels, normal_percentage = load(\"../mitdb\", set_number)\n  words = convert_beats_to_words(beats)\n  return words, labels, normal_percentage\n\ndef combine_sets_word_beats_and_features(sets_numbers):\n  sets_data = []\n  for set_number in sets_numbers:\n    print(\"Loading set: \" + str(set_number))\n    words, labels, normal_percentage = load_word_beats_and_features(set_number)\n    sets_data.append({\n      \"words\": words,\n      \"labels\": labels,\n      \"normal_percentage\": normal_percentage\n    })\n  return sets_data\n\ndef vectorize_using_prepared_word2vec(words, labels, word2Vec_model, num_features):\n  valid_beats_indices, features = create_beats_features(words, num_features, word2Vec_model)\n\n  valid_beats_labels = [labels[i] for i in valid_beats_indices]\n  return features, valid_beats_labels\n\ndef concatenate_rows(dataframe):\n  all_words = np.empty(0)\n  all_labels = np.empty(0)\n  for i, row in dataframe.iterrows():\n    all_words = np.concatenate((all_words, row[\"words\"]))\n    all_labels = np.concatenate((all_words, row[\"labels\"]))\n  return all_words, all_labels\n\ndef get_sets_names(dir_path):\n  from os import listdir\n  from os.path import isfile, join\n  onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n  def get_file_name(file):\n    return file.split(\".\", 1)[0]\n\n  data = [int(x) for x in list(set(list(map(get_file_name, onlyfiles))))]\n  data.sort()\n  return data\n\ndef load_and_save_set(filename, reload=False):\n  if os.path.exists(filename) and not reload:\n    return pd.read_pickle(filename)\n  else:\n    data_frame = pd.DataFrame(combine_sets_word_beats_and_features(get_sets_names(\"../mitdb\")))\n    data_frame.to_pickle(filename)\n    return data_frame\n\ndef prepare_train_and_test_sets():\n  data_frame = load_and_save_set(\"database_cache.pkl\", False)\n  bins = [0, 0.2, 0.6, 1.0]\n  data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n  train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n\n  return concatenate_rows(train), concatenate_rows(validation)\n159/281:\n\n\ndata_frame = load_and_save_set(\"database_cache.pkl\", False)\n\n\n\nprint(get_sets_names(\"../mitdb\"))\n\n#prepare_train_and_test_sets()\n159/282: data_frame\n159/283:\n\ndef prepare_data_for_random_forest():\n  (train_words, train_labels), (validation_words, validation_labels) = prepare_train_and_test_sets()\n\n  # Create Word2Vec model based on train data\n  num_features = 300\n  word2Vec_model = teach_word2vec_or_get_from_path(train_words, 'model_all', num_features, False)\n\n  # Vectorize train data using Word2Vec\n  train_data = vectorize_using_prepared_word2vec(train_words, train_labels, word2Vec_model, num_features)\n\n  # Vectorize test data using trained Word2Vec\n  test_data = vectorize_using_prepared_word2vec(validation_words, validation_labels, word2Vec_model, num_features)\n  return train_data, test_data\n159/284:\n\ndef execute_random_forest_test():\n  (train_x, train_y), (validation_x, validation_y) = prepare_data_for_random_forest()\n\n  forest = RandomForestClassifier(n_estimators = 100)\n  forest = forest.fit(train_x, train_y)\n\n  result = forest.predict(validation_x)\n\n  print(classification_report(validation_y, result))\n\n# for i, row in train[\"words\"].iterrows():\n#   print(row)\n159/285: execute_random_forest_test()\n   1:\n\nnp.sqrt(np.mean(np.square(((original_full - restored_full) / original_full)), axis=0))\n\n\n#%history -g\n   2: %history -g\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "%history -g -f random_forest.ipynb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}