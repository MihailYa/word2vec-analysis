{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# !pip install wfdb\n",
    "# !pip install py-ecg-detectors\n",
    "# !pip install sklearn\n",
    "# !pip install gensim\n",
    "# !pip install wget\n",
    "# !pip install pandas\n",
    "# !pip install -U scikit-learn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import uuid\n",
    "import os\n",
    "import wfdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from ecgdetectors import Detectors\n",
    "from gensim.models import Word2Vec\n",
    "from datetime import datetime"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# Returns (p_waves, qrs_waves, t_waves)\n",
    "def split_beats_in_p_qrt_t(beats_list):\n",
    "  c_p_waves = []\n",
    "  c_qrs_waves = []\n",
    "  c_t_waves = []\n",
    "  heart_beat_len = len(beats_list[0])\n",
    "  retreat_value = heart_beat_len // 2\n",
    "  for j in range(len(beats_list)):\n",
    "    c_p_waves.append(beats_list[j][:retreat_value - 15])\n",
    "    c_qrs_waves.append(beats_list[j][retreat_value - 15:retreat_value + 15])\n",
    "    c_t_waves.append(beats_list[j][retreat_value + 15:])\n",
    "  return c_p_waves, c_qrs_waves, c_t_waves\n",
    "\n",
    "# Returns (kmeans, predicted)\n",
    "def calculate_kmeans(waves, n_clusters, n_init):\n",
    "  c_kmeans = KMeans(init='k-means++', n_clusters=n_clusters, n_init=n_init)\n",
    "  c_kmeans.fit(waves)\n",
    "  c_predicted = c_kmeans.predict(waves)\n",
    "  return c_kmeans, c_predicted\n",
    "\n",
    "# Convert PT cluster predictions to letters\n",
    "def convert_pt_predictions_to_letters(predictions):\n",
    "  alphabet_for_pt = {0:'a', 1:'b', 2:'c', 3:'d', 4:'e', 5:'f', 6:'g', 7:'h', 8:'i',\n",
    "                     9:'j', 10:'k', 11:'l', 12:'m', 13:'n', 14:'o', 15:'p', 16:'q', 17:'r',\n",
    "                     18:'s', 19:'t'}\n",
    "\n",
    "  def get_symbol_pt(x):\n",
    "    return alphabet_for_pt[x]\n",
    "\n",
    "  vfunc = np.vectorize(get_symbol_pt)\n",
    "\n",
    "  return vfunc(predictions)\n",
    "\n",
    "# Convert QRS clusters predictions to letters\n",
    "def convert_qrs_predictions_to_letters(predictions):\n",
    "  alphabet_for_qrs = {0:'u', 1:'v', 2:'w', 3:'x', 4:'y', 5:'z'}\n",
    "\n",
    "\n",
    "  def get_symbol_qrs(x):\n",
    "    return alphabet_for_qrs[x]\n",
    "\n",
    "  vfunc_2 = np.vectorize(get_symbol_qrs)\n",
    "\n",
    "  return vfunc_2(predictions)\n",
    "\n",
    "def join_waves_letters_to_words(qrs_letters, pt_letters):\n",
    "  words = []\n",
    "  signal_half_len = len(qrs_letters)//2\n",
    "  for i in range(len(qrs_letters)):\n",
    "    word = ''\n",
    "    word += pt_letters[i]\n",
    "    word += qrs_letters[i]\n",
    "    # T signal is encoded in second half\n",
    "    word += pt_letters[i + signal_half_len]\n",
    "    words.append(word)\n",
    "\n",
    "  return words\n",
    "\n",
    "def convert_beats_to_words(beats_list):\n",
    "  (p_waves, qrs, t_waves) = split_beats_in_p_qrt_t(beats_list)\n",
    "  qrs_kmeans, qrs_predicted = calculate_kmeans(qrs, 6, 3)\n",
    "  c_pt_waves = np.array(p_waves + t_waves)\n",
    "  pt_kmeans, pt_predicted = calculate_kmeans(c_pt_waves, 20, 10)\n",
    "\n",
    "  qrs_letters = convert_qrs_predictions_to_letters(qrs_predicted)\n",
    "  pt_letters = convert_pt_predictions_to_letters(pt_predicted)\n",
    "  return join_waves_letters_to_words(qrs_letters, pt_letters)\n",
    "\n",
    "class BeatsToWordsConverter:\n",
    "  qrs_kmeans = None\n",
    "  pt_kmeans = None\n",
    "  \n",
    "  def save_to_file(self, file_name):\n",
    "    pickle.dump(self.qrs_kmeans, open(file_name + \"_qrs.pkl\", \"wb\"))\n",
    "    pickle.dump(self.pt_kmeans, open(file_name + \"_pt.pkl\", \"wb\"))\n",
    "  \n",
    "  def load_from_file(self, file_name):\n",
    "    self.qrs_kmeans = pickle.load(open(file_name + \"_qrs.pkl\", \"rb\"))\n",
    "    self.pt_kmeans = pickle.load(open(file_name + \"_pt.pkl\", \"rb\"))\n",
    "  \n",
    "  def fit_and_predict_words(self, beats_list, cache_file_name, reset_cache=False):\n",
    "    \n",
    "    if reset_cache or not (os.path.exists(cache_file_name + \"_qrs.pkl\") and os.path.exists(cache_file_name + \"_pt.pkl\")):\n",
    "      (p_waves, qrs, t_waves) = split_beats_in_p_qrt_t(beats_list)\n",
    "      print(\"Fitting QRS KMeans\")\n",
    "      self.qrs_kmeans = KMeans(init='k-means++', n_clusters=6, n_init=3)\n",
    "      self.qrs_kmeans.fit(qrs)\n",
    "      \n",
    "      qrs_predicted = self.qrs_kmeans.predict(qrs)\n",
    "      \n",
    "      print(\"Fitting PT KMeans\")\n",
    "      c_pt_waves = np.array(p_waves + t_waves)\n",
    "      self.pt_kmeans = KMeans(init='k-means++', n_clusters=20, n_init=10)\n",
    "      self.pt_kmeans.fit(c_pt_waves)\n",
    "      \n",
    "      print(\"Saving BeatsToWordsConverter to cache: \", cache_file_name)\n",
    "      self.save_to_file(cache_file_name)\n",
    "      \n",
    "      pt_predicted = self.pt_kmeans.predict(c_pt_waves) \n",
    "    \n",
    "      qrs_letters = convert_qrs_predictions_to_letters(qrs_predicted)\n",
    "      pt_letters = convert_pt_predictions_to_letters(pt_predicted)\n",
    "      return join_waves_letters_to_words(qrs_letters, pt_letters)\n",
    "    else:\n",
    "      print(\"Using cached BeatsToWordsConverter from file: \", cache_file_name)\n",
    "      self.load_from_file(cache_file_name)\n",
    "      return self.predict_words(beats_list)\n",
    "  \n",
    "  def predict_words(self, beats_list):\n",
    "    (p_waves, qrs, t_waves) = split_beats_in_p_qrt_t(beats_list)\n",
    "    c_pt_waves = np.array(p_waves + t_waves)\n",
    "    qrs_predicted = self.qrs_kmeans.predict(qrs)\n",
    "    pt_predicted = self.pt_kmeans.predict(c_pt_waves)\n",
    "    \n",
    "    qrs_letters = convert_qrs_predictions_to_letters(qrs_predicted)\n",
    "    pt_letters = convert_pt_predictions_to_letters(pt_predicted)\n",
    "    return join_waves_letters_to_words(qrs_letters, pt_letters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Word2Vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def teach_word2vec_or_get_from_path(words, file_path, num_features, use_cached = False):\n",
    "  \"\"\"Create and save model to file_path Word2Vec model.\n",
    "  \"\"\"\n",
    "  min_word_count = 1   # Minimum word count\n",
    "  num_workers = 3       # Number of threads to run in parallel\n",
    "  context = 10          # Context window size\n",
    "  downsampling = 1e-3   # Downsample setting for frequent words\n",
    "  if not (os.path.exists(file_path) and use_cached):\n",
    "    model = Word2Vec([words,], workers=num_workers,\n",
    "                     vector_size=num_features, min_count = min_word_count,\n",
    "                     window = context, sample = downsampling)\n",
    "    model.save(file_path)\n",
    "  else:\n",
    "    print(\"Using cached Word2Vec from file: \", file_path)\n",
    "    model = Word2Vec.load(file_path)\n",
    "\n",
    "  return model\n",
    "\n",
    "def create_beats_features(words, num_features, word2vec_model):\n",
    "  review_feature_vecs = np.zeros((len(words),num_features), dtype='float32')\n",
    "\n",
    "  index2word_set =set(word2vec_model.wv.index_to_key)\n",
    "\n",
    "  indices = []\n",
    "  valid_words_count = 0\n",
    "  for i in range(len(words)):\n",
    "    cur_word = words[i]\n",
    "    if cur_word in index2word_set:\n",
    "      review_feature_vecs[valid_words_count] = word2vec_model.wv[cur_word]\n",
    "      valid_words_count = valid_words_count + 1\n",
    "      indices.append(i)\n",
    "\n",
    "  return indices, review_feature_vecs[:valid_words_count]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Loading and preparing MIT DB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "invalid_beat = [\n",
    "  \"[\", \"!\", \"]\", \"x\", \"(\", \")\", \"p\", \"t\",\n",
    "  \"u\", \"`\", \"'\", \"^\", \"|\", \"~\", \"+\", \"s\",\n",
    "  \"T\", \"*\", \"D\", \"=\", '\"', \"@\"\n",
    "]\n",
    "\n",
    "abnormal_beats = [\n",
    "  \"L\", \"R\", \"B\", \"A\", \"a\", \"J\", \"S\", \"V\",\n",
    "  \"r\", \"F\", \"e\", \"j\", \"n\", \"E\", \"/\", \"f\", \"Q\", \"?\"\n",
    "]\n",
    "\n",
    "def classify_beat(symbol):\n",
    "  if symbol in abnormal_beats:\n",
    "    return 1\n",
    "  elif symbol == \"N\" or symbol == \".\":\n",
    "    return 0\n",
    "\n",
    "def define_r_peaks_indices(wave):\n",
    "  # Defining r-peaks\n",
    "  fs = 250\n",
    "  detectors = Detectors(fs)\n",
    "  r_peaks = detectors.engzee_detector(wave)\n",
    "  # Amendment for first peak\n",
    "  r_peaks[0] += 20\n",
    "  return r_peaks\n",
    "\n",
    "def define_heartbeat_len(r_peaks):\n",
    "  distances = []\n",
    "  for i in range(len(r_peaks)):\n",
    "    if i + 1 < len(r_peaks):\n",
    "      distances.append(r_peaks[i + 1] - r_peaks[i])\n",
    "  distances = np.array(distances)\n",
    "  heart_beat_len = int(distances.mean())\n",
    "  return heart_beat_len\n",
    "\n",
    "def get_sequence(signal, beat_loc, window_sec, fs):\n",
    "  window_one_side = window_sec * fs\n",
    "  beat_start = beat_loc - window_one_side\n",
    "  beat_end = beat_loc + window_one_side\n",
    "  if beat_end < signal.shape[0]:\n",
    "    sequence = signal[beat_start:beat_end, 0]\n",
    "    return sequence\n",
    "  else:\n",
    "    return np.array([])\n",
    "def split_in_beats(wave, heart_beat_len, r_peaks):\n",
    "  beats = []\n",
    "  for i in range(len(r_peaks)):\n",
    "    if i != 0 :\n",
    "      r_index = r_peaks[i]\n",
    "      retreat = heart_beat_len // 2\n",
    "      beats.append(wave[r_index - retreat:r_index + retreat])\n",
    "\n",
    "  return beats\n",
    "\n",
    "def download_dataset(out_dir):\n",
    "  if os.path.isdir(out_dir):\n",
    "    print('You already have the data')\n",
    "  else:\n",
    "    wfdb.dl_database('mitdb', out_dir)\n",
    "\n",
    "def load(out_dir, record_number):\n",
    "  download_dataset(out_dir)\n",
    "  record = wfdb.rdrecord(str(out_dir) + '/' + str(record_number))\n",
    "  annotation = wfdb.rdann(str(out_dir) + '/' + str(record_number), \"atr\")\n",
    "  atr_symbols = annotation.symbol\n",
    "  atr_samples = annotation.sample\n",
    "  fs = record.fs\n",
    "  scaler = StandardScaler()\n",
    "  signal = scaler.fit_transform(record.p_signal)\n",
    "\n",
    "  # r_peaks = define_r_peaks_indices(record_wave)\n",
    "  # heartbeat_len = define_heartbeat_len(r_peaks)\n",
    "  # beats = split_in_beats(record_wave, heartbeat_len, r_peaks)\n",
    "  labels = []\n",
    "  valid_beats = []\n",
    "  window_sec = 3\n",
    "  for i, i_sample in enumerate(atr_samples):\n",
    "    label = classify_beat(atr_symbols[i])\n",
    "    if label is not None:\n",
    "      sequence = get_sequence(signal, i_sample, window_sec, fs)\n",
    "      if sequence.size > 0:\n",
    "        labels.append(label)\n",
    "        valid_beats.append(sequence)\n",
    "\n",
    "  normal_percentage = sum(labels) / len(labels)\n",
    "\n",
    "  assert len(valid_beats) == len(labels)\n",
    "  return valid_beats, labels, normal_percentage"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def combine_sets_beats_and_features(sets_numbers):\n",
    "  sets_data = []\n",
    "  for set_number in sets_numbers:\n",
    "    print(\"Loading set: \" + str(set_number))\n",
    "    beats, labels, normal_percentage = load(\"../mitdb\", set_number)\n",
    "    sets_data.append({\n",
    "      \"beats\": beats,\n",
    "      \"labels\": labels,\n",
    "      \"normal_percentage\": normal_percentage\n",
    "    })\n",
    "  return sets_data\n",
    "\n",
    "def vectorize_using_prepared_word2vec(words, labels, word2vec_model, num_features):\n",
    "  valid_beats_indices, features = create_beats_features(words, num_features, word2vec_model)\n",
    "\n",
    "  valid_beats_labels = [labels[i] for i in valid_beats_indices]\n",
    "  return features, valid_beats_labels\n",
    "\n",
    "def concatenate_rows(dataframe):\n",
    "  \"\"\"\n",
    "\n",
    "  :param dataframe: \n",
    "  :return: dataframe with connected rows\n",
    "  \"\"\"\n",
    "  all_beats = []\n",
    "  all_labels = []\n",
    "  for i, row in dataframe.iterrows():\n",
    "    all_beats.append(row[\"beats\"])\n",
    "    all_labels.append(row[\"labels\"])\n",
    "  \n",
    "  beats = np.concatenate(all_beats).tolist()\n",
    "  labels = np.concatenate(all_labels).tolist()\n",
    "  return pd.DataFrame({\n",
    "    \"beats\": beats,\n",
    "    \"labels\": labels\n",
    "  })\n",
    "\n",
    "def get_sets_names(dir_path):\n",
    "  from os import listdir\n",
    "  from os.path import isfile, join\n",
    "  onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n",
    "  def get_file_name(file):\n",
    "    return file.split(\".\", 1)[0]\n",
    "\n",
    "  data = [int(x) for x in list(set(list(map(get_file_name, onlyfiles))))]\n",
    "  data.sort()\n",
    "  return data\n",
    "\n",
    "def load_and_save_set(filename, reload=False):\n",
    "  if os.path.exists(filename) and not reload:\n",
    "    print(\"Using cached dataset\")\n",
    "    return pd.read_pickle(filename)\n",
    "  else:\n",
    "    print(\"Reloading dataset\")\n",
    "    data_frame = pd.DataFrame(combine_sets_beats_and_features(get_sets_names(\"../mitdb\")))\n",
    "    data_frame.to_pickle(filename)\n",
    "    return data_frame\n",
    "\n",
    "def prepare_train_and_test_sets(reload=False, train_path = \"train_db_cache.pkl\", validation_path = \"validation_db_cache.pkl\", reload_database_set = False, database_set_path =\"database_cache.pkl\", sets_count_limit=None):\n",
    "  if reload or not (os.path.exists(train_path) and os.path.exists(validation_path)):\n",
    "    print(\"Loading dataset\")\n",
    "    data_frame = load_and_save_set(database_set_path, reload_database_set)\n",
    "    if sets_count_limit is not None:\n",
    "      data_frame = data_frame[:sets_count_limit]\n",
    "    print(\"Splitting dataset\")\n",
    "    bins = [0, 0.2, 0.6, 1.0]\n",
    "    data_frame[\"bin\"] = pd.cut(data_frame['normal_percentage'], bins=bins, labels=False, include_lowest=True)\n",
    "    train, validation = train_test_split(data_frame, test_size=0.25, stratify=data_frame[\"bin\"], random_state=42)\n",
    "    \n",
    "    print(\"Applying explode to train data\")\n",
    "    train_df = concatenate_rows(train)\n",
    "    print(\"Saving train data\")\n",
    "    train_df.to_pickle(train_path)\n",
    "    \n",
    "    print(\"Applying explode to validation data\")\n",
    "    validation_df = concatenate_rows(validation)\n",
    "    print(\"Saving validation data\")\n",
    "    validation_df.to_pickle(validation_path)\n",
    "    \n",
    "    print(\"DF to lists conversion\")\n",
    "    return (train_df[\"beats\"].tolist(), train_df[\"labels\"].tolist()), (validation_df[\"beats\"].tolist(), validation_df[\"labels\"].tolist())\n",
    "  else:\n",
    "    print(\"Using cached train&validation datasets\")\n",
    "    train_df = pd.read_pickle(train_path)\n",
    "    validation_df = pd.read_pickle(validation_path)\n",
    "    return (train_df[\"beats\"].tolist(), train_df[\"labels\"].tolist()), (validation_df[\"beats\"].tolist(), validation_df[\"labels\"].tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "class TestParameters:\n",
    "  # Limit number of beats in train set (Can invalidate result)\n",
    "  train_size_limit: int\n",
    "  # Limit number of beats in validation set (Can invalidate result)\n",
    "  validation_size_limit: int\n",
    "  \n",
    "  # Limit number of sets used in test\n",
    "  base_data_sets_count_limit: int\n",
    "  \n",
    "  # Use cached train&validation sets with names: train_path and validation_path\n",
    "  use_cache_train_and_validation_sets: bool\n",
    "  train_path: str\n",
    "  validation_path: str\n",
    "  \n",
    "  reset_beats2words_cache: bool\n",
    "  beats2words_cache_path: str\n",
    "  \n",
    "  reset_word2vec_cache: bool\n",
    "  word2vec_cache_path: str\n",
    "  \n",
    "  \n",
    "  def __init__(self, train_size_limit=None, validation_size_limit=None, base_data_sets_count_limit=None, use_cache_train_and_validation_sets=True, train_path=\"train_db_cache.pkl\", validation_path=\"validation_db_cache.pkl\", reset_beats2words_cache=True, beats2words_cache_path=\"beats2words\", reset_word2vec_cache=True, word2vec_cache_path=\"word2vecModel\"):\n",
    "    self.train_size_limit = train_size_limit\n",
    "    self.validation_size_limit = validation_size_limit\n",
    "    self.base_data_sets_count_limit= base_data_sets_count_limit\n",
    "    self.use_cache_train_and_validation_sets = use_cache_train_and_validation_sets\n",
    "    self.train_path=train_path\n",
    "    self.validation_path = validation_path\n",
    "    self.reset_beats2words_cache = reset_beats2words_cache\n",
    "    self.beats2words_cache_path = beats2words_cache_path\n",
    "    self.reset_word2vec_cache = reset_word2vec_cache\n",
    "    self.word2vec_cache_path = word2vec_cache_path\n",
    "  \n",
    "  def __str__(self):\n",
    "    return \"{\\ntrain_size_limit=\" + str(self.train_size_limit)\\\n",
    "           +\"\\nvalidation_size_limit=\" + str(self.validation_size_limit)\\\n",
    "           +\"\\nbase_data_sets_count_limit=\" + str(self.base_data_sets_count_limit)\\\n",
    "           +\"\\nuse_cache_train_and_validation_sets=\" + str(self.use_cache_train_and_validation_sets)\\\n",
    "           +\"\\ntrain_path=\" + str(self.train_path)\\\n",
    "           +\"\\nvalidation_path=\" + str(self.validation_path)\\\n",
    "           +\"\\nreset_beats2words_cache=\" + str(self.reset_beats2words_cache)\\\n",
    "           +\"\\nbeats2words_cache_path=\" + str(self.beats2words_cache_path)\\\n",
    "           +\"\\nreset_word2vec_cache=\" + str(self.reset_word2vec_cache)\\\n",
    "           +\"\\nword2vec_cache_path=\" + str(self.word2vec_cache_path)\\\n",
    "           + \"\\n}\"\n",
    "  \n",
    "\n",
    "def prepare_data_for_random_forest(test_parameters):\n",
    "  \"\"\"\n",
    "  \n",
    "  :param test_parameters: TestParameters\n",
    "  :return: \n",
    "  \"\"\"\n",
    "  (train_beats, train_labels), (validation_beats, validation_labels) = prepare_train_and_test_sets(\n",
    "    reload=not test_parameters.use_cache_train_and_validation_sets,\n",
    "    train_path=test_parameters.train_path,\n",
    "    validation_path=test_parameters.validation_path,\n",
    "    sets_count_limit=test_parameters.base_data_sets_count_limit\n",
    "  )\n",
    "  if test_parameters.train_size_limit is not None:\n",
    "    train_beats = train_beats[:test_parameters.train_size_limit]\n",
    "    train_labels = train_labels[:test_parameters.train_size_limit]\n",
    "  if test_parameters.validation_size_limit is not None:\n",
    "    validation_beats = validation_beats[:test_parameters.validation_size_limit]\n",
    "    validation_labels = validation_labels[:test_parameters.validation_size_limit]\n",
    "  \n",
    "  # Create KMeans model based on train data\n",
    "  print(\"Training BeatsToWordsConverter\")\n",
    "  beats2words_converter = BeatsToWordsConverter()\n",
    "  train_words = beats2words_converter.fit_and_predict_words(train_beats, test_parameters.beats2words_cache_path, test_parameters.reset_beats2words_cache)\n",
    "  \n",
    "  # Convert test data into words based on trained BeatsToWordsConverter\n",
    "  print(\"Predicting validation words using trained BeatsToWordsConverter\")\n",
    "  validation_words = beats2words_converter.predict_words(validation_beats)\n",
    "\n",
    "  # Create Word2Vec model based on train data\n",
    "  print(\"Training Word2Vec\")\n",
    "  num_features = 300\n",
    "  word2Vec_model = teach_word2vec_or_get_from_path(train_words, test_parameters.word2vec_cache_path, num_features, not test_parameters.reset_word2vec_cache)\n",
    "\n",
    "  print(\"Vectorizing train data\")\n",
    "  # Vectorize train data using Word2Vec\n",
    "  train_data = vectorize_using_prepared_word2vec(train_words, train_labels, word2Vec_model, num_features)\n",
    "\n",
    "  print(\"Vectorizing test data\")\n",
    "  # Vectorize test data using trained Word2Vec\n",
    "  test_data = vectorize_using_prepared_word2vec(validation_words, validation_labels, word2Vec_model, num_features)\n",
    "  return train_data, test_data\n",
    "\n",
    "def execute_random_forest_test(test_parameters, comment):\n",
    "  \"\"\"\n",
    "  \n",
    "  :param test_parameters: TestParameters\n",
    "  :param comment: \n",
    "  :return: \n",
    "  \"\"\"\n",
    "  test_start_time = datetime.now()\n",
    "  \n",
    "  (train_x, train_y), (validation_x, validation_y) = prepare_data_for_random_forest(test_parameters)\n",
    "\n",
    "  print(\"Training random forest\")\n",
    "  forest = RandomForestClassifier(n_estimators = 100, verbose=True, n_jobs=4)\n",
    "  forest = forest.fit(train_x, train_y)\n",
    "\n",
    "  print(\"Predicting using random forest\")\n",
    "  result = forest.predict(validation_x)\n",
    "\n",
    "  print(\"Creating classification report\")\n",
    "  repord_id = uuid.uuid1()\n",
    "  report = str(classification_report(validation_y, result))\n",
    "  print(\"Report id: \" + str(repord_id))\n",
    "  print(report)\n",
    "  f = open(\"results.txt\", \"a\")\n",
    "  \n",
    "  now = datetime.now()\n",
    "  f.write(\"===========================================\\n\")\n",
    "  f.write(\"Report id: \" + str(repord_id) + \"\\n\")\n",
    "  f.write(\"Report start date: \" + test_start_time.strftime(\"%d.%m.%Y %H:%M:%S\") + \"\\n\")\n",
    "  f.write(\"Report end date: \" + now.strftime(\"%d.%m.%Y %H:%M:%S\") + \"\\n\")\n",
    "  f.write(\"Test parameters:\\n\" + str(test_parameters) + \"\\n\")\n",
    "  f.write(\"Actual train size: \" + str(len(train_x)) + \"\\n\")\n",
    "  f.write(\"Actual validation size: \" + str(len(validation_x)) + \"\\n\")\n",
    "  f.write(\"Comment: \" + comment + \"\\n\")\n",
    "  f.write(report)\n",
    "  f.write(\"\\n===========================================\")\n",
    "  f.write(\"\\n\\n\")\n",
    "  f.close()\n",
    "  #os.system(\"shutdown /s /t 1\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Using cached train&validation datasets\n",
      "Training BeatsToWordsConverter\n",
      "Fitting QRS KMeans\n",
      "Fitting PT KMeans\n",
      "Saving BeatsToWordsConverter to cache:  beats2words_20\n",
      "Predicting validation words using trained BeatsToWordsConverter\n",
      "Training Word2Vec\n",
      "Vectorizing train data\nVectorizing test data\n",
      "Training random forest\n",
      "Predicting using random forest\n",
      "Creating classification report\n",
      "Report id: bc8936e4-29bf-11ec-bde0-ec8eb50e3be9\n              precision    recall  f1-score   support\n\n           0       0.91      0.60      0.72      6841\n           1       0.38      0.81      0.52      2120\n\n    accuracy                           0.65      8961\n   macro avg       0.65      0.70      0.62      8961\nweighted avg       0.79      0.65      0.67      8961\n\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   13.1s finished\n[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.6s finished\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "execute_random_forest_test(TestParameters(\n",
    "  train_size_limit=None,\n",
    "  validation_size_limit=None,\n",
    "  base_data_sets_count_limit=20,\n",
    "  \n",
    "  use_cache_train_and_validation_sets=True,\n",
    "  train_path=\"short20_train_db_cache.pkl\",\n",
    "  validation_path=\"short20_validation_db_cache.pkl\",\n",
    "  \n",
    "  reset_beats2words_cache=True,\n",
    "  beats2words_cache_path=\"beats2words_20\",\n",
    "  reset_word2vec_cache=True,\n",
    "  word2vec_cache_path=\"word2vec_20.pkl\"\n",
    "), \"Test with first 6 datasets with cached from 5 datasets word2vec and kmeans\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}